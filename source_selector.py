import json
import os
import re
import time
from typing import Dict, List, Tuple, Any, Optional

import requests

import yfinance as yf
from yahoo_fin import stock_info

from api import LLMClient
from timing_utils import TimingRecorder

class IntelligentSourceSelector:
    """æ™ºèƒ½æºé€‰æ‹©å™¨ - å¸¦å…·ä½“APIé…ç½®çš„ç‰ˆæœ¬"""
    
    def __init__(
        self,
        llm_client: Optional[LLMClient] = None,
        *,
        use_llm: Optional[bool] = None,
        google_api_key: Optional[str] = None,
        google_weather_base_url: str = "https://weather.googleapis.com/v1",
        google_routes_base_url: str = "https://routes.googleapis.com",
        google_geocode_url: str = "https://maps.googleapis.com/maps/api/geocode/json",
        request_timeout: int = 12,
        finnhub_api_key: Optional[str] = None,
        sportsdb_api_key: Optional[str] = None,
        apisports_api_key: Optional[str] = None,  # æ·»åŠ ç¼ºå¤±å‚æ•°
    ):
        # é¢†åŸŸå…³é”®è¯æ˜ å°„
        self.domain_keywords = {
            "weather": [
                "å¤©æ°”", "æ°”æ¸©", "æ¸©åº¦", "ä¸‹é›¨", "ä¸‹é›ª", "å°é£", "æš´é›¨",
                "å¤©æ°£", "æ°£æº«", "æº«åº¦", "é¢±é¢¨",
                "weather", "temperature", "rain", "snow", "typhoon",
                "ç©ºæ°”è´¨é‡", "ç©ºæ°”æ±¡æŸ“", "AQI", "PM2.5", "PM10", "é›¾éœ¾", "ç©ºæ°”æŒ‡æ•°",
                "air quality", "air pollution", "AQI", "PM2.5", "PM10", "smog", "haze",
                "æŒ‡æ•°", "æ±¡æŸ“", "pm25", "pm10", "æŒ‡æ•°", "è´¨é‡", "aqi", "pm2.5"
            ],
            "transportation": [
                "äº¤é€š", "å…¬äº¤", "åœ°é“", "æ‹¥å µ", "è·¯å†µ", "èˆªç­", "ç«è½¦", "é«˜é“",
                "å…¬è»Š", "åœ°éµ", "æ“å µ", "è·¯æ³", "èˆªç­", "ç«è»Š", "é«˜éµ",
                "traffic", "bus", "subway", "congestion", "flight", "train"
            ],
            "finance": [
                "è‚¡ç¥¨", "è‚¡ä»·", "é‡‘è", "æ±‡ç‡", "æŠ•èµ„", "åŸºé‡‘", "é»„é‡‘", "åŸæ²¹",
                "è‚¡åƒ¹", "åŒ¯ç‡", "æŠ•è³‡", "åŸºé‡‘", "é»ƒé‡‘", "åŸæ²¹",
                "stock", "finance", "exchange rate", "investment", "fund"
            ],
            "sports": [
                "ä½“è‚²", "è¶³çƒ", "ç¯®çƒ", "ç½‘çƒ", "æ¯”èµ›", "æ¯”åˆ†", "NBA", "å¥¥è¿", "ä¸–ç•Œæ¯", "è‹±è¶…",
                "sports", "football", "basketball", "tennis", "match", "score", "NBA", "Olympics", "Premier League"
            ],
            "temporal_change": [
                # æ•™è‚²æ’åç›¸å…³
                "å¤§å­¦", "é«˜æ ¡", "å­¦é™¢", "å­¦æ ¡", "æ’å", "QS", "THE", "ARWU", "US News",
                "university", "college", "ranking", "rankings", "education", "higher education",
                "é¦™æ¸¯ä¸­æ–‡å¤§å­¸", "é¦™æ¸¯ç§‘æŠ€å¤§å­¸", "é¦™æ¸¯å¤§å­¸", "CUHK", "HKUST", "HKU",
                "é¦™æ¸¯ä¸­æ–‡å¤§å­¦", "é¦™æ¸¯ç§‘æŠ€å¤§å­¦", "é¦™æ¸¯å¤§å­¦",
                # æ—¶é—´å˜åŒ–ç›¸å…³
                "æœ€è¿‘10å¹´", "è¿‡å»10å¹´", "10å¹´", "åå¹´", "å†å¹´", "å†å²", "å˜åŒ–", "è¶‹åŠ¿", "å‘å±•",
                "10 years", "decade", "historical", "trend", "development", "evolution",
                "å¯¹æ¯”", "æ¯”è¾ƒ", "å˜åŒ–è¶‹åŠ¿", "æ—¶é—´åºåˆ—", "å¹´åº¦", "é€å¹´",
                "comparison", "compare", "trend over time", "time series", "yearly", "year by year",
                # å…¶ä»–å¯èƒ½çš„æ—¶é—´å˜åŒ–æŸ¥è¯¢
                "å¢é•¿", "ä¸‹é™", "æ³¢åŠ¨", "å˜åŒ–ç‡", "å¢é•¿ç‡", "æ¶¨è·Œ",
                "growth", "decline", "fluctuation", "rate of change", "growth rate", "rise and fall"
            ],
            "location": [
                "æœ€è¿‘", "é™„è¿‘", "è·ç¦»", "å“ªå®¶", "å“ªé‡Œ", "åœ¨å“ª", "å‘¨è¾¹", "æ—è¾¹",
                "æœ€è¿‘çš„", "é™„è¿‘çš„", "ç¦»", "é è¿‘",
                "è·é›¢", "å“ªè£¡", "é€±é‚Š",
                "nearest", "nearby", "closest", "near", "around", "where is",
                "find", "locate", "location", "place", "places"
            ],
            "general": []  # é€šç”¨é¢†åŸŸï¼Œæ— ç‰¹å®šå…³é”®è¯
        }
        
        # å…·ä½“çš„æ•°æ®æºAPIé…ç½®
        self.domain_sources = {
            "weather": [
                {
                    "name": "Google Weather API",
                    "url": "https://weather.googleapis.com/v1/currentConditions:lookup",
                    "type": "rest_api",
                    "description": "Google Cloud æä¾›çš„å®æ—¶å¤©æ°”æ•°æ®"
                },
                {
                    "name": "Google Air Quality API",
                    "url": "https://airquality.googleapis.com/v1/currentConditions:lookup",
                    "type": "rest_api",
                    "description": "Google Cloud æä¾›çš„å®æ—¶ç©ºæ°”è´¨é‡æ•°æ®ï¼ŒåŒ…æ‹¬AQIã€PM2.5ç­‰æ±¡æŸ“ç‰©ä¿¡æ¯"
                },
                {
                    "name": "Google Geocoding API",
                    "url": "https://maps.googleapis.com/maps/api/geocode/json",
                    "type": "rest_api",
                    "description": "ç”¨äºå°†åœ°ç‚¹åç§°è§£æä¸ºåæ ‡ä»¥ä¾¿è·å–å¤©æ°”å’Œç©ºæ°”è´¨é‡"
                }
            ],
            "transportation": [
                {
                    "name": "Google Routes Preferred API",
                    "url": "https://routes.googleapis.com/directions/v2:computeRoutes",
                    "type": "rest_api",
                    "description": "æ”¯æŒäº¤é€šæ‹¥å µçš„è·¯çº¿è§„åˆ’ï¼ˆå«å®æ—¶è·¯å†µï¼‰"
                },
                {
                    "name": "Google Geocoding API",
                    "url": "https://maps.googleapis.com/maps/api/geocode/json",
                    "type": "rest_api",
                    "description": "èµ·ç‚¹/ç»ˆç‚¹åœ°åè§£æ"
                }
            ],
            "finance": [
                {
                    "name": "yfinance",
                    "type": "python_lib",
                    "description": "Yahoo Finance Pythonåº“ (yfinance)"
                },
                {
                    "name": "yahoo-fin",
                    "type": "python_lib",
                    "description": "Yahoo Finance Pythonåº“ (yahoo-fin)"
                },
                {
                    "name": "Finnhub",
                    "url": "https://finnhub.io/api/v1/quote",
                    "type": "rest_api",
                    "description": "å®æ—¶è‚¡ç¥¨æŠ¥ä»·å’Œé‡‘èå¸‚åœºæ•°æ®"
                }
            ],
            "sports": [
                {
                    "name": "TheSportsDB",
                    "url": "https://www.thesportsdb.com/api/v1/json/1/search_all_events.php",
                    "type": "rest_api",
                    "description": "ä½“è‚²èµ›äº‹ã€çƒé˜Ÿå’Œæ¯”åˆ†æ•°æ®"
                }
            ],
            "temporal_change": [
                {
                    "name": "Google Search API",
                    "url": "https://www.googleapis.com/customsearch/v1",
                    "type": "search_api",
                    "description": "æ—¶é—´å˜åŒ–ç›¸å…³æ•°æ®æœç´¢ï¼ŒåŒ…æ‹¬å†å²æ’åã€è¶‹åŠ¿åˆ†æç­‰"
                },
                {
                    "name": "Wikipedia API",
                    "url": "https://en.wikipedia.org/api/rest_v1/page/summary/",
                    "type": "knowledge_api",
                    "description": "å†å²æ•°æ®å’ŒçŸ¥è¯†åº“ä¿¡æ¯"
                },
                {
                    "name": "Google Trends API",
                    "url": "https://trends.googleapis.com/trends/v1/",
                    "type": "rest_api",
                    "description": "è·å–è¶‹åŠ¿æ•°æ®å’Œå˜åŒ–æ¨¡å¼"
                }
            ],
            "location": [
                {
                    "name": "Google Places API (Nearby Search)",
                    "url": "https://places.googleapis.com/v1/places:searchNearby",
                    "type": "rest_api",
                    "description": "æœç´¢é™„è¿‘çš„åœ°ç‚¹/å…´è¶£ç‚¹(POI)"
                },
                {
                    "name": "Google Geocoding API",
                    "url": "https://maps.googleapis.com/maps/api/geocode/json",
                    "type": "rest_api",
                    "description": "å°†åœ°ç‚¹åç§°è§£æä¸ºåæ ‡"
                }
            ],
            "general": [
                {
                    "name": "Google Search API",
                    "url": "https://www.googleapis.com/customsearch/v1",
                    "type": "search_api",
                    "description": "é€šç”¨ç½‘é¡µæœç´¢"
                },
                {
                    "name": "Wikipedia API",
                    "url": "https://en.wikipedia.org/api/rest_v1/page/summary/",
                    "type": "knowledge_api",
                    "description": "çŸ¥è¯†åº“æ•°æ®æº"
                }
            ]
        }
        self.llm_client = llm_client
        self.use_llm = use_llm if use_llm is not None else llm_client is not None
        self.google_api_key = (google_api_key or os.getenv("GOOGLE_API_KEY") or "").strip()
        self.google_weather_base_url = google_weather_base_url.rstrip("/")
        self.google_routes_base_url = google_routes_base_url.rstrip("/")
        self.google_geocode_url = google_geocode_url
        self.request_timeout = max(3, int(request_timeout))
        self.finnhub_api_key = (finnhub_api_key or os.getenv("FINNHUB_API_KEY") or "").strip()
        
        self.sportsdb_api_key = (sportsdb_api_key or os.getenv("SPORTSDB_API_KEY") or "123").strip()
        self.apisports_api_key = (apisports_api_key or os.getenv("APISPORTS_KEY") or "").strip()
    
    def classify_domain(self, query: str, timing_recorder: Optional[TimingRecorder] = None) -> str:
        """åˆ†ç±»æŸ¥è¯¢çš„é¢†åŸŸ"""
        if self.use_llm and self.llm_client:
            domain = self._classify_with_llm(query, timing_recorder=timing_recorder)
            if domain:
                return domain
        return self._classify_with_keywords(query)

    def _classify_with_keywords(self, query: str) -> str:
        query_lower = query.lower()
        
        # ç»Ÿè®¡å„é¢†åŸŸå…³é”®è¯å‘½ä¸­æ•°
        domain_scores = {}
        for domain, keywords in self.domain_keywords.items():
            if domain == "general":
                continue
                
            score = 0
            for keyword in keywords:
                if keyword in query_lower:
                    score += 1
            domain_scores[domain] = score
        
        # æ‰¾åˆ°æœ€é«˜åˆ†çš„é¢†åŸŸ
        if domain_scores:
            best_domain = max(domain_scores.items(), key=lambda x: x[1])
            if best_domain[1] > 0:  # è‡³å°‘å‘½ä¸­ä¸€ä¸ªå…³é”®è¯
                return best_domain[0]
        
        return "general"

    def _classify_with_llm(self, query: str, timing_recorder: Optional[TimingRecorder] = None) -> Optional[str]:
        allowed = sorted(self.domain_keywords.keys())
        prompt = (
            "ä½ æ˜¯NLUåˆ†ç±»å™¨ï¼Œè¯·å°†ç”¨æˆ·é—®é¢˜å½’ç±»åˆ°å›ºå®šé¢†åŸŸä¸­ã€‚"
            "åªå…è®¸ä»¥ä¸‹æ ‡ç­¾: weather, transportation, finance, sports, temporal_change, location, general.\n"
            "- location: ç”¨äºæŸ¥æ‰¾é™„è¿‘åœ°ç‚¹ã€æœ€è¿‘çš„å•†åº—/é¤å…/è®¾æ–½ç­‰ï¼ˆå¦‚'æœ€è¿‘çš„KFC'ã€'é™„è¿‘çš„åŒ»é™¢'ï¼‰\n"
            "- temporal_change: ç”¨äºæ¶‰åŠæ—¶é—´å˜åŒ–çš„æŸ¥è¯¢ï¼Œå¦‚å†å²æ’åã€è¶‹åŠ¿åˆ†æã€å¹´åº¦å¯¹æ¯”ç­‰ï¼ˆå¦‚'æœ€è¿‘10å¹´æ’åå˜åŒ–'ã€'å†å¹´æ•°æ®å¯¹æ¯”'ï¼‰\n"
            "è¾“å‡ºä¸¥æ ¼çš„JSONï¼Œä¾‹å¦‚ {\"domain\": \"location\"}.\n\n"
            f"ç”¨æˆ·é—®é¢˜: {query}"
        )
        try:
            response_start = time.perf_counter()
            response = self.llm_client.chat(
                system_prompt="You classify intents into fixed domains.",
                user_prompt=prompt,
                max_tokens=200,
                temperature=0.0,
            )
        except Exception:
            return None
        finally:
            if timing_recorder:
                duration_ms = (time.perf_counter() - response_start) * 1000
                timing_recorder.record_llm_call(
                    label="domain_classification",
                    duration_ms=duration_ms,
                    provider=getattr(self.llm_client, "provider", None),
                    model=getattr(self.llm_client, "model_id", None),
                    extra={"stage": "source_selector"},
                )

        content = response.get("content")
        if not isinstance(content, str) or not content.strip():
            return None

        try:
            parsed = json.loads(content)
        except json.JSONDecodeError:
            start = content.find("{")
            end = content.rfind("}")
            if start == -1 or end == -1 or end <= start:
                return None
            try:
                parsed = json.loads(content[start : end + 1])
            except json.JSONDecodeError:
                return None

        if not isinstance(parsed, dict):
            return None

        domain_raw = parsed.get("domain")
        if not isinstance(domain_raw, str):
            return None
        domain = domain_raw.strip().lower()
        return domain if domain in allowed else None
    
    def select_sources(self, query: str, timing_recorder: Optional[TimingRecorder] = None) -> Tuple[str, List[Dict[str, Any]]]:
        """é€‰æ‹©æ•°æ®æº - è¿”å›å…·ä½“APIä¿¡æ¯"""
        domain = self.classify_domain(query, timing_recorder=timing_recorder)
        sources = self.domain_sources.get(domain, [
            {
                "name": "Default Search",
                "url": "https://serpapi.com/search",
                "type": "search_api",
                "description": "é»˜è®¤æœç´¢å¼•æ“"
            }
        ])
        
        try:
            print(f"query: '{query}'")
            print(f"detected domain: {domain}")
            print("selected sources:")
            for source in sources:
                url = source.get('url', 'N/A')
                print(f"   - {source['name']}: {url}")
        except (UnicodeEncodeError, UnicodeDecodeError):
            # åœ¨ä¸æ”¯æŒUTF-8çš„ç¯å¢ƒä¸­é™é»˜è·³è¿‡æ‰“å°
            pass
        
        return domain, sources

    def generate_domain_specific_query(self, query: str, domain: str) -> str:
        """æ ¹æ®è¯†åˆ«å‡ºçš„é¢†åŸŸä¸ºæŸ¥è¯¢è¡¥å……ä¸Šä¸‹æ–‡å…³é”®è¯"""
        cleaned_query = query.strip()
        domain = (domain or "general").lower()

        if not cleaned_query or domain == "general":
            return cleaned_query

        # å„é¢†åŸŸä½¿ç”¨æ›´å…·ä½“çš„å…³é”®è¯ä»¥è·å–è¯¦ç»†æ•°æ®
        domain_context = {
            "weather": "current weather forecast humidity wind speed",
            "transportation": "live traffic status transit delays road conditions",
            "finance": "latest market data stock price trend analysis",
            "sports": "box score player stats çƒå‘˜å¾—åˆ†ç»Ÿè®¡ æ¯”èµ›æ•°æ®",
            "temporal_change": "historical data trend analysis year by year comparison time series",
        }

        # ç‰¹æ®Šé¢†åŸŸå¤„ç†ï¼šæ£€æµ‹æ˜¯å¦æ˜¯æ±‚å…·ä½“æ•°æ®çš„æŸ¥è¯¢
        if domain == "sports":
            enhanced_query = self._enhance_sports_query(cleaned_query)
        elif domain == "temporal_change":
            enhanced_query = self._enhance_temporal_change_query(cleaned_query)
        else:
            supplemental_keywords = " ".join(self.domain_keywords.get(domain, [])[:3])
            enhanced_query = " ".join(
                part for part in [cleaned_query, domain_context.get(domain, ""), supplemental_keywords] if part
            )

        try:
            print(f"ğŸ§  é¢†åŸŸå¢å¼ºæŸ¥è¯¢: {enhanced_query}")
        except (UnicodeEncodeError, UnicodeDecodeError):
            # åœ¨ä¸æ”¯æŒUTF-8çš„ç¯å¢ƒä¸­é™é»˜è·³è¿‡æ‰“å°
            pass
        return enhanced_query
    
    def _enhance_sports_query(self, query: str) -> str:
        """å¢å¼ºä½“è‚²æŸ¥è¯¢ä»¥è·å–è¯¦ç»†çš„æ¯”èµ›å’Œçƒå‘˜æ•°æ®
        
        ç­–ç•¥ï¼šæ·»åŠ æ–°é—»å¯¼å‘å’Œæ•°æ®å¯¼å‘çš„ç²¾å‡†å…³é”®è¯ï¼Œè·å–æœ€æ–°æ¯”èµ›æˆ˜æŠ¥
        """
        query_lower = query.lower()
        
        # NBAé˜Ÿä¼å…³é”®è¯æ˜ å°„ï¼ˆä¸­è‹±æ–‡ï¼‰
        nba_teams = {
            "æ¹–äºº": "Lakers", "å‹‡å£«": "Warriors", "ç‹¬è¡Œä¾ ": "Mavericks",
            "ç¯®ç½‘": "Nets", "å‡¯å°”ç‰¹äºº": "Celtics", "çƒ­ç«": "Heat",
            "é›·éœ†": "Thunder", "æ˜é‡‘": "Nuggets", "å¤ªé˜³": "Suns",
            "å¿«èˆ¹": "Clippers", "å›½ç‹": "Kings", "å¼€æ‹“è€…": "Blazers",
            "ç«ç®­": "Rockets", "é©¬åˆº": "Spurs", "ç°ç†Š": "Grizzlies",
            "é¹ˆé¹•": "Pelicans", "æ£®æ—ç‹¼": "Timberwolves", "çˆµå£«": "Jazz",
            "é›„é¹¿": "Bucks", "å…¬ç‰›": "Bulls", "éª‘å£«": "Cavaliers",
            "æ´»å¡": "Pistons", "æ­¥è¡Œè€…": "Pacers", "è€é¹°": "Hawks",
            "é»„èœ‚": "Hornets", "é­”æœ¯": "Magic", "å°¼å…‹æ–¯": "Knicks",
            "76äºº": "76ers", "çŒ›é¾™": "Raptors", "å¥‡æ‰": "Wizards"
        }
        
        # æ£€æµ‹æŸ¥è¯¢ä¸­çš„NBAçƒé˜Ÿ
        detected_team = None
        detected_team_en = None
        for cn_name, en_name in nba_teams.items():
            if cn_name in query_lower or en_name.lower() in query_lower:
                detected_team = cn_name
                detected_team_en = en_name
                break
        
        # æ£€æµ‹æ˜¯å¦æ˜¯æœ€è¿‘æ¯”èµ›æŸ¥è¯¢
        recent_keywords = ["ä¸Šä¸€åœº", "æœ€è¿‘", "æœ€æ–°", "æ˜¨å¤©", "ä»Šå¤©", "æ¦‚å†µ",
                          "last", "latest", "recent", "yesterday", "today"]
        is_recent = any(kw in query_lower for kw in recent_keywords)
        
        # æ„å»ºç²¾å‡†å¢å¼º - ä½¿ç”¨æ–°é—»å¯¼å‘å…³é”®è¯è·å–æˆ˜æŠ¥è€Œéèµ›ç¨‹
        enhancements = []
        
        if detected_team and is_recent:
            # æœ€è¿‘æ¯”èµ›æŸ¥è¯¢ï¼šæ·»åŠ æ–°é—»/æˆ˜æŠ¥å¯¼å‘å…³é”®è¯
            # æ ¸å¿ƒç­–ç•¥ï¼š
            # 1. "æˆ˜æŠ¥" "highlights" è·å–æ¯”èµ›æ–°é—»è€Œéèµ›ç¨‹
            # 2. "å¾—åˆ† ç»Ÿè®¡" "box score" è·å–çƒå‘˜æ•°æ®
            # 3. æ·»åŠ è‹±æ–‡å…³é”®è¯æå‡æœç´¢è´¨é‡
            enhancements = [
                f"{detected_team_en} game highlights",  # è‹±æ–‡æ–°é—»å…³é”®è¯
                "æˆ˜æŠ¥",  # ä¸­æ–‡æ–°é—»å…³é”®è¯
                "å¾—åˆ†ç»Ÿè®¡",  # çƒå‘˜æ•°æ®å…³é”®è¯
                "box score"  # è‹±æ–‡æ•°æ®å…³é”®è¯
            ]
        elif detected_team:
            # ä¸€èˆ¬çƒé˜ŸæŸ¥è¯¢
            enhancements = [detected_team_en, "æ¯”èµ›æˆ˜æŠ¥", "score highlights"]
        elif is_recent:
            # æœ€è¿‘æ¯”èµ›ä½†æœªæŒ‡å®šçƒé˜Ÿ
            enhancements = ["æˆ˜æŠ¥", "å¾—åˆ†ç»Ÿè®¡", "highlights box score"]
        else:
            # é€šç”¨ä½“è‚²æŸ¥è¯¢
            enhancements = ["æœ€æ–°æˆ˜æŠ¥", "results highlights"]
        
        # ç»„åˆï¼šåŸå§‹æŸ¥è¯¢ + ç²¾å‡†å…³é”®è¯
        enhanced = query + " " + " ".join(enhancements)
        return enhanced
    
    def _enhance_temporal_change_query(self, query: str) -> str:
        """å¢å¼ºæ—¶é—´å˜åŒ–æŸ¥è¯¢ä»¥è·å–è¯¦ç»†çš„å†å²æ•°æ®å’Œè¶‹åŠ¿åˆ†æ
        
        ç­–ç•¥ï¼šæ·»åŠ æ—¶é—´å˜åŒ–ç›¸å…³çš„å…³é”®è¯ï¼Œè·å–å†å²æ•°æ®å’Œè¶‹åŠ¿åˆ†æ
        """
        query_lower = query.lower()
        
        # é¦™æ¸¯å¤§å­¦å…³é”®è¯æ˜ å°„ï¼ˆä¸­è‹±æ–‡ï¼‰
        hk_universities = {
            "é¦™æ¸¯ä¸­æ–‡å¤§å­¸": "CUHK", "é¦™æ¸¯ç§‘æŠ€å¤§å­¸": "HKUST", "é¦™æ¸¯å¤§å­¸": "HKU",
            "é¦™æ¸¯ä¸­æ–‡å¤§å­¦": "CUHK", "é¦™æ¸¯ç§‘æŠ€å¤§å­¦": "HKUST", "é¦™æ¸¯å¤§å­¦": "HKU",
            "ä¸­æ–‡å¤§å­¦": "CUHK", "ç§‘æŠ€å¤§å­¦": "HKUST", "é¦™æ¸¯å¤§å­¦": "HKU"
        }
        
        # æ£€æµ‹æŸ¥è¯¢ä¸­çš„é¦™æ¸¯å¤§å­¦
        detected_universities = []
        for cn_name, en_name in hk_universities.items():
            if cn_name in query_lower or en_name.lower() in query_lower:
                detected_universities.append((cn_name, en_name))
        
        # æ£€æµ‹æ˜¯å¦æ˜¯æ’åæŸ¥è¯¢
        ranking_keywords = ["æ’å", "rankings", "å¯¹æ¯”", "comparison", "æ¯”è¾ƒ", "compare"]
        is_ranking_query = any(kw in query_lower for kw in ranking_keywords)
        
        # æ£€æµ‹æ˜¯å¦æ˜¯æ—¶é—´èŒƒå›´æŸ¥è¯¢
        time_keywords = ["æœ€è¿‘10å¹´", "è¿‡å»10å¹´", "10å¹´", "åå¹´", "10 years", "decade", "å†å¹´", "å†å²", "å˜åŒ–", "è¶‹åŠ¿"]
        is_time_range_query = any(kw in query_lower for kw in time_keywords)
        
        # æ£€æµ‹æ˜¯å¦æ˜¯å¢é•¿/å˜åŒ–æŸ¥è¯¢
        growth_keywords = ["å¢é•¿", "ä¸‹é™", "æ³¢åŠ¨", "å˜åŒ–ç‡", "å¢é•¿ç‡", "æ¶¨è·Œ", "growth", "decline", "fluctuation", "rate of change"]
        is_growth_query = any(kw in query_lower for kw in growth_keywords)
        
        # æ„å»ºç²¾å‡†å¢å¼º - ä½¿ç”¨æ—¶é—´å˜åŒ–å¯¼å‘å…³é”®è¯è·å–å†å²æ•°æ®
        enhancements = []
        
        if detected_universities and is_ranking_query:
            # ç‰¹å®šå¤§å­¦æ’åæŸ¥è¯¢
            enhancements = [
                "QS World University Rankings",  # QSæ’åå…³é”®è¯
                "THE World University Rankings",  # THEæ’åå…³é”®è¯
                "ARWU Academic Ranking",  # ARWUæ’åå…³é”®è¯
                "å†å¹´æ’åå¯¹æ¯”",  # ä¸­æ–‡æ’åå¯¹æ¯”å…³é”®è¯
                "historical rankings comparison",  # è‹±æ–‡æ’åå¯¹æ¯”å…³é”®è¯
                "ranking trends",  # æ’åè¶‹åŠ¿
                "year by year ranking"  # å¹´åº¦æ’å
            ]
            if is_time_range_query:
                enhancements.extend(["å†å¹´æ•°æ®", "historical data", "trend analysis", "time series"])
        elif detected_universities and is_growth_query:
            # ç‰¹å®šå¤§å­¦å¢é•¿/å˜åŒ–æŸ¥è¯¢
            enhancements = [
                "historical performance",
                "development trends",
                "growth analysis",
                "å˜åŒ–è¶‹åŠ¿åˆ†æ",
                "å†å²è¡¨ç°"
            ]
        elif detected_universities:
            # ä¸€èˆ¬å¤§å­¦æŸ¥è¯¢
            enhancements = [
                "university profile",
                "å­¦æœ¯æ’å",
                "academic reputation",
                "æ•™è‚²è´¨é‡",
                "historical development"
            ]
        elif is_ranking_query:
            # æ’åæŸ¥è¯¢ä½†æœªæŒ‡å®šå¤§å­¦
            enhancements = [
                "QS World University Rankings",
                "THE World University Rankings",
                "ä¸–ç•Œå¤§å­¦æ’å",
                "global university rankings"
            ]
            if is_time_range_query:
                enhancements.extend(["å†å¹´å˜åŒ–", "ranking trends", "historical data", "time series analysis"])
        elif is_growth_query:
            # ä¸€èˆ¬å¢é•¿/å˜åŒ–æŸ¥è¯¢
            enhancements = [
                "historical trends",
                "trend analysis",
                "time series data",
                "historical comparison",
                "å˜åŒ–è¶‹åŠ¿",
                "å†å²å¯¹æ¯”"
            ]
        else:
            # é€šç”¨æ—¶é—´å˜åŒ–æŸ¥è¯¢
            enhancements = ["å†å²æ•°æ®", "historical data", "è¶‹åŠ¿åˆ†æ", "trend analysis"]
        
        # ç»„åˆï¼šåŸå§‹æŸ¥è¯¢ + ç²¾å‡†å…³é”®è¯
        enhanced = query + " " + " ".join(enhancements)
        return enhanced
    
    def get_source_details(self, domain: str) -> List[Dict[str, Any]]:
        """è·å–æŒ‡å®šé¢†åŸŸçš„è¯¦ç»†æ•°æ®æºä¿¡æ¯"""
        return self.domain_sources.get(domain, [])

    # === Google Cloud ä¸“ç”¨è°ƒç”¨ ===
    def fetch_domain_data(
        self,
        query: str,
        domain: str,
        timing_recorder: Optional[TimingRecorder] = None,
    ) -> Optional[Dict[str, Any]]:
        """è°ƒç”¨ç‰¹å®šé¢†åŸŸçš„Google Cloud APIå¹¶è¿”å›ç»“æ„åŒ–ç»“æœ"""
        domain = (domain or "").lower().strip()
        if domain not in {"weather", "transportation", "finance", "sports", "location"}:
            return None

        if not self.google_api_key:
            return {"handled": True, "error": "missing_google_api_key"}

        if domain == "weather":
            return self._handle_weather(query, timing_recorder=timing_recorder)
        if domain == "transportation":
            return self._handle_transportation(query, timing_recorder=timing_recorder)
        if domain == "finance":
            return self._handle_finance(query, timing_recorder=timing_recorder)
        if domain == "sports":
            return self._handle_sports(query, timing_recorder=timing_recorder)
        if domain == "location":
            return self._handle_location(query, timing_recorder=timing_recorder)

    def _handle_weather(
        self,
        query: str,
        timing_recorder: Optional[TimingRecorder],
    ) -> Dict[str, Any]:
        # æ£€æµ‹ç©ºæ°”è´¨é‡æŸ¥è¯¢
        air_quality_keywords = ["ç©ºæ°”è´¨é‡", "ç©ºæ°”æ±¡æŸ“", "AQI", "PM2.5", "PM10", "é›¾éœ¾", "ç©ºæ°”æŒ‡æ•°",
                               "air quality", "air pollution", "AQI", "PM2.5", "PM10", "smog", "haze"]
        is_air_quality_query = any(kw in query for kw in air_quality_keywords)
        
        # æ£€æµ‹é¢„æŠ¥æŸ¥è¯¢ï¼Œfallback æœç´¢
        forecast_keywords = ["æ˜å¤©", "åå¤©", "é¢„æŠ¥", "forecast", "tomorrow"]
        if any(kw in query for kw in forecast_keywords):
            return {"handled": False, "reason": "forecast_requested_fallback_search"}

        location_hint = self._extract_weather_location(query)
        if not location_hint:
            return {"handled": False, "reason": "cannot_parse_location", "skipped": True}

        geocode = self._geocode_text(location_hint, timing_recorder=timing_recorder)
        if not geocode or geocode.get("error"):
            return {
                "handled": True,
                "error": geocode.get("error") if geocode else "geocode_failed",
                "location": location_hint,
            }

        # å¦‚æœæ˜¯ç©ºæ°”è´¨é‡æŸ¥è¯¢ï¼Œä½¿ç”¨ç©ºæ°”è´¨é‡API
        if is_air_quality_query:
            # æ£€æŸ¥æ˜¯å¦æ˜¯ä¸­å›½åœ°åŒºï¼ˆGoogle Air Quality APIæ”¯æŒä¸­å›½ï¼‰
            air_quality_payload = self._call_google_air_quality(
                geocode["lat"],
                geocode["lng"],
                timing_recorder=timing_recorder,
            )
            if not air_quality_payload or air_quality_payload.get("error"):
                return {
                    "handled": True,
                    "error": air_quality_payload.get("error") if air_quality_payload else "air_quality_request_failed",
                    "location": geocode,
                }

            answer = self._format_air_quality_answer(location_hint, geocode, air_quality_payload)
            return {
                "handled": True,
                "provider": "google",
                "endpoint": "https://airquality.googleapis.com/v1/currentConditions:lookup",
                "location": geocode,
                "data": air_quality_payload,
                "answer": answer,
            }
        
        # æ™®é€šå¤©æ°”æŸ¥è¯¢
        if "ä¸­å›½" in geocode.get("formatted_address", "") or "China" in geocode.get("formatted_address", ""):
            return {"handled": True, "skipped": True, "reason": "china_location_not_supported_by_google_weather", "location": geocode}

        weather_payload = self._call_google_weather(
            geocode["lat"],
            geocode["lng"],
            timing_recorder=timing_recorder,
        )
        if not weather_payload or weather_payload.get("error"):
            return {
                "handled": True,
                "error": weather_payload.get("error") if weather_payload else "weather_request_failed",
                "location": geocode,
            }

        answer = self._format_weather_answer(location_hint, geocode, weather_payload)
        return {
            "handled": True,
            "provider": "google",
            "endpoint": f"{self.google_weather_base_url}/currentConditions:lookup",
            "location": geocode,
            "data": weather_payload,
            "answer": answer,
        }

    def _handle_transportation(
        self,
        query: str,
        timing_recorder: Optional[TimingRecorder],
    ) -> Dict[str, Any]:
        parsed = self._extract_route(query)
        if not parsed:
            return {"handled": False, "skipped": True, "reason": "cannot_parse_route"}

        origin_geo = self._geocode_text(parsed["origin"], timing_recorder=timing_recorder)
        dest_geo = self._geocode_text(parsed["destination"], timing_recorder=timing_recorder)
        if (not origin_geo or origin_geo.get("error")) or (not dest_geo or dest_geo.get("error")):
            return {
                "handled": True,
                "error": "geocode_failed",
                "origin": origin_geo or parsed.get("origin"),
                "destination": dest_geo or parsed.get("destination"),
            }

        origin_label = origin_geo.get("formatted_address") or parsed["origin"]
        dest_label = dest_geo.get("formatted_address") or parsed["destination"]

        modes = [
            {"internal": "DRIVING", "api": "DRIVE", "display": "é©¾è½¦"},
            {"internal": "TRANSIT", "api": "TRANSIT", "display": "å…¬å…±äº¤é€š"},
        ]

        routes = []
        answers = []
        for m in modes:
            route_payload = self._call_google_routes(
                origin_label,
                dest_label,
                mode=m["api"],
                timing_recorder=timing_recorder,
            )
            if route_payload and not route_payload.get("error"):
                answer = self._format_route_answer(
                    {"mode": m["internal"]}, origin_geo, dest_geo, route_payload
                )
                routes.append({
                    "mode": m["display"],
                    "data": route_payload,
                    "answer": answer
                })
                answers.append(answer)
            else:
                answers.append(f"{m['display']}ï¼šè·å–å¤±è´¥ ({route_payload.get('error') if route_payload else 'æœªçŸ¥é”™è¯¯'})")

        combined_answer = f"{origin_label} -> {dest_label}\n" + "\n".join(answers)

        return {
            "handled": True,
            "provider": "google",
            "endpoint": f"{self.google_routes_base_url}/directions/v2:computeRoutes",
            "origin": origin_geo,
            "destination": dest_geo,
            "routes": routes,
            "data": {"routes": [r["data"] for r in routes]},
            "answer": combined_answer,
        }

    def _handle_finance(
        self,
        query: str,
        timing_recorder: Optional[TimingRecorder],
    ) -> Dict[str, Any]:
        symbols = self._extract_finance_symbols(query)
        if not symbols:
            # æ— æ³•æå–ä»£ç æ—¶è·³è¿‡è€Œä¸æŠ¥é”™ï¼Œäº¤ç”±é€šç”¨æœç´¢å¤„ç†
            return {"handled": False, "reason": "cannot_parse_symbol", "skipped": True}

        # Check for historical/reasoning intent
        query_lower = query.lower()
        history_keywords = [
            "è¿‡å»",
            "éå»",
            "past",
            "days",
            "history",
            "trend",
            "å†å²",
            "æ­·å²",
            "èµ°åŠ¿",
            "èµ°å‹¢",
            "è¡¨ç°",
            "è¡¨ç¾",
            "è¿‘",
            "æœ€è¿‘",
        ]
        reasoning_keywords = ["ä¸ºä»€ä¹ˆ", "why", "reason", "cause", "news", "analysis", "åˆ†æ", "åŸå› ", "å½±å“"]
        
        is_history = any(kw in query_lower for kw in history_keywords)
        is_reasoning = any(kw in query_lower for kw in reasoning_keywords)

        # Regex to extract time period if present (e.g. "5 days", "2å¹´", "åå¹´")
        period = "1d"
        
        # Check for years first (e.g. "2å¹´", "3 years", "åå¹´")
        match_years = re.search(r'(å|(\d+))\s*(?:å¹´|years?)', query_lower)
        if match_years:
            is_history = True
            if match_years.group(1) == "å":
                years = 10
            else:
                years = int(match_years.group(2))
            
            # Map years to yfinance valid periods
            if years <= 1:
                period = "1y"
            elif years <= 2:
                period = "2y"
            elif years <= 5:
                period = "5y"
            elif years <= 10:
                period = "10y"
            else:
                period = "max"
        else:
            # Check for days (e.g. "5å¤©", "10 days")
            match_days = re.search(r'(\d+)\s*(?:å¤©|days)', query_lower)
            if match_days:
                is_history = True
                days = int(match_days.group(1))
                # Map to yfinance valid periods roughly
                if days <= 5:
                    period = "5d"
                elif days <= 30:
                    period = "1mo"
                elif days <= 90:
                    period = "3mo"
                elif days <= 180:
                    period = "6mo"
                elif days <= 365:
                    period = "1y"
                else:
                    period = "max"
            elif is_history:
                period = "1mo"  # Default history if not specified

        results = []
        for symbol in symbols:
            if is_history:
                quote = self._query_stock_history(symbol, period, timing_recorder=timing_recorder)
            else:
                quote = self._query_stock_price(symbol, timing_recorder=timing_recorder)
            
            if quote:
                quote["symbol"] = symbol
                results.append(quote)

        if not results:
             return {
                "handled": True,
                "error": "data_fetch_failed_for_all_symbols",
                "symbols": symbols,
            }

        # Identify key events for historical data
        key_events = []
        if is_history:
            for result in results:
                if not result.get("error") and "yearly_returns" in result:
                    events = self._identify_key_events(result, query)
                    if events:
                        key_events.extend(events)
        
        answer = self._format_finance_answer_multi(results, is_history)
        
        # Add key events to answer if any were identified
        if key_events:
            answer += "\n\nğŸ” **å…³é”®äº‹ä»¶åˆ†æ**:\n"
            for event in key_events:
                answer += f"   â€¢ {event}\n"
        
        return {
            "handled": True,
            "provider": "yfinance" if is_history else "finnhub/yfinance",
            "endpoint": "yfinance.history" if is_history else "quote",
            "symbols": symbols,
            "data": results,
            "answer": answer,
            "key_events": key_events,
            # If the user asks for reasons/analysis, we must continue to the main search pipeline
            # to retrieve news/web content, while providing the data we found as context.
            "continue_search": is_reasoning
        }

    def _extract_finance_symbols(self, query: str) -> List[str]:
        """Extract multiple finance symbols from query.
        
        Uses a multi-step approach:
        1. Check predefined mappings (indices, crypto, company names)
        2. Use regex patterns to find potential symbols
        3. Use LLM to identify company names and get stock symbols (if available)
        4. Use Google Search as fallback to find stock symbols for unknown companies
        """
        symbols = set()
        query_upper = query.upper()
        
        # 1. Check specific maps first
        index_map = {
            "HANG SENG": "^HSI", "æ’ç”Ÿ": "^HSI",
            "NASDAQ": "^IXIC", "çº³æ–¯è¾¾å…‹": "^IXIC",
            "DOW JONES": "^DJI", "é“ç¼æ–¯": "^DJI",
            "S&P 500": "^GSPC", "æ ‡æ™®500": "^GSPC",
        }
        for name, sym in index_map.items():
            if name in query_upper:
                symbols.add(sym)

        # 2. Crypto Map (including Chinese names)
        crypto_map = {
            "BTC": "BTC-USD",
            "BITCOIN": "BTC-USD",
            "æ¯”ç‰¹å¸": "BTC-USD",
            "æ¯”ç‰¹å¹£": "BTC-USD",
            "ETH": "ETH-USD",
            "ETHEREUM": "ETH-USD",
            "ä»¥å¤ªåŠ": "ETH-USD",
            "ä»¥å¤ªå¹£": "ETH-USD",
            "DOGE": "DOGE-USD",
            "ç‹—ç‹—å¸": "DOGE-USD",
            "ç‹—ç‹—å¹£": "DOGE-USD",
            "SOL": "SOL-USD",
            "SOLANA": "SOL-USD",
            "ç´¢æ‹‰çº³": "SOL-USD",
            "XRP": "XRP-USD",
            "ç‘æ³¢å¸": "XRP-USD",
            "ç‘æ³¢å¹£": "XRP-USD",
            "ADA": "ADA-USD",
            "è‰¾è¾¾å¸": "ADA-USD",
            "DOT": "DOT-USD",
            "æ³¢å¡": "DOT-USD",
            "MATIC": "MATIC-USD",
            "AVAX": "AVAX-USD",
            "LINK": "LINK-USD",
            "UNI": "UNI-USD",
        }
        # Check both uppercase and original query for crypto
        for name, sym in crypto_map.items():
            if name in query_upper or name in query:
                symbols.add(sym)

        # 2.5. Chinese Company Name Map
        chinese_company_map = {
            "è‹¹æœ": "AAPL",
            "è‹¹æœå…¬å¸": "AAPL",
            "å¾®è½¯": "MSFT",
            "å¾®è½¯å…¬å¸": "MSFT",
            "è°·æ­Œ": "GOOGL",
            "è°·æ­Œå…¬å¸": "GOOGL",
            "äºšé©¬é€Š": "AMZN",
            "äºšé©¬é€Šå…¬å¸": "AMZN",
            "Amazon": "AMZN",
            "ç‰¹æ–¯æ‹‰": "TSLA",
            "ç‰¹æ–¯æ‹‰å…¬å¸": "TSLA",
            "è„¸ä¹¦": "META",
            "è„¸ä¹¦å…¬å¸": "META",
            "Meta": "META",
            "è‹±ä¼Ÿè¾¾": "NVDA",
            "è‹±ä¼Ÿè¾¾å…¬å¸": "NVDA",
            # Intel å’Œ AMD
            "è‹±ç‰¹å°”": "INTC",
            "è‹±ç‰¹å°”å…¬å¸": "INTC",
            "Intel": "INTC",
            "INTEL": "INTC",
            "è¶…å¨åŠå¯¼ä½“": "AMD",
            "è¶…å¾®åŠå¯¼ä½“": "AMD",
            "AMDå…¬å¸": "AMD",
            # å…¶ä»–å…¬å¸
            "é˜¿é‡Œå·´å·´": "BABA",
            "é˜¿é‡Œå·´å·´é›†å›¢": "BABA",
            "è…¾è®¯": "0700.HK",
            "è…¾è®¯æ§è‚¡": "0700.HK",
            "å°ç§¯ç”µ": "TSM",
            "å°ç§¯ç”µå…¬å¸": "TSM",
            "æ¯”äºšè¿ª": "BYD",
            "æ¯”äºšè¿ªå…¬å¸": "BYD",
            "èŒ…å°": "600519.SS",
            "è´µå·èŒ…å°": "600519.SS",
            "ä¸­å›½å¹³å®‰": "601318.SS",
            "å¹³å®‰": "601318.SS",
            "ä¸­å›½ç§»åŠ¨": "0941.HK",
            "ä¸­å›½è”é€š": "600050.SS",
            "ä¸­å›½ç”µä¿¡": "601728.SS",
            "äº¬ä¸œ": "JD",
            "äº¬ä¸œé›†å›¢": "JD",
            "ç™¾åº¦": "BIDU",
            "ç™¾åº¦å…¬å¸": "BIDU",
            "ç½‘æ˜“": "NTES",
            "ç½‘æ˜“å…¬å¸": "NTES",
            "å°ç±³": "1810.HK",
            "å°ç±³é›†å›¢": "1810.HK",
            "ç¾å›¢": "3690.HK",
            "ç¾å›¢ç‚¹è¯„": "3690.HK",
            "æ‹¼å¤šå¤š": "PDD",
            "æ‹¼å¤šå¤šå…¬å¸": "PDD",
            "è”šæ¥": "NIO",
            "è”šæ¥æ±½è½¦": "NIO",
            "ç†æƒ³æ±½è½¦": "LI",
            "å°é¹æ±½è½¦": "XPEV",
            "é«˜é€š": "QCOM",
            "é«˜é€šå…¬å¸": "QCOM",
            "åšé€š": "AVGO",
            "åšé€šå…¬å¸": "AVGO",
            "æ ‡æ™®500": "^GSPC",
            "æ ‡æ™®": "^GSPC",
            "é“ç¼æ–¯": "^DJI",
            "é“æŒ‡": "^DJI",
            "çº³æ–¯è¾¾å…‹": "^IXIC",
            "çº³æŒ‡": "^IXIC",
            "æ’ç”ŸæŒ‡æ•°": "^HSI",
            "æ’æŒ‡": "^HSI",
        }
        
        # Check for Chinese company names
        for name, sym in chinese_company_map.items():
            if name in query:
                symbols.add(sym)

        # 3. Regex for Stock Symbols
        
        # Matches: (NVDA), (AMD)
        matches_paren = re.findall(r'\(([A-Z]{1,5})\)', query_upper)
        symbols.update(matches_paren)
        
        # Matches: 6 digits (CN/HK codes)
        matches_digits = re.findall(r'(?<!\d)\d{6}(?!\d)', query)
        symbols.update(matches_digits)

        # Matches: NVDA, AMD, AMAZON (2-6 letters)
        # Improved regex to handle mixed language boundaries and case insensitivity
        # Look for 2-6 letter words not surrounded by other letters
        candidates = re.findall(r'(?<![a-zA-Z])[a-zA-Z]{2,6}(?![a-zA-Z])', query)
        
        stopwords = {
            "AND", "THE", "FOR", "WHO", "WHY", "USD", "HKD", "RMB",
            "STOCK", "PRICE", "DAYS", "PAST", "COMPARE", "WITH", "FROM", "TO",
            "WHAT", "WHEN", "WHERE", "HOW", "IS", "ARE", "WAS", "WERE",
            "YEAR", "MONTH", "WEEK", "DAY", "TODAY", "NOW", "NEWS",
            "ANALYSIS", "TREND", "HISTORY", "PERFORMANCE", "VS", "OR",
            "TOP", "BEST", "NEW", "OLD", "BIG", "SMALL", "BUY", "SELL",
            "OF", "IN", "ON", "AT", "BY", "IT", "AS", "IF", "DO", "GO",
            "MY", "ME", "WE", "UP", "SO", "NO", "KEY", "MODEL", "DATA",
            "INFO", "SHOW", "TELL", "GIVE", "GET", "SET", "RUN", "USE",
            "TRY", "ASK", "SAY", "SEE", "SAW", "LOT", "BIT", "PUT", "LET",
            "ANY", "ALL", "ONE", "TWO", "SIX", "TEN", "HAS", "HAD", "NOT",
            "BUT", "CAN", "MAY", "OUT", "OFF", "TAX", "LAW", "ACT", "ART",
            "MAP", "APP", "WEB", "NET", "COM", "ORG", "EDU", "GOV", "MIL",
            "INT", "DESCRIBE", "EXPLAIN", "ABOUT",
            # Date/time related words that should not be treated as stock symbols
            "DATE", "TIME", "CURRENT", "UTC", "ISO",
            # Company name words that are not stock symbols themselves
            "INTEL", "NVIDIA", "GOOGLE", "APPLE", "AMAZON", "TESLA",
        }
        
        for m in candidates:
            if m.upper() not in stopwords:
                symbols.add(m.upper())

        # 4. If no symbols found, try LLM extraction
        if not symbols and self.use_llm and self.llm_client:
            llm_symbols = self._extract_symbols_with_llm(query)
            if llm_symbols:
                symbols.update(llm_symbols)
        
        # 5. If still no symbols, try Google Search fallback
        if not symbols and self.google_api_key:
            search_symbols = self._extract_symbols_with_search(query)
            if search_symbols:
                symbols.update(search_symbols)

        return list(symbols)
    
    def _extract_symbols_with_llm(self, query: str) -> List[str]:
        """Use LLM to extract company names and convert to stock symbols."""
        if not self.llm_client:
            return []
        
        prompt = (
            "ä»ç”¨æˆ·çš„é‡‘èæŸ¥è¯¢ä¸­æå–æ‰€æœ‰å…¬å¸åç§°ï¼Œå¹¶è¿”å›å®ƒä»¬å¯¹åº”çš„è‚¡ç¥¨ä»£ç ã€‚\n"
            "è¾“å‡ºJSONæ ¼å¼ï¼Œä¾‹å¦‚ï¼š{\"symbols\": [\"AAPL\", \"MSFT\"]}\n"
            "è§„åˆ™ï¼š\n"
            "- ç¾è‚¡ä½¿ç”¨æ ‡å‡†ä»£ç ï¼ˆå¦‚AAPL, MSFT, GOOGLï¼‰\n"
            "- æ¸¯è‚¡ä½¿ç”¨.HKåç¼€ï¼ˆå¦‚0700.HKï¼‰\n"
            "- Aè‚¡ä½¿ç”¨.SSï¼ˆä¸Šæµ·ï¼‰æˆ–.SZï¼ˆæ·±åœ³ï¼‰åç¼€ï¼ˆå¦‚600519.SSï¼‰\n"
            "- å¦‚æœæ— æ³•ç¡®å®šè‚¡ç¥¨ä»£ç ï¼Œè¿”å›ç©ºæ•°ç»„\n"
            "- åªè¿”å›ç¡®å®šçš„è‚¡ç¥¨ä»£ç ï¼Œä¸è¦çŒœæµ‹\n\n"
            f"ç”¨æˆ·æŸ¥è¯¢ï¼š{query}"
        )
        
        try:
            response = self.llm_client.chat(
                system_prompt="You are a financial assistant that extracts stock symbols from queries.",
                user_prompt=prompt,
                max_tokens=200,
                temperature=0.0,
            )
            content = response.get("content", "{}")
            
            # Try to parse JSON
            try:
                parsed = json.loads(content)
            except json.JSONDecodeError:
                # Try to extract JSON from response
                start = content.find("{")
                end = content.rfind("}")
                if start != -1 and end != -1 and end > start:
                    try:
                        parsed = json.loads(content[start:end+1])
                    except json.JSONDecodeError:
                        return []
                else:
                    return []
            
            symbols = parsed.get("symbols", [])
            if isinstance(symbols, list):
                # Validate symbols format
                valid_symbols = []
                for sym in symbols:
                    if isinstance(sym, str) and sym.strip():
                        sym = sym.strip().upper()
                        # Basic validation: alphanumeric with optional suffix
                        if re.match(r'^[A-Z0-9\^]{1,6}(\.[A-Z]{1,2})?$', sym):
                            valid_symbols.append(sym)
                return valid_symbols
        except Exception as exc:
            try:
                print(f"[LLM Symbol Extraction] Error: {exc}")
            except (UnicodeEncodeError, UnicodeDecodeError):
                pass
        
        return []
    
    def _extract_symbols_with_search(self, query: str) -> List[str]:
        """Use Google Search to find stock symbols for companies mentioned in query."""
        if not self.google_api_key:
            return []
        
        # Extract potential company names from query
        # Look for capitalized words or Chinese company patterns
        company_patterns = [
            r'([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)',  # Capitalized words like "Intel Corporation"
            r'([\u4e00-\u9fff]{2,}(?:å…¬å¸|é›†å›¢|æ§è‚¡|ç§‘æŠ€|ç”µå­|åŠå¯¼ä½“)?)',  # Chinese company names
        ]
        
        potential_companies = []
        for pattern in company_patterns:
            matches = re.findall(pattern, query)
            potential_companies.extend(matches)
        
        if not potential_companies:
            return []
        
        symbols = []
        for company in potential_companies[:3]:  # Limit to 3 companies to avoid too many API calls
            symbol = self._search_stock_symbol(company)
            if symbol:
                symbols.append(symbol)
        
        return symbols
    
    def _search_stock_symbol(self, company_name: str) -> Optional[str]:
        """Search for a company's stock symbol using Google Search."""
        if not self.google_api_key:
            return None
        
        # Construct search query
        search_query = f"{company_name} stock symbol ticker"
        
        try:
            # Use Google Custom Search API
            params = {
                "key": self.google_api_key,
                "cx": os.getenv("GOOGLE_CX", ""),  # Custom Search Engine ID
                "q": search_query,
                "num": 3,
            }
            
            # Skip if no CX configured
            if not params["cx"]:
                return self._search_stock_symbol_simple(company_name)
            
            response = requests.get(
                "https://www.googleapis.com/customsearch/v1",
                params=params,
                timeout=self.request_timeout,
            )
            response.raise_for_status()
            data = response.json()
            
            items = data.get("items", [])
            for item in items:
                title = item.get("title", "")
                snippet = item.get("snippet", "")
                combined = f"{title} {snippet}"
                
                # Look for stock symbol patterns in results
                # Pattern: (SYMBOL) or SYMBOL: or ticker: SYMBOL
                symbol_patterns = [
                    r'\(([A-Z]{1,5})\)',  # (AAPL)
                    r'(?:ticker|symbol|stock)[\s:]+([A-Z]{1,5})',  # ticker: AAPL
                    r'([A-Z]{1,5})(?:\s+stock|\s+shares)',  # AAPL stock
                ]
                
                for pattern in symbol_patterns:
                    match = re.search(pattern, combined, re.IGNORECASE)
                    if match:
                        symbol = match.group(1).upper()
                        # Validate it's a real symbol by checking with yfinance
                        if self._validate_symbol(symbol):
                            return symbol
            
        except Exception as exc:
            try:
                print(f"[Google Search Symbol] Error: {exc}")
            except (UnicodeEncodeError, UnicodeDecodeError):
                pass
        
        return None
    
    def _search_stock_symbol_simple(self, company_name: str) -> Optional[str]:
        """Simple fallback to search stock symbol using yfinance search."""
        try:
            import yfinance as yf
            
            # Try common variations
            variations = [
                company_name,
                company_name.replace(" ", ""),
                company_name.split()[0] if " " in company_name else company_name,
            ]
            
            for variation in variations:
                try:
                    # Try to get ticker info directly
                    ticker = yf.Ticker(variation)
                    info = ticker.info
                    if info and info.get("symbol"):
                        return info["symbol"]
                except Exception:
                    continue
            
            # Try yfinance search (if available)
            try:
                # yfinance doesn't have a direct search API, but we can try common patterns
                # For well-known companies, the symbol is often the first few letters
                if len(company_name) >= 2:
                    potential_symbol = company_name[:4].upper()
                    ticker = yf.Ticker(potential_symbol)
                    info = ticker.info
                    if info and info.get("symbol") and info.get("shortName"):
                        # Verify the company name matches
                        if company_name.lower() in info.get("shortName", "").lower():
                            return info["symbol"]
            except Exception:
                pass
                
        except Exception as exc:
            try:
                print(f"[yfinance Symbol Search] Error: {exc}")
            except (UnicodeEncodeError, UnicodeDecodeError):
                pass
        
        return None
    
    def _validate_symbol(self, symbol: str) -> bool:
        """Validate if a symbol is a real stock symbol using yfinance."""
        try:
            import yfinance as yf
            ticker = yf.Ticker(symbol)
            info = ticker.info
            # Check if we got valid data
            return bool(info and (info.get("symbol") or info.get("shortName")))
        except Exception:
            return False

    def _identify_key_events(self, stock_data: Dict[str, Any], query: str) -> List[str]:
        """Identify key events based on stock performance data."""
        events = []
        symbol = stock_data.get("symbol", "Unknown")
        
        # Analyze yearly returns for significant events
        yearly_returns = stock_data.get("yearly_returns", [])
        if yearly_returns:
            # Find best and worst years
            best_year = max(yearly_returns, key=lambda x: x["return"])
            worst_year = min(yearly_returns, key=lambda x: x["return"])
            
            if best_year["return"] > 30:
                events.append(f"{best_year['year']}å¹´è¡¨ç°å¼ºåŠ²ï¼Œä¸Šæ¶¨{best_year['return']:.2f}%")
            
            if worst_year["return"] < -20:
                events.append(f"{worst_year['year']}å¹´è¡¨ç°ç–²è½¯ï¼Œä¸‹è·Œ{abs(worst_year['return']):.2f}%")
            
            # Check for consecutive up/down years
            consecutive_up = 0
            consecutive_down = 0
            max_consecutive_up = 0
            max_consecutive_down = 0
            
            for i, year_data in enumerate(yearly_returns):
                if year_data["return"] > 0:
                    consecutive_up += 1
                    consecutive_down = 0
                    max_consecutive_up = max(max_consecutive_up, consecutive_up)
                else:
                    consecutive_down += 1
                    consecutive_up = 0
                    max_consecutive_down = max(max_consecutive_down, consecutive_down)
            
            if max_consecutive_up >= 3:
                events.append(f"æ›¾è¿ç»­{max_consecutive_up}å¹´ä¸Šæ¶¨ï¼Œè¡¨ç°æŒç»­å‘å¥½")
            
            if max_consecutive_down >= 2:
                events.append(f"æ›¾è¿ç»­{max_consecutive_down}å¹´ä¸‹è·Œï¼Œé¢ä¸´è°ƒæ•´å‹åŠ›")
        
        # Analyze volatility
        volatility = stock_data.get("volatility")
        if volatility is not None:
            if volatility > 40:
                events.append("ä»·æ ¼æ³¢åŠ¨æå¤§ï¼Œå¸‚åœºæƒ…ç»ªä¸ç¨³å®š")
            elif volatility > 30:
                events.append("ä»·æ ¼æ³¢åŠ¨è¾ƒå¤§ï¼ŒæŠ•èµ„é£é™©è¾ƒé«˜")
        
        # Analyze drawdown
        max_drawdown = stock_data.get("max_drawdown")
        if max_drawdown is not None and max_drawdown < -40:
            events.append(f"æ›¾ç»å†å¤§å¹…å›æ’¤({max_drawdown:.2f}%)ï¼Œéœ€æ³¨æ„é£é™©æ§åˆ¶")
        
        # Analyze trend and momentum
        trend = stock_data.get("trend_direction", "")
        momentum = stock_data.get("momentum")
        momentum_desc = stock_data.get("momentum_desc", "")
        
        if "å¼ºåŠ²ä¸Šæ¶¨" in trend and momentum and momentum > 10:
            events.append("å½“å‰å‘ˆç°å¼ºåŠ²ä¸Šæ¶¨è¶‹åŠ¿ï¼ŒçŸ­æœŸåŠ¨é‡å……è¶³")
        elif "å¤§å¹…ä¸‹è·Œ" in trend and momentum and momentum < -10:
            events.append("è¿‘æœŸè¡¨ç°ç–²è½¯ï¼ŒçŸ­æœŸåŠ¨é‡ä¸è¶³")
        
        # Check for moving average signals
        ma_20 = stock_data.get("ma_20")
        ma_50 = stock_data.get("ma_50")
        ma_200 = stock_data.get("ma_200")
        current_price = stock_data.get("end_price")
        
        if all(x is not None for x in [ma_20, ma_50, ma_200, current_price]):
            if current_price > ma_20 > ma_50 > ma_200:
                events.append("æŠ€æœ¯é¢å‘ˆç°å¤šå¤´æ’åˆ—ï¼Œé•¿æœŸè¶‹åŠ¿å‘å¥½")
            elif current_price < ma_20 < ma_50 < ma_200:
                events.append("æŠ€æœ¯é¢å‘ˆç°ç©ºå¤´æ’åˆ—ï¼Œé•¿æœŸè¶‹åŠ¿å‘æ·¡")
        
        # Add symbol-specific insights if available
        symbol_insights = {
            "^GSPC": "æ ‡æ™®500æŒ‡æ•°",
            "^DJI": "é“ç¼æ–¯æŒ‡æ•°",
            "^IXIC": "çº³æ–¯è¾¾å…‹æŒ‡æ•°",
            "^HSI": "æ’ç”ŸæŒ‡æ•°",
            "AAPL": "è‹¹æœå…¬å¸",
            "MSFT": "å¾®è½¯å…¬å¸",
            "GOOGL": "è°·æ­Œå…¬å¸",
            "TSLA": "ç‰¹æ–¯æ‹‰å…¬å¸",
            "AMZN": "äºšé©¬é€Šå…¬å¸",
        }
        
        symbol_name = symbol_insights.get(symbol, symbol)
        if symbol_name != symbol:
            events.insert(0, f"åˆ†æå¯¹è±¡ï¼š{symbol_name}({symbol})")
        
        return events

    def _query_stock_history(self, symbol: str, period: str, timing_recorder: Optional[TimingRecorder] = None) -> Dict[str, Any]:
        """Fetch historical data using yfinance with technical indicators."""
        start = time.perf_counter()
        try:
            # Ensure yfinance is available
            import yfinance as yf
            import numpy as np
            
            ticker = yf.Ticker(symbol)
            # Fetch history
            df = ticker.history(period=period)
            if df.empty:
                return {"error": "no_history_data"}
            
            # Calculate basic stats
            # Start/End close
            if len(df) >= 2:
                start_price = df.iloc[0]['Close']
                end_price = df.iloc[-1]['Close']
                change = end_price - start_price
                pct_change = (change / start_price) * 100
                
                # Calculate technical indicators
                closes = df['Close'].values
                
                # 1. Moving averages
                ma_20 = None
                ma_50 = None
                ma_200 = None
                
                if len(closes) >= 20:
                    ma_20 = np.mean(closes[-20:])
                if len(closes) >= 50:
                    ma_50 = np.mean(closes[-50:])
                if len(closes) >= 200:
                    ma_200 = np.mean(closes[-200:])
                
                # 2. Volatility (standard deviation of daily returns)
                daily_returns = np.diff(closes) / closes[:-1]
                volatility = np.std(daily_returns) * np.sqrt(252) * 100  # Annualized volatility
                
                # 3. Maximum drawdown
                cumulative_returns = np.cumprod(1 + daily_returns)
                running_max = np.maximum.accumulate(cumulative_returns)
                drawdowns = (cumulative_returns - running_max) / running_max
                max_drawdown = np.min(drawdowns) * 100
                
                # 4. Yearly returns (if data spans multiple years)
                yearly_returns = []
                if len(df) > 365:
                    df_years = df.groupby(df.index.year)
                    for year, group in df_years:
                        if len(group) > 1:  # Need at least 2 days to calculate return
                            year_start = group.iloc[0]['Close']
                            year_end = group.iloc[-1]['Close']
                            year_return = ((year_end - year_start) / year_start) * 100
                            yearly_returns.append({"year": year, "return": year_return})
                
                # 5. Trend analysis
                trend_direction = "æ¨ªç›˜"
                if pct_change > 20:
                    trend_direction = "å¼ºåŠ²ä¸Šæ¶¨"
                elif pct_change > 10:
                    trend_direction = "æ¸©å’Œä¸Šæ¶¨"
                elif pct_change < -20:
                    trend_direction = "å¤§å¹…ä¸‹è·Œ"
                elif pct_change < -10:
                    trend_direction = "æ¸©å’Œä¸‹è·Œ"
                
                # 6. Price momentum (recent 30 days vs previous 30 days)
                momentum = None
                momentum_desc = "æ— æ³•è®¡ç®—"
                if len(closes) >= 60:
                    recent_30 = np.mean(closes[-30:])
                    prev_30 = np.mean(closes[-60:-30])
                    momentum = ((recent_30 - prev_30) / prev_30) * 100
                    if momentum > 5:
                        momentum_desc = "å¼ºåŠ²ä¸Šå‡"
                    elif momentum > 2:
                        momentum_desc = "æ¸©å’Œä¸Šå‡"
                    elif momentum < -5:
                        momentum_desc = "å¿«é€Ÿä¸‹é™"
                    elif momentum < -2:
                        momentum_desc = "æ¸©å’Œä¸‹é™"
                    else:
                        momentum_desc = "åŸºæœ¬ç¨³å®š"
                
                # Serialize a summary with technical indicators
                history_summary = {
                    "start_date": str(df.index[0].date()),
                    "end_date": str(df.index[-1].date()),
                    "start_price": start_price,
                    "end_price": end_price,
                    "change": change,
                    "pct_change": pct_change,
                    "high": df['High'].max(),
                    "low": df['Low'].min(),
                    "volatility": volatility,
                    "max_drawdown": max_drawdown,
                    "trend_direction": trend_direction,
                    "ma_20": ma_20,
                    "ma_50": ma_50,
                    "ma_200": ma_200,
                    "momentum": momentum,
                    "momentum_desc": momentum_desc,
                    "yearly_returns": yearly_returns,
                    "daily_data": [
                        {"date": str(idx.date()), "close": row['Close'], "volume": row['Volume']}
                        for idx, row in df.iterrows()
                    ]
                }
                return history_summary
            else:
                return {"error": "insufficient_history_data"}

        except Exception as exc:
            return {"error": str(exc)}
        finally:
            if timing_recorder:
                duration_ms = (time.perf_counter() - start) * 1000
                timing_recorder.record_search_timing(
                    source="yfinance_history",
                    label="yfinance History",
                    duration_ms=duration_ms,
                )

    def _format_finance_answer_multi(self, results: List[Dict[str, Any]], is_history: bool) -> str:
        parts = []
        for res in results:
            sym = res.get("symbol", "Unknown")
            if res.get("error"):
                parts.append(f"{sym}: æ•°æ®è·å–å¤±è´¥ ({res['error']})")
                continue
            
            if is_history:
                # Format historical summary with enhanced analysis
                pct = res.get("pct_change", 0)
                sign = "+" if pct >= 0 else ""
                
                s_p = res.get('start_price')
                e_p = res.get('end_price')
                l_p = res.get('low')
                h_p = res.get('high')
                
                s_str = f"{s_p:.2f}" if isinstance(s_p, (int, float)) else "N/A"
                e_str = f"{e_p:.2f}" if isinstance(e_p, (int, float)) else "N/A"
                l_str = f"{l_p:.2f}" if isinstance(l_p, (int, float)) else "N/A"
                h_str = f"{h_p:.2f}" if isinstance(h_p, (int, float)) else "N/A"

                # Enhanced analysis with technical indicators
                analysis_parts = []
                
                # Basic performance
                analysis_parts.append(
                    f"ğŸ“Š **{sym}** ({res.get('start_date', '?')} è‡³ {res.get('end_date', '?')}):\n"
                    f"   - æ¶¨è·Œå¹…: {sign}{pct:.2f}%\n"
                    f"   - æ”¶ç›˜ä»·: {s_str} -> {e_str}\n"
                    f"   - æœŸé—´æ³¢åŠ¨: {l_str} - {h_str}"
                )
                
                # Trend analysis
                trend = res.get("trend_direction", "æœªçŸ¥")
                analysis_parts.append(f"\n   - è¶‹åŠ¿åˆ†æ: {trend}")
                
                # Volatility and risk
                volatility = res.get("volatility")
                if volatility is not None:
                    vol_level = "ä½" if volatility < 15 else "ä¸­" if volatility < 30 else "é«˜"
                    analysis_parts.append(f"\n   - å¹´åŒ–æ³¢åŠ¨ç‡: {volatility:.2f}% ({vol_level}é£é™©)")
                
                # Maximum drawdown
                max_dd = res.get("max_drawdown")
                if max_dd is not None:
                    analysis_parts.append(f"\n   - æœ€å¤§å›æ’¤: {max_dd:.2f}%")
                
                # Moving averages
                ma_20 = res.get("ma_20")
                ma_50 = res.get("ma_50")
                ma_200 = res.get("ma_200")
                
                if ma_20 is not None:
                    analysis_parts.append(f"\n   - 20æ—¥å‡çº¿: {ma_20:.2f}")
                    if e_p is not None:
                        ma20_signal = "ä¸Šæ–¹" if e_p > ma_20 else "ä¸‹æ–¹"
                        analysis_parts.append(f"     (å½“å‰ä»·æ ¼åœ¨20æ—¥å‡çº¿{ma20_signal})")
                
                if ma_50 is not None:
                    analysis_parts.append(f"\n   - 50æ—¥å‡çº¿: {ma_50:.2f}")
                    if e_p is not None:
                        ma50_signal = "ä¸Šæ–¹" if e_p > ma_50 else "ä¸‹æ–¹"
                        analysis_parts.append(f"     (å½“å‰ä»·æ ¼åœ¨50æ—¥å‡çº¿{ma50_signal})")
                
                if ma_200 is not None:
                    analysis_parts.append(f"\n   - 200æ—¥å‡çº¿: {ma_200:.2f}")
                    if e_p is not None:
                        ma200_signal = "ä¸Šæ–¹" if e_p > ma_200 else "ä¸‹æ–¹"
                        analysis_parts.append(f"     (å½“å‰ä»·æ ¼åœ¨200æ—¥å‡çº¿{ma200_signal})")
                
                # Momentum
                momentum = res.get("momentum")
                momentum_desc = res.get("momentum_desc")
                if momentum is not None and momentum_desc is not None:
                    analysis_parts.append(f"\n   - è¿‘æœŸåŠ¨é‡: {momentum_desc} ({momentum:.2f}%)")
                
                # Yearly returns
                yearly_returns = res.get("yearly_returns", [])
                if yearly_returns:
                    analysis_parts.append("\n   - å¹´åº¦æ”¶ç›Šç‡:")
                    for yr in yearly_returns:
                        yr_sign = "+" if yr["return"] >= 0 else ""
                        analysis_parts.append(f"     {yr['year']}å¹´: {yr_sign}{yr['return']:.2f}%")
                
                # Key insights summary
                insights = []
                if pct > 20:
                    insights.append("æ•´ä½“è¡¨ç°å¼ºåŠ²ï¼Œæ˜¾è‘—ä¸Šæ¶¨")
                elif pct < -20:
                    insights.append("æ•´ä½“è¡¨ç°ç–²è½¯ï¼Œæ˜¾è‘—ä¸‹è·Œ")
                
                if volatility is not None:
                    if volatility > 30:
                        insights.append("ä»·æ ¼æ³¢åŠ¨è¾ƒå¤§ï¼ŒæŠ•èµ„é£é™©è¾ƒé«˜")
                    elif volatility < 15:
                        insights.append("ä»·æ ¼ç›¸å¯¹ç¨³å®šï¼ŒæŠ•èµ„é£é™©è¾ƒä½")
                
                if max_dd is not None and max_dd < -30:
                    insights.append("æ›¾ç»å†è¾ƒå¤§å›æ’¤ï¼Œéœ€æ³¨æ„é£é™©æ§åˆ¶")
                
                if ma_20 is not None and ma_50 is not None and ma_200 is not None:
                    if e_p > ma_20 > ma_50 > ma_200:
                        insights.append("æŠ€æœ¯é¢å‘ˆç°å¤šå¤´æ’åˆ—ï¼Œè¶‹åŠ¿å‘å¥½")
                    elif e_p < ma_20 < ma_50 < ma_200:
                        insights.append("æŠ€æœ¯é¢å‘ˆç°ç©ºå¤´æ’åˆ—ï¼Œè¶‹åŠ¿å‘æ·¡")
                
                if insights:
                    analysis_parts.append("\n   - å…³é”®æ´å¯Ÿ:")
                    for insight in insights:
                        analysis_parts.append(f"     â€¢ {insight}")
                
                parts.append("".join(analysis_parts))
            else:
                # Format current quote (reuse logic or simple)
                c = res.get("c") or res.get("currentPrice")
                parts.append(f"ğŸ“ˆ **{sym}** ç°ä»·: {c}")

        return "\n\n".join(parts)


    def _handle_location(
        self,
        query: str,
        timing_recorder: Optional[TimingRecorder] = None,
    ) -> Dict[str, Any]:
        """å¤„ç†åœ°ç‚¹/POIæœç´¢æŸ¥è¯¢ï¼Œå¦‚"è·ç¦»HKUSTæœ€è¿‘çš„KFCæ˜¯å“ªå®¶"""
        # æå–å‚è€ƒåœ°ç‚¹å’Œç›®æ ‡ç±»å‹
        parsed = self._extract_location_query(query)
        if not parsed:
            return {"handled": False, "reason": "cannot_parse_location_query", "skipped": True}
        
        reference_location = parsed.get("reference_location")
        target_type = parsed.get("target_type")
        
        if not reference_location or not target_type:
            return {"handled": False, "reason": "missing_reference_or_target", "skipped": True}
        
        # è·å–å‚è€ƒåœ°ç‚¹çš„åæ ‡
        geocode = self._geocode_text(reference_location, timing_recorder=timing_recorder)
        if not geocode or geocode.get("error"):
            return {
                "handled": True,
                "error": geocode.get("error") if geocode else "geocode_failed",
                "reference_location": reference_location,
            }
        
        lat = geocode.get("lat")
        lng = geocode.get("lng")
        
        if lat is None or lng is None:
            return {
                "handled": True,
                "error": "invalid_coordinates",
                "reference_location": reference_location,
            }
        
        # ä½¿ç”¨ Google Places API æœç´¢é™„è¿‘åœ°ç‚¹
        places_result = self._call_google_places_nearby(
            lat=lat,
            lng=lng,
            keyword=target_type,
            timing_recorder=timing_recorder,
        )
        
        if not places_result or places_result.get("error"):
            return {
                "handled": True,
                "error": places_result.get("error") if places_result else "places_search_failed",
                "reference_location": geocode,
                "target_type": target_type,
            }
        
        places = places_result.get("places", [])
        if not places:
            return {
                "handled": True,
                "error": "no_places_found",
                "reference_location": geocode,
                "target_type": target_type,
                "answer": f"åœ¨ {geocode.get('formatted_address', reference_location)} é™„è¿‘æœªæ‰¾åˆ° {target_type}ã€‚",
            }
        
        # æ ¼å¼åŒ–ç­”æ¡ˆ
        answer = self._format_location_answer(
            reference_location=reference_location,
            geocode=geocode,
            target_type=target_type,
            places=places,
        )
        
        return {
            "handled": True,
            "provider": "google_places",
            "endpoint": "https://places.googleapis.com/v1/places:searchNearby",
            "reference_location": geocode,
            "target_type": target_type,
            "data": places_result,
            "answer": answer,
        }

    def _extract_location_query(self, query: str) -> Optional[Dict[str, str]]:
        """ä»æŸ¥è¯¢ä¸­æå–å‚è€ƒåœ°ç‚¹å’Œç›®æ ‡ç±»å‹"""
        # å¸¸è§æ¨¡å¼ï¼š
        # "è·ç¦»Xæœ€è¿‘çš„Yæ˜¯å“ªå®¶" / "Xé™„è¿‘çš„Y" / "ç¦»Xæœ€è¿‘çš„Y"
        # "nearest Y to X" / "Y near X"
        
        patterns_cn = [
            r"è·ç¦»(.+?)æœ€è¿‘çš„(.+?)(?:æ˜¯å“ª|åœ¨å“ª|æœ‰å“ª)",
            r"ç¦»(.+?)æœ€è¿‘çš„(.+?)(?:æ˜¯å“ª|åœ¨å“ª|æœ‰å“ª)",
            r"(.+?)é™„è¿‘çš„(.+?)(?:æ˜¯å“ª|åœ¨å“ª|æœ‰å“ª)",
            r"(.+?)é™„è¿‘æœ‰(?:ä»€ä¹ˆ|å“ªäº›)?(.+)",
            r"(.+?)å‘¨è¾¹çš„(.+)",
            r"è·ç¦»(.+?)æœ€è¿‘çš„(.+)",
            r"ç¦»(.+?)æœ€è¿‘çš„(.+)",
            r"(.+?)é™„è¿‘çš„(.+)",
        ]
        
        patterns_en = [
            r"nearest\s+(.+?)\s+(?:to|from|near)\s+(.+)",
            r"closest\s+(.+?)\s+(?:to|from|near)\s+(.+)",
            r"(.+?)\s+near(?:est)?\s+(.+)",
            r"find\s+(.+?)\s+near\s+(.+)",
        ]
        
        # å°è¯•ä¸­æ–‡æ¨¡å¼
        for pattern in patterns_cn:
            match = re.search(pattern, query, re.IGNORECASE)
            if match:
                reference = match.group(1).strip()
                target = match.group(2).strip()
                # æ¸…ç†ç›®æ ‡ç±»å‹ä¸­çš„å¸¸è§åç¼€
                target = re.sub(r"(?:æ˜¯å“ªå®¶|åœ¨å“ªé‡Œ|æœ‰å“ªäº›|æ˜¯ä»€ä¹ˆ)$", "", target).strip()
                if reference and target:
                    return {"reference_location": reference, "target_type": target}
        
        # å°è¯•è‹±æ–‡æ¨¡å¼
        for pattern in patterns_en:
            match = re.search(pattern, query, re.IGNORECASE)
            if match:
                # è‹±æ–‡æ¨¡å¼ä¸­ç›®æ ‡å’Œå‚è€ƒä½ç½®çš„é¡ºåºå¯èƒ½ä¸åŒ
                g1 = match.group(1).strip()
                g2 = match.group(2).strip()
                # "nearest KFC to HKUST" -> target=KFC, reference=HKUST
                # "KFC near HKUST" -> target=KFC, reference=HKUST
                return {"reference_location": g2, "target_type": g1}
        
        # LLM fallback
        if self.use_llm and self.llm_client:
            prompt = (
                "ä»ç”¨æˆ·é—®é¢˜ä¸­æå–ï¼š\n"
                "1. reference_location: å‚è€ƒåœ°ç‚¹ï¼ˆç”¨æˆ·æƒ³è¦ä»å“ªé‡Œå‡ºå‘/ä»¥å“ªé‡Œä¸ºä¸­å¿ƒï¼‰\n"
                "2. target_type: ç›®æ ‡ç±»å‹ï¼ˆç”¨æˆ·æƒ³è¦æ‰¾ä»€ä¹ˆç±»å‹çš„åœ°ç‚¹ï¼Œå¦‚é¤å…ã€å•†åº—åç§°ç­‰ï¼‰\n\n"
                "è¾“å‡ºJSONæ ¼å¼ï¼Œä¾‹å¦‚ï¼š\n"
                "{\"reference_location\": \"é¦™æ¸¯ç§‘æŠ€å¤§å­¦\", \"target_type\": \"KFC\"}\n\n"
                "å¦‚æœæ— æ³•æå–ï¼Œè¿”å›ç©ºå¯¹è±¡ {}\n\n"
                f"ç”¨æˆ·é—®é¢˜ï¼š{query}"
            )
            try:
                response = self.llm_client.chat(
                    system_prompt="You extract location search parameters from queries.",
                    user_prompt=prompt,
                    max_tokens=200,
                    temperature=0.0,
                )
                content = response.get("content", "{}")
                parsed = json.loads(content)
                ref = (parsed.get("reference_location") or "").strip()
                target = (parsed.get("target_type") or "").strip()
                if ref and target:
                    return {"reference_location": ref, "target_type": target}
            except Exception:
                pass
        
        return None

    def _call_google_places_nearby(
        self,
        lat: float,
        lng: float,
        keyword: str,
        radius: int = 5000,
        timing_recorder: Optional[TimingRecorder] = None,
    ) -> Optional[Dict[str, Any]]:
        """è°ƒç”¨ Google Places API (New) æœç´¢é™„è¿‘åœ°ç‚¹"""
        url = "https://places.googleapis.com/v1/places:searchNearby"
        
        headers = {
            "Content-Type": "application/json",
            "X-Goog-Api-Key": self.google_api_key,
            "X-Goog-FieldMask": "places.displayName,places.formattedAddress,places.location,places.rating,places.userRatingCount,places.googleMapsUri,places.primaryType,places.shortFormattedAddress"
        }
        
        payload = {
            "locationRestriction": {
                "circle": {
                    "center": {
                        "latitude": lat,
                        "longitude": lng
                    },
                    "radius": radius
                }
            },
            "maxResultCount": 10,
            "languageCode": "zh-CN"
        }
        
        # æ·»åŠ å…³é”®è¯/æ–‡æœ¬æŸ¥è¯¢
        if keyword:
            # ä½¿ç”¨ includedTypes æˆ– textQuery å–å†³äºå…³é”®è¯ç±»å‹
            # å¯¹äºå“ç‰Œåç§°ï¼ˆå¦‚KFCï¼‰ï¼Œä½¿ç”¨ textQuery æ›´åˆé€‚
            payload["textQuery"] = keyword
        
        start = time.perf_counter()
        try:
            response = requests.post(
                url,
                headers=headers,
                json=payload,
                timeout=self.request_timeout,
            )
            response.raise_for_status()
            data = response.json()
            return data
        except requests.exceptions.HTTPError as exc:
            # å°è¯•ä½¿ç”¨æ—§ç‰ˆ Places API ä½œä¸ºå¤‡é€‰
            return self._call_google_places_nearby_legacy(
                lat=lat,
                lng=lng,
                keyword=keyword,
                radius=radius,
                timing_recorder=timing_recorder,
            )
        except Exception as exc:
            return {"error": str(exc)}
        finally:
            if timing_recorder:
                duration_ms = (time.perf_counter() - start) * 1000
                timing_recorder.record_search_timing(
                    source="google_places_nearby",
                    label="Google Places Nearby",
                    duration_ms=duration_ms,
                )

    def _call_google_places_nearby_legacy(
        self,
        lat: float,
        lng: float,
        keyword: str,
        radius: int = 5000,
        timing_recorder: Optional[TimingRecorder] = None,
    ) -> Optional[Dict[str, Any]]:
        """ä½¿ç”¨æ—§ç‰ˆ Google Places API (nearbysearch) ä½œä¸ºå¤‡é€‰"""
        url = "https://maps.googleapis.com/maps/api/place/nearbysearch/json"
        
        params = {
            "location": f"{lat},{lng}",
            "radius": radius,
            "keyword": keyword,
            "key": self.google_api_key,
            "language": "zh-CN",
        }
        
        start = time.perf_counter()
        try:
            response = requests.get(
                url,
                params=params,
                timeout=self.request_timeout,
            )
            response.raise_for_status()
            data = response.json()
            
            # è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼
            if data.get("status") == "OK" and data.get("results"):
                places = []
                for result in data["results"][:10]:
                    place = {
                        "displayName": {"text": result.get("name", "")},
                        "formattedAddress": result.get("vicinity", ""),
                        "location": {
                            "latitude": result.get("geometry", {}).get("location", {}).get("lat"),
                            "longitude": result.get("geometry", {}).get("location", {}).get("lng"),
                        },
                        "rating": result.get("rating"),
                        "userRatingCount": result.get("user_ratings_total"),
                    }
                    places.append(place)
                return {"places": places}
            elif data.get("status") == "ZERO_RESULTS":
                return {"places": []}
            else:
                return {"error": data.get("status", "unknown_error")}
        except Exception as exc:
            return {"error": str(exc)}
        finally:
            if timing_recorder:
                duration_ms = (time.perf_counter() - start) * 1000
                timing_recorder.record_search_timing(
                    source="google_places_nearby_legacy",
                    label="Google Places Nearby (Legacy)",
                    duration_ms=duration_ms,
                )

    def _format_location_answer(
        self,
        reference_location: str,
        geocode: Dict[str, Any],
        target_type: str,
        places: List[Dict[str, Any]],
    ) -> str:
        """æ ¼å¼åŒ–åœ°ç‚¹æœç´¢ç»“æœ"""
        ref_address = geocode.get("formatted_address", reference_location)
        ref_lat = geocode.get("lat")
        ref_lng = geocode.get("lng")
        
        if not places:
            return f"åœ¨ {ref_address} é™„è¿‘æœªæ‰¾åˆ° {target_type}ã€‚"
        
        # è®¡ç®—è·ç¦»å¹¶æ’åº
        places_with_distance = []
        for place in places:
            place_lat = place.get("location", {}).get("latitude")
            place_lng = place.get("location", {}).get("longitude")
            
            distance = None
            if ref_lat and ref_lng and place_lat and place_lng:
                # ä½¿ç”¨ Haversine å…¬å¼è®¡ç®—è·ç¦»
                distance = self._haversine_distance(ref_lat, ref_lng, place_lat, place_lng)
            
            places_with_distance.append({
                "place": place,
                "distance": distance,
            })
        
        # æŒ‰è·ç¦»æ’åº
        places_with_distance.sort(key=lambda x: x["distance"] if x["distance"] is not None else float("inf"))
        
        # æ ¼å¼åŒ–è¾“å‡º
        lines = [f"ğŸ“ åœ¨ **{ref_address}** é™„è¿‘æ‰¾åˆ°ä»¥ä¸‹ **{target_type}**ï¼š\n"]
        
        for i, item in enumerate(places_with_distance[:5], 1):
            place = item["place"]
            distance = item["distance"]
            
            name = place.get("displayName", {}).get("text", "æœªçŸ¥åç§°")
            address = place.get("formattedAddress") or place.get("shortFormattedAddress", "")
            rating = place.get("rating")
            rating_count = place.get("userRatingCount")
            
            line = f"{i}. **{name}**"
            if distance is not None:
                if distance < 1:
                    line += f" - çº¦ {int(distance * 1000)} ç±³"
                else:
                    line += f" - çº¦ {distance:.1f} å…¬é‡Œ"
            if address:
                line += f"\n   ğŸ“« {address}"
            if rating:
                stars = "â­" * int(rating)
                line += f"\n   {stars} {rating}"
                if rating_count:
                    line += f" ({rating_count} æ¡è¯„ä»·)"
            
            lines.append(line)
        
        # æ·»åŠ æœ€è¿‘çš„åœ°ç‚¹æ€»ç»“
        if places_with_distance:
            nearest = places_with_distance[0]
            nearest_name = nearest["place"].get("displayName", {}).get("text", "æœªçŸ¥")
            nearest_dist = nearest["distance"]
            
            summary = f"\n\nâœ… **æœ€è¿‘çš„ {target_type}** æ˜¯ **{nearest_name}**"
            if nearest_dist is not None:
                if nearest_dist < 1:
                    summary += f"ï¼Œè·ç¦»çº¦ {int(nearest_dist * 1000)} ç±³ã€‚"
                else:
                    summary += f"ï¼Œè·ç¦»çº¦ {nearest_dist:.1f} å…¬é‡Œã€‚"
            lines.append(summary)
        
        return "\n".join(lines)

    @staticmethod
    def _haversine_distance(lat1: float, lng1: float, lat2: float, lng2: float) -> float:
        """ä½¿ç”¨ Haversine å…¬å¼è®¡ç®—ä¸¤ç‚¹ä¹‹é—´çš„è·ç¦»ï¼ˆå…¬é‡Œï¼‰"""
        import math
        
        R = 6371  # åœ°çƒåŠå¾„ï¼ˆå…¬é‡Œï¼‰
        
        lat1_rad = math.radians(lat1)
        lat2_rad = math.radians(lat2)
        delta_lat = math.radians(lat2 - lat1)
        delta_lng = math.radians(lng2 - lng1)
        
        a = math.sin(delta_lat / 2) ** 2 + math.cos(lat1_rad) * math.cos(lat2_rad) * math.sin(delta_lng / 2) ** 2
        c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))
        
        return R * c

    def _handle_sports(
        self,
        query: str,
        timing_recorder: Optional[TimingRecorder] = None,
    ) -> Dict[str, Any]:
        # ä¸å¯ç”¨ SportsDB APIï¼ˆä¿ç•™ä»£ç ä½†è·³è¿‡è°ƒç”¨ï¼‰
        if not self.sportsdb_api_key or self.sportsdb_api_key == "123":
            return {"handled": False, "skipped": True, "reason": "sportsdb_disabled"}

        entity = self._extract_sports_entity(query)
        if not entity:
            return {"handled": False, "reason": "cannot_parse_sports_entity", "skipped": True}

        if not self.sportsdb_api_key:
            return {"handled": True, "error": "missing_sportsdb_api_key"}

        data = self._call_sportsdb_events(entity, timing_recorder=timing_recorder)
        if not data or data.get("error") or not data.get("events"):
            return {
                "handled": True,
                "error": data.get("error", "no_events_found") if data else "api_failed",
                "entity": entity,
            }

        answer = self._format_sports_answer(entity, data["events"][0])
        return {
            "handled": True,
            "provider": "sportsdb",
            "endpoint": f"https://www.thesportsdb.com/api/v1/json/{self.sportsdb_api_key}/search_all_events.php",
            "entity": entity,
            "data": data,
            "answer": answer,
        }

    def _extract_weather_location(self, query: str) -> str:
        cleaned = query.strip()
        # ç§»é™¤å¸¸è§å¤©æ°”å…³é”®è¯ï¼Œä¿ç•™åœ°ç‚¹æç¤º
        for kw in self.domain_keywords.get("weather", []):
            cleaned = cleaned.replace(kw, " ")
        cleaned = re.sub(r"[?ï¼Ÿã€‚,.!ï¼]", " ", cleaned)
        cleaned = " ".join(token for token in cleaned.split() if token)
        if cleaned:
            return cleaned

        if self.use_llm and self.llm_client:
            prompt = (
                "ä»ç”¨æˆ·é—®é¢˜ä¸­æå–åœ°ç†ä½ç½®ï¼Œè¾“å‡ºJSONæ ¼å¼ï¼Œä¾‹å¦‚ {\"location\": \"åŒ—äº¬\"}ã€‚"
                "å¦‚æœæ— æ³•æå–ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²ã€‚\n\nç”¨æˆ·é—®é¢˜ï¼š" + query
            )
            response = self.llm_client.chat(
                system_prompt="You extract a location name or city.",
                user_prompt=prompt,
                max_tokens=150,
                temperature=0.0,
            )
            try:
                payload = json.loads(response.get("content") or "{}")
                location = payload.get("location") or ""
                if isinstance(location, str) and location.strip():
                    return location.strip()
            except Exception:
                pass
        return query

    def _extract_route(self, query: str) -> Optional[Dict[str, str]]:
        # åŸºç¡€æ­£åˆ™ï¼šä»Aåˆ°B / from A to B
        match_cn = re.search(r"ä»(.+?)åˆ°(.+)", query)
        if match_cn:
            origin = match_cn.group(1).strip()
            destination = match_cn.group(2).strip()
            if origin and destination:
                return {"origin": origin, "destination": destination, "mode": "DRIVING"}

        match_en = re.search(r"from\s+(.+?)\s+to\s+(.+)", query, flags=re.IGNORECASE)
        if match_en:
            origin = match_en.group(1).strip()
            destination = match_en.group(2).strip()
            if origin and destination:
                return {"origin": origin, "destination": destination, "mode": "DRIVING"}

        if self.use_llm and self.llm_client:
            prompt = (
                "ä»ç”¨æˆ·é—®é¢˜é‡Œæå–å‡ºè¡Œèµ·ç‚¹ã€ç»ˆç‚¹ä¸æ–¹å¼ï¼Œè¾“å‡ºJSONï¼Œå¦‚ï¼š"
                "{\"origin\": \"ä¸Šæµ·\", \"destination\": \"è‹å·\", \"mode\": \"DRIVING\"}ã€‚"
                "mode å– DRIVING/TRANSIT/WALKING/BICYCLINGã€‚æå–ä¸åˆ°è¿”å›ç©ºå­—ç¬¦ä¸²ã€‚\n\n"
                f"ç”¨æˆ·é—®é¢˜ï¼š{query}"
            )
            response = self.llm_client.chat(
                system_prompt="You extract travel origin/destination/mode.",
                user_prompt=prompt,
                max_tokens=150,
                temperature=0.0,
            )
            try:
                payload = json.loads(response.get("content") or "{}")
                origin = (payload.get("origin") or "").strip()
                destination = (payload.get("destination") or "").strip()
                mode = (payload.get("mode") or "DRIVING").upper()
                if origin and destination:
                    if mode not in {"DRIVING", "WALKING", "BICYCLING", "TRANSIT"}:
                        mode = "DRIVING"
                    return {"origin": origin, "destination": destination, "mode": mode}
            except Exception:
                return None
        return None

    def _extract_finance_symbol(self, query: str) -> str:
        # å¸¸è§æŒ‡æ•°åç§°åˆ°ç¬¦å·çš„æ˜ å°„
        index_map = {
            "hang seng index": "^HSI",
            "æ’ç”ŸæŒ‡æ•°": "^HSI",
            "nasdaq": "^IXIC",
            "çº³æ–¯è¾¾å…‹": "^IXIC",
            "dow jones": "^DJI",
            "é“ç¼æ–¯": "^DJI",
            "s&p 500": "^GSPC",
            "æ ‡æ™®500": "^GSPC",
        }
        query_lower = query.lower()
        for iname, symbol in index_map.items():
            if iname in query_lower:
                return symbol

        # åŠ å¯†è´§å¸å¸¸è§åç§°åˆ°ç¬¦å·æ˜ å°„
        crypto_map = {
            "æ¯”ç‰¹å¸": "BTC",
            "æ¯”ç‰¹å¹£": "BTC",
            "ä»¥å¤ªåŠ": "ETH",
            "ç‹—ç‹—å¸": "DOGE",
            "è±ç‰¹å¸": "LTC",
            "ç´¢æ‹‰çº³": "SOL",
        }
        for cname, symbol in crypto_map.items():
            if cname.lower() in query_lower:
                return symbol
        
        # ç®€å•æ­£åˆ™åŒ¹é…å¸¸è§è‚¡ç¥¨ä»£ç ï¼Œå¦‚ AAPL, TSLA, 600000 ç­‰
        match_us = re.search(r'\b([A-Z]{1,5})\b(?=\s*(?:è‚¡ä»·|è‚¡ç¥¨|price|stock))', query, re.IGNORECASE)
        if match_us:
            return match_us.group(1).upper()
        
        match_cn = re.search(r'\b([0-9]{6})\b(?=\s*(?:è‚¡ä»·|è‚¡ç¥¨))', query)
        if match_cn:
            return match_cn.group(1)
        
        # LLM fallback
        if self.use_llm and self.llm_client:
            prompt = (
                "ä»ç”¨æˆ·é—®é¢˜ä¸­æå–è‚¡ç¥¨ä»£ç ã€æŒ‡æ•°ä»£ç æˆ–åŠ å¯†è´§å¸ç¬¦å·ï¼ˆç¾è‚¡å¦‚AAPLï¼ŒAè‚¡å¦‚600000ï¼Œæ¸¯è‚¡å¦‚0700.HK, æŒ‡æ•°å¦‚^HSI, åŠ å¯†è´§å¸å¦‚BTCï¼‰ï¼Œ"
                "è¾“å‡ºJSON {\"symbol\": \"^HSI\"}ã€‚"
                "æ— æ³•æå–è¿”å›ç©ºå­—ç¬¦ä¸²ã€‚\n\nç”¨æˆ·é—®é¢˜ï¼š" + query
            )
            response = self.llm_client.chat(
                system_prompt="Extract stock, index, or crypto symbol.",
                user_prompt=prompt,
                max_tokens=100,
                temperature=0.0,
            )
            try:
                payload = json.loads(response.get("content") or "{}")
                symbol = payload.get("symbol") or ""
                if isinstance(symbol, str) and re.match(r'^[A-Z\^]{1,6}(\.HK)?$|^[0-9]{6}$', symbol, re.IGNORECASE):
                    return symbol.upper()
            except Exception:
                pass
        return ""

    def _extract_sports_entity(self, query: str) -> str:
        # æ­£åˆ™åŒ¹é…å¸¸è§ä½“è‚²å®ä½“
        patterns = [
            r'(?:çƒé˜Ÿ|é˜Ÿ|æ¯”èµ›|èµ›äº‹|vs|å¯¹é˜µ)\s*[:ï¼š]?\s*([^\s,ã€‚ï¼Ÿ?]+(?:\s+[^\s,ã€‚ï¼Ÿ?]+)*)',
            r'([a-zA-Z]{2,}(?:\s+[a-zA-Z]{2,})?)(?:\s+(?:vs|å¯¹|æˆ˜)\s+[a-zA-Z]{2,})?',
            r'([^\s,ã€‚ï¼Ÿ?]{2,})(?:\s*(?:æ¯”èµ›|score|ç»“æœ))?'
        ]
        for pattern in patterns:
            match = re.search(pattern, query, re.IGNORECASE)
            if match:
                candidate = match.group(1).strip()
                if len(candidate) > 1:
                    return candidate

        # LLM fallback
        if self.use_llm and self.llm_client:
            prompt = (
                "ä»ä½“è‚²é—®é¢˜ä¸­æå–æ ¸å¿ƒå®ä½“ï¼ˆçƒé˜Ÿåã€èµ›äº‹åï¼‰ï¼Œè¾“å‡ºJSON {\"entity\": \"æ›¼è”\"}ã€‚"
                "æ— æ³•æå–è¿”å›ç©ºå­—ç¬¦ä¸²ã€‚\n\nç”¨æˆ·é—®é¢˜ï¼š" + query
            )
            try:
                response = self.llm_client.chat(
                    system_prompt="Extract sports entity like team or event.",
                    user_prompt=prompt,
                    max_tokens=100,
                    temperature=0.0,
                )
                parsed = json.loads(response.get("content") or "{}")
                entity = parsed.get("entity") or ""
                if isinstance(entity, str) and entity.strip():
                    return entity.strip()
            except Exception:
                pass

        # Fallback to first meaningful word
        words = re.findall(r'\b[a-zA-Z\u4e00-\u9fff]{2,}\b', query)
        return words[0] if words else ""

    def _call_finnhub_quote(
        self,
        symbol: str,
        timing_recorder: Optional[TimingRecorder] = None,
    ) -> Optional[Dict[str, Any]]:
        url = "https://finnhub.io/api/v1/quote"
        params = {
            "symbol": symbol,
            "token": self.finnhub_api_key,
        }
        start = time.perf_counter()
        try:
            response = requests.get(
                url,
                params=params,
                timeout=self.request_timeout,
            )
            response.raise_for_status()
            return response.json()
        except Exception as exc:
            return {"error": str(exc)}
        finally:
            if timing_recorder:
                duration_ms = (time.perf_counter() - start) * 1000
                timing_recorder.record_search_timing(
                    source="finnhub_quote",
                    label="Finnhub Quote",
                    duration_ms=duration_ms,
                )

    def _call_yfinance_quote(
        self,
        symbol: str,
        timing_recorder: Optional[TimingRecorder] = None,
    ) -> Optional[Dict[str, Any]]:
        start = time.perf_counter()
        try:
            ticker = yf.Ticker(symbol)
            info = ticker.info
            quote = {
                "c": info.get("currentPrice") or info.get("regularMarketPrice"),
                "h": info.get("dayHigh") or info.get("regularMarketDayHigh"),
                "l": info.get("dayLow") or info.get("regularMarketDayLow"),
                "o": info.get("regularMarketOpen"),
                "pc": info.get("regularMarketPreviousClose"),
            }
            quote = {k: v for k, v in quote.items() if v is not None}
            if not quote:
                return {"error": "no_data"}
            return quote
        except Exception as exc:
            return {"error": str(exc)}
        finally:
            if timing_recorder:
                duration_ms = (time.perf_counter() - start) * 1000
                timing_recorder.record_search_timing(
                    source="yfinance",
                    label="yfinance Quote",
                    duration_ms=duration_ms,
                )

    def _call_yahoo_fin_quote(
        self,
        symbol: str,
        timing_recorder: Optional[TimingRecorder] = None,
    ) -> Optional[Dict[str, Any]]:
        start = time.perf_counter()
        try:
            c = stock_info.get_live_price(symbol)
            if c is None:
                raise ValueError("No price data")
            return {"c": c}
        except Exception as exc:
            return {"error": str(exc)}
        finally:
            if timing_recorder:
                duration_ms = (time.perf_counter() - start) * 1000
                timing_recorder.record_search_timing(
                    source="yahoo_fin",
                    label="yahoo_fin Quote",
                    duration_ms=duration_ms,
                )

    def _query_stock_price(
        self,
        symbol: str,
        timing_recorder: Optional[TimingRecorder] = None,
    ) -> Optional[Dict[str, Any]]:
        """å°è¯•æŒ‰ä¼˜å…ˆçº§è·å–è‚¡ç¥¨/åŠ å¯†è´§å¸æŠ¥ä»·ï¼šFinnhub -> yfinance -> yahoo_finã€‚

        è¿”å›ç¬¬ä¸€ä¸ªæˆåŠŸçš„ã€é error çš„ç»“æœå­—å…¸ï¼›è‹¥æ‰€æœ‰åç«¯éƒ½å¤±è´¥ï¼Œè¿”å›æœ€åä¸€ä¸ªé”™è¯¯ç»“æ„ã€‚
        """
        last_error: Optional[Dict[str, Any]] = None

        # 1) Finnhubï¼ˆå¦‚æœé…ç½®äº† api keyï¼‰
        if self.finnhub_api_key:
            try:
                quote = self._call_finnhub_quote(symbol, timing_recorder=timing_recorder)
                if quote and not quote.get("error") and quote.get("c", 0) != 0:
                    quote["source_name"] = "Finnhub"
                    return quote
                last_error = quote or {"error": "finnhub_no_response"}
            except Exception as exc:
                last_error = {"error": str(exc)}

        # 2) yfinance
        try:
            quote = self._call_yfinance_quote(symbol, timing_recorder=timing_recorder)
            if quote and not quote.get("error"):
                quote["source_name"] = "Yahoo Finance (yfinance)"
                return quote
            last_error = quote or {"error": "yfinance_no_data"}
        except Exception as exc:
            last_error = {"error": str(exc)}

        # 3) yahoo_fin
        try:
            quote = self._call_yahoo_fin_quote(symbol, timing_recorder=timing_recorder)
            if quote and not quote.get("error"):
                quote["source_name"] = "Yahoo Finance (yahoo-fin)"
                return quote
            last_error = quote or {"error": "yahoo_fin_no_data"}
        except Exception as exc:
            last_error = {"error": str(exc)}

        return last_error or {"error": "no_price_providers_available"}

    def _call_sportsdb_events(
        self,
        entity: str,
        timing_recorder: Optional[TimingRecorder] = None,
    ) -> Optional[Dict[str, Any]]:
        url = f"https://www.thesportsdb.com/api/v1/json/{self.sportsdb_api_key}/search_all_events.php?e={requests.utils.quote(entity)}"
        start = time.perf_counter()
        try:
            response = requests.get(url, timeout=self.request_timeout)
            response.raise_for_status()
            data = response.json()
            return data
        except Exception as exc:
            return {"error": str(exc)}
        finally:
            if timing_recorder:
                duration_ms = (time.perf_counter() - start) * 1000
                timing_recorder.record_search_timing(
                    source="sportsdb_events",
                    label="TheSportsDB Events",
                    duration_ms=duration_ms,
                )

    def _format_sports_answer(self, entity: str, event: Dict) -> str:
        date_event = event.get('dateEvent', 'æœªçŸ¥æ—¥æœŸ')
        str_time = event.get('strTime', 'æœªçŸ¥æ—¶é—´')
        str_home_team = event.get('strHomeTeam', 'æœªçŸ¥ä¸»é˜Ÿ')
        str_away_team = event.get('strAwayTeam', 'æœªçŸ¥å®¢é˜Ÿ')
        str_league = event.get('strLeague', 'æœªçŸ¥è”èµ›')
        str_status = event.get('intHomeScore', 'N/A') + '-' + event.get('intAwayScore', 'N/A') if event.get('intHomeScore') is not None else 'æœªå¼€å§‹'
        return (
            f"{entity} æœ€æ–°ç›¸å…³èµ›äº‹ï¼š\n"
            f"å¯¹é˜µï¼š{str_home_team} vs {str_away_team}\n"
            f"è”èµ›ï¼š{str_league}\n"
            f"æ¯”åˆ†ï¼š{str_status}\n"
            f"æ—¶é—´ï¼š{date_event} {str_time}"
        )

    def _geocode_text(
        self,
        location: str,
        timing_recorder: Optional[TimingRecorder] = None,
    ) -> Optional[Dict[str, Any]]:
        """ä½¿ç”¨ Google Geocoding API å°†æ–‡æœ¬åœ°ç‚¹è½¬æ¢ä¸ºåæ ‡"""
        if not self.google_api_key:
            return {"error": "missing_google_api_key"}

        params = {
            "address": location,
            "key": self.google_api_key,
        }
        start = time.perf_counter()
        try:
            response = requests.get(
                self.google_geocode_url,
                params=params,
                timeout=self.request_timeout,
            )
            response.raise_for_status()
            data = response.json()

            results = data.get("results", [])
            if not results:
                return {"error": "no_geocode_results"}

            result = results[0]
            geometry = result.get("geometry", {}).get("location", {})
            formatted = result.get("formatted_address", "")

            return {
                "lat": geometry.get("lat"),
                "lng": geometry.get("lng"),
                "formatted_address": formatted,
            }
        except Exception as exc:
            return {"error": str(exc)}
        finally:
            if timing_recorder:
                duration_ms = (time.perf_counter() - start) * 1000
                timing_recorder.record_search_timing(
                    source="google_geocode",
                    label="Google Geocode",
                    duration_ms=duration_ms,
                )

    def _call_google_weather(
        self,
        lat: float,
        lng: float,
        timing_recorder: Optional[TimingRecorder] = None,
    ) -> Optional[Dict[str, Any]]:
        """è°ƒç”¨ Google Weather API è·å–å½“å‰å¤©æ°”"""
        params = {
            "key": self.google_api_key,
            "lat": lat,
            "lon": lng,
        }
        start = time.perf_counter()
        try:
            response = requests.get(
                f"{self.google_weather_base_url}/currentConditions:lookup",
                params=params,
                timeout=self.request_timeout,
            )
            response.raise_for_status()
            return response.json()
        except Exception as exc:
            return {"error": str(exc)}
        finally:
            if timing_recorder:
                duration_ms = (time.perf_counter() - start) * 1000
                timing_recorder.record_search_timing(
                    source="google_weather",
                    label="Google Weather",
                    duration_ms=duration_ms,
                )

    def _call_google_air_quality(
        self,
        lat: float,
        lng: float,
        timing_recorder: Optional[TimingRecorder] = None,
    ) -> Optional[Dict[str, Any]]:
        """è°ƒç”¨ Google Air Quality API è·å–å½“å‰ç©ºæ°”è´¨é‡"""
        url = "https://airquality.googleapis.com/v1/currentConditions:lookup"
        headers = {
            "Content-Type": "application/json",
        }
        payload = {
            "location": {
                "latitude": lat,
                "longitude": lng
            },
            "extraComputations": [
                "HEALTH_RECOMMENDATIONS",
                "DOMINANT_POLLUTANT_CONCENTRATION",
                "POLLUTANT_CONCENTRATION",
                "LOCAL_AQI"
            ],
            "uaqiColorPalette": "RED_GREEN",
            "universalAqi": True
        }
        
        start = time.perf_counter()
        try:
            response = requests.post(
                url,
                headers=headers,
                params={"key": self.google_api_key},
                json=payload,
                timeout=self.request_timeout,
            )
            response.raise_for_status()
            return response.json()
        except Exception as exc:
            return {"error": str(exc)}
        finally:
            if timing_recorder:
                duration_ms = (time.perf_counter() - start) * 1000
                timing_recorder.record_search_timing(
                    source="google_air_quality",
                    label="Google Air Quality",
                    duration_ms=duration_ms,
                )

    def _format_weather_answer(
        self,
        location_hint: str,
        geocode: Dict[str, Any],
        weather_data: Dict[str, Any],
    ) -> str:
        """æ ¼å¼åŒ–å¤©æ°”å›å¤"""
        try:
            condition = (
                weather_data.get("weatherCondition", {})
                .get("description", {})
                .get("text", "æœªçŸ¥")
            )
            temp = weather_data.get("temperature", {}).get("degrees", "æœªçŸ¥")
            humidity = weather_data.get("relativeHumidity", {}).get("value", "æœªçŸ¥")
            wind = (
                weather_data.get("wind", {})
                .get("speed", {})
                .get("value", "æœªçŸ¥")
            )
            location = geocode.get("formatted_address", location_hint)
            return (
                f"{location} å½“å‰å¤©æ°”ï¼š{condition}ï¼Œ"
                f"{temp}Â°Cï¼Œæ¹¿åº¦{int(humidity)}%ï¼Œé£é€Ÿ{wind}km/hã€‚"
            )
        except Exception:
            return f"{location_hint} å¤©æ°”æ•°æ®è·å–æˆåŠŸï¼Œä½†è§£æå¤±è´¥ã€‚"

    def _format_air_quality_answer(
        self,
        location_hint: str,
        geocode: Dict[str, Any],
        air_quality_data: Dict[str, Any],
    ) -> str:
        """æ ¼å¼åŒ–ç©ºæ°”è´¨é‡å›å¤"""
        try:
            location = geocode.get("formatted_address", location_hint)
            
            # è·å–AQIä¿¡æ¯
            indexes = air_quality_data.get("indexes", [])
            if not indexes:
                return f"{location} ç©ºæ°”è´¨é‡æ•°æ®è·å–æˆåŠŸï¼Œä½†ç¼ºå°‘AQIä¿¡æ¯ã€‚"
            
            # ä¼˜å…ˆä½¿ç”¨é€šç”¨AQI (UAQI)
            uaqi = next((idx for idx in indexes if idx.get("code") == "uaqi"), indexes[0])
            aqi_value = uaqi.get("aqi", "æœªçŸ¥")
            aqi_category = uaqi.get("category", "æœªçŸ¥")
            dominant_pollutant = uaqi.get("dominantPollutant", "æœªçŸ¥")
            
            # è·å–æ±¡æŸ“ç‰©ä¿¡æ¯
            pollutants = air_quality_data.get("pollutants", [])
            pollutant_info = []
            
            # æŸ¥æ‰¾ä¸»è¦æ±¡æŸ“ç‰©çš„è¯¦ç»†ä¿¡æ¯
            if pollutants and dominant_pollutant:
                main_pollutant = next(
                    (p for p in pollutants if p.get("code") == dominant_pollutant), 
                    None
                )
                if main_pollutant:
                    concentration = main_pollutant.get("concentration", {})
                    value = concentration.get("value", "æœªçŸ¥")
                    units = concentration.get("units", "æœªçŸ¥")
                    display_name = main_pollutant.get("displayName", dominant_pollutant)
                    pollutant_info.append(f"{display_name}: {value} {units}")
            
            # è·å–å¥åº·å»ºè®®
            health_recommendations = air_quality_data.get("healthRecommendations", {})
            general_recommendation = health_recommendations.get("generalPopulation", "")
            
            # æ„å»ºå›å¤
            answer = f"{location} å½“å‰ç©ºæ°”è´¨é‡ï¼š\n"
            answer += f"â€¢ AQIæŒ‡æ•°: {aqi_value} ({aqi_category})\n"
            
            if pollutant_info:
                answer += f"â€¢ ä¸»è¦æ±¡æŸ“ç‰©: {', '.join(pollutant_info)}\n"
            
            # æ·»åŠ å…¶ä»–å¸¸è§æ±¡æŸ“ç‰©ä¿¡æ¯
            common_pollutants = ["pm25", "pm10", "o3", "no2", "so2", "co"]
            other_pollutants = []
            for pollutant_code in common_pollutants:
                if pollutant_code == dominant_pollutant:
                    continue  # å·²ä½œä¸ºä¸»è¦æ±¡æŸ“ç‰©æ˜¾ç¤º
                pollutant = next(
                    (p for p in pollutants if p.get("code") == pollutant_code), 
                    None
                )
                if pollutant:
                    concentration = pollutant.get("concentration", {})
                    value = concentration.get("value")
                    if value is not None:
                        display_name = pollutant.get("displayName", pollutant_code)
                        units = concentration.get("units", "")
                        other_pollutants.append(f"{display_name}: {value} {units}")
            
            if other_pollutants:
                answer += f"â€¢ å…¶ä»–æ±¡æŸ“ç‰©: {', '.join(other_pollutants[:3])}\n"  # é™åˆ¶æ˜¾ç¤ºå‰3ä¸ª
            
            # æ·»åŠ å¥åº·å»ºè®®
            if general_recommendation:
                answer += f"â€¢ å¥åº·å»ºè®®: {general_recommendation}\n"
            
            # æ·»åŠ è¿åŠ¨å»ºè®®ï¼ˆé’ˆå¯¹è·‘æ­¥ç­‰æˆ·å¤–æ´»åŠ¨ï¼‰
            if aqi_value != "æœªçŸ¥" and isinstance(aqi_value, (int, float)):
                if aqi_value <= 50:
                    exercise_advice = "ç©ºæ°”è´¨é‡ä¼˜ç§€ï¼Œéå¸¸é€‚åˆæˆ·å¤–è·‘æ­¥ç­‰è¿åŠ¨ã€‚"
                elif aqi_value <= 100:
                    exercise_advice = "ç©ºæ°”è´¨é‡è‰¯å¥½ï¼Œé€‚åˆæˆ·å¤–è¿åŠ¨ã€‚"
                elif aqi_value <= 150:
                    exercise_advice = "ç©ºæ°”è´¨é‡ä¸€èˆ¬ï¼Œæ•æ„Ÿäººç¾¤åº”å‡å°‘æˆ·å¤–è¿åŠ¨ã€‚"
                elif aqi_value <= 200:
                    exercise_advice = "ç©ºæ°”è´¨é‡è¾ƒå·®ï¼Œä¸å»ºè®®æˆ·å¤–è·‘æ­¥ç­‰è¿åŠ¨ã€‚"
                else:
                    exercise_advice = "ç©ºæ°”è´¨é‡å¾ˆå·®ï¼Œé¿å…æˆ·å¤–è¿åŠ¨ã€‚"
                answer += f"â€¢ è¿åŠ¨å»ºè®®: {exercise_advice}"
            
            return answer
        except Exception as e:
            return f"{location_hint} ç©ºæ°”è´¨é‡æ•°æ®è·å–æˆåŠŸï¼Œä½†è§£æå¤±è´¥: {str(e)}"

    def _format_finance_answer(self, symbol: str, quote: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–è‚¡ç¥¨/åŠ å¯†è´§å¸æŠ¥ä»·ä¸ºå¯è¯»çš„ä¸­æ–‡å›ç­”ã€‚

        æ”¯æŒå¤šç§åç«¯è¿”å›ç»“æ„ï¼ˆfinnhubã€yfinanceã€yahoo_finï¼‰ï¼Œå°½é‡æå–å¸¸è§å­—æ®µã€‚
        """
        if not isinstance(quote, dict):
            return f"{symbol} æŠ¥ä»·è·å–å¤±è´¥ï¼šæ— æ•ˆè¿”å›æ ¼å¼ã€‚"

        if quote.get("error"):
            return f"{symbol} æŠ¥ä»·è·å–å¤±è´¥ï¼š{quote.get('error')}"

        # å¸¸è§å­—æ®µæ˜ å°„
        def _num(val: Any) -> Optional[float]:
            try:
                return float(val)
            except (ValueError, TypeError):
                return None

        # Finnhub: c (current), h (high), l (low), o (open), pc (prev close)
        c = _num(quote.get("c") or quote.get("currentPrice") or quote.get("price"))
        h = _num(quote.get("h") or quote.get("dayHigh") or quote.get("high"))
        l = _num(quote.get("l") or quote.get("dayLow") or quote.get("low"))
        o = _num(quote.get("o") or quote.get("open") or quote.get("regularMarketOpen"))
        pc = _num(quote.get("pc") or quote.get("previousClose") or quote.get("regularMarketPreviousClose"))

        change = None
        change_percent = None
        if c is not None and pc is not None and pc != 0:
            change = c - pc
            change_percent = (change / pc) * 100

        parts: List[str] = []
        if c is not None:
            parts.append(f"å½“å‰ä»· {c:g}")
        
        if change is not None and change_percent is not None:
            sign = "+" if change > 0 else ""
            parts.append(f"æ¶¨è·Œ {sign}{change:.2f} ({sign}{change_percent:.2f}%)")

        if o is not None:
            parts.append(f"å¼€ç›˜ {o:g}")
        if h is not None and l is not None:
            parts.append(f"åŒºé—´ {l:g} - {h:g}")
        else:
            if h is not None:
                parts.append(f"æœ€é«˜ {h:g}")
            if l is not None:
                parts.append(f"æœ€ä½ {l:g}")
        if pc is not None:
            parts.append(f"æ˜¨æ”¶ {pc:g}")

        if not parts:
            # æœ€åå°è¯•æŠŠæ•´ä¸ª quote è½¬ä¸ºç®€çŸ­å­—ç¬¦ä¸²
            try:
                summary = json.dumps({k: v for k, v in quote.items() if v is not None}, ensure_ascii=False)
                return f"{symbol} æŠ¥ä»·ï¼š{summary}"
            except Exception:
                return f"{symbol} æŠ¥ä»·è·å–æˆåŠŸï¼Œä½†æ— æ³•è§£æå…·ä½“å­—æ®µã€‚"

        source = quote.get("source_name", "unknown")
        return f"{symbol}ï¼š" + "ï¼Œ".join(parts) + f"ï¼ˆæ•°æ®æº: {source}ï¼‰"
    

    # 1. Google Routes è·¯çº¿è®¡ç®—æ–¹æ³• (ä¿®å¤å½“å‰çš„ AttributeError)
    def _call_google_routes(
        self,
        origin: str,
        destination: str,
        mode: str = "DRIVE",
        timing_recorder: Optional[TimingRecorder] = None,
    ) -> Optional[Dict[str, Any]]:
        """è°ƒç”¨ Google Routes API (v2) è®¡ç®—è·¯çº¿"""
        url = f"{self.google_routes_base_url}/directions/v2:computeRoutes"
        
        payload = {
            "origin": {"address": origin},
            "destination": {"address": destination},
            "travelMode": mode,
            "routingPreference": "TRAFFIC_AWARE" if mode == "DRIVE" else None,
            "languageCode": "zh-CN",
            "units": "METRIC"
        }
        
        # å¿…é¡»æ·»åŠ  FieldMask æ‰èƒ½è·å–æ•°æ®
        headers = {
            "Content-Type": "application/json",
            "X-Goog-Api-Key": self.google_api_key,
            "X-Goog-FieldMask": "routes.duration,routes.distanceMeters,routes.description,routes.legs"
        }

        start = time.perf_counter()
        try:
            response = requests.post(url, json=payload, headers=headers, timeout=self.request_timeout)
            response.raise_for_status()
            return response.json()
        except Exception as exc:
            print(f"[Routes] API Request Failed: {exc}")
            return {"error": str(exc)}
        finally:
            if timing_recorder:
                timing_recorder.record_search_timing(
                    source="google_routes",
                    label="Google Routes",
                    duration_ms=(time.perf_counter() - start) * 1000
                )

    # 2. è·¯çº¿ç»“æœæ ¼å¼åŒ–æ–¹æ³•
    def _format_route_answer(
        self,
        mode_info: Dict[str, str],
        origin_geo: Dict[str, Any],
        dest_geo: Dict[str, Any],
        route_payload: Dict[str, Any]
    ) -> str:
        """å°†è·¯çº¿æ•°æ®æ ¼å¼åŒ–ä¸ºå¯è¯»æ–‡æœ¬"""
        if not route_payload or "routes" not in route_payload or not route_payload["routes"]:
            return f"{mode_info.get('display')}ï¼šæœªæ‰¾åˆ°æœ‰æ•ˆè·¯çº¿æˆ– API æŠ¥é”™ã€‚"
            
        route = route_payload["routes"][0]
        
        # è§£æè·ç¦» (ç±³ -> å…¬é‡Œ)
        dist_meters = route.get("distanceMeters", 0)
        dist_km = dist_meters / 1000
        
        # è§£ææ—¶é—´ (æ ¼å¼å¦‚ "1800s")
        dur_str = route.get("duration", "0s")
        seconds = 0
        if isinstance(dur_str, str) and dur_str.endswith("s"):
            try:
                seconds = int(dur_str[:-1])
            except ValueError:
                pass
        
        hours = seconds // 3600
        minutes = (seconds % 3600) // 60
        time_str = ""
        if hours > 0: time_str += f"{hours}å°æ—¶"
        if minutes > 0 or hours == 0: time_str += f"{minutes}åˆ†é’Ÿ"
        
        mode_label = mode_info.get("display", "è¡Œç¨‹")
        return f"ğŸš— **{mode_label}**ï¼šé¢„è®¡è€—æ—¶ **{time_str}**ï¼Œè·ç¦» **{dist_km:.1f}å…¬é‡Œ**"

    # 3. è‚¡ç¥¨æŸ¥è¯¢åˆ†å‘æ–¹æ³• (é¢„é˜²é‡‘èæŸ¥è¯¢æŠ¥é”™)
    def _query_stock_price(
        self,
        symbol: str,
        timing_recorder: Optional[TimingRecorder] = None,
    ) -> Optional[Dict[str, Any]]:
        """å°è¯•å¤šç§æ¸ é“æŸ¥è¯¢è‚¡ä»·"""
        # ä¼˜å…ˆå°è¯• Finnhub (API)
        if self.finnhub_api_key:
            res = self._call_finnhub_quote(symbol, timing_recorder)
            if res and not res.get("error") and res.get("c"):
                res["source_name"] = "Finnhub"
                return res
        
        # å…¶æ¬¡å°è¯• yfinance (åº“)
        try:
            res = self._call_yfinance_quote(symbol, timing_recorder)
            if res and not res.get("error"):
                res["source_name"] = "Yahoo Finance (yfinance)"
                return res
        except Exception:
            pass

        return {"error": "æ‰€æœ‰é‡‘èæ•°æ®æºå‡ä¸å¯ç”¨"}

    # 4. é‡‘èç»“æœæ ¼å¼åŒ–æ–¹æ³•
    def _format_finance_answer(self, symbol: str, quote: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–è‚¡ä»·ä¿¡æ¯"""
        price = quote.get("c", "N/A")
        high = quote.get("h", "N/A")
        low = quote.get("l", "N/A")
        src = quote.get("source_name", "Unknown")
        
        return (
            f"ğŸ“ˆ **{symbol}** å®æ—¶è¡Œæƒ… (æ¥æº: {src}):\n"
            f"ğŸ’° å½“å‰ä»·æ ¼: **{price}**\n"
            f"â¬†ï¸ ä»Šæ—¥æœ€é«˜: {high}\n"
            f"â¬‡ï¸ ä»Šæ—¥æœ€ä½: {low}"
        )

def test_basic_functionality():
    """åŸºç¡€åŠŸèƒ½æµ‹è¯•"""
    selector = IntelligentSourceSelector(use_llm=False)
    
    test_cases = [
        "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ",
        "åŒ—äº¬äº¤é€šçŠ¶å†µ",
        "è…¾è®¯è‚¡ç¥¨ä»·æ ¼",
        "æ›¼è”æœ€è¿‘æ¯”èµ›",
        "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½",
        "æœ€æ–°NBAæ¯”èµ›ç»“æœ",
        "æ˜å¤©çš„å¤©æ°”é¢„æŠ¥",
        "ä»ä¸Šæµ·åˆ°åŒ—äº¬çš„é«˜é“",
        "è‹¹æœå…¬å¸çš„è‚¡ä»·",
        "åˆ‡å°”è¥¿å¯¹é˜µæ›¼è”çš„æ¯”èµ›"
    ]
    
    print("âœ… åŸºç¡€åŠŸèƒ½éªŒè¯æµ‹è¯•")
    print("=" * 40)
    
    try:
        for query in test_cases:
            domain, sources = selector.select_sources(query)
            print(f"query '{query}' -> domain: {domain}, sources: {len(sources)}")
    except (UnicodeEncodeError, UnicodeDecodeError):
        # åœ¨ä¸æ”¯æŒUTF-8çš„ç¯å¢ƒä¸­é™é»˜è·³è¿‡æ‰“å°
        pass
    
    print("\nğŸ‰ åŸºç¡€æµ‹è¯•å®Œæˆï¼")

if __name__ == "__main__":
    test_basic_functionality()
