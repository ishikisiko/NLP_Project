from __future__ import annotations

import os
import sys
import re
import time
import logging
from dataclasses import asdict
from typing import Dict, List, Optional, Tuple
from urllib.parse import urlparse

# Add parent directory to path for imports
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from llm.api import HKGAIClient
from langchain.langchain_support import Document, FileReader, LangChainVectorStore
from search.rerank import BaseReranker
from search.search import SearchClient, SearchHit, GoogleSearchClient
from utils.timing_utils import TimingRecorder

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


DEFAULT_SYSTEM_PROMPT = (
    "You are an information assistant. "
    "Answer user questions concisely using ONLY the provided search results and local documents. "
    "CRITICAL: Do NOT fabricate, invent, or guess any specific data (such as scores, numbers, statistics, dates, or names) "
    "that is not EXPLICITLY stated in the search results or local documents. "
    "If specific information is not found in the search results or local documents, clearly state 'æœªåœ¨æœç´¢ç»“æœå’Œæœ¬åœ°æ–‡æ¡£ä¸­æ‰¾åˆ°å…·ä½“æ•°æ®' or 'specific data not found in search results and local documents'. "
    "When unsure, acknowledge the uncertainty instead of guessing. "
    "Always answer in the same language as the user's question."
)


class SearchRAG:
    """A unified RAG pipeline that combines web search with optional local document retrieval."""

    def __init__(
        self,
        llm_client: HKGAIClient,
        search_client: SearchClient,
        data_path: Optional[str] = None,
        system_prompt: str = DEFAULT_SYSTEM_PROMPT,
        *,
        reranker: Optional[BaseReranker] = None,
        min_rerank_score: float = 0.0,
        max_per_domain: int = 1,
        embedding_model: str = "all-MiniLM-L6-v2",
    ) -> None:
        self.llm_client = llm_client
        self.search_client = search_client
        self.system_prompt = system_prompt
        self.reranker = reranker
        self.min_rerank_score = min_rerank_score
        self.max_per_domain = max(1, max_per_domain)

        # Initialize local document retrieval if data_path is provided
        self.vector_store = None
        if data_path:
            print("Loading and indexing local documents...")
            try:
                reader = FileReader(data_path)
                documents = reader.load()
                self.vector_store = LangChainVectorStore(model_name=embedding_model)
                chunk_count = self.vector_store.index(documents)
                print(f"Indexed {chunk_count} chunks from local documents.")
            except Exception as e:
                print(f"Failed to load local documents: {e}")
                self.vector_store = None

    def _format_search_hits(self, hits: List[SearchHit]) -> str:
        if not hits:
            return "No search results were returned."

        formatted_rows = []
        for idx, hit in enumerate(hits, start=1):
            snippet = hit.snippet or "No snippet available."
            url = hit.url or "No URL available."
            title = hit.title or f"Result {idx}"
            formatted_rows.append(
                f"{idx}. {title}\n"
                f"   URL: {url}\n"
                f"   Snippet: {snippet}"
            )
        return "\n".join(formatted_rows)
    
    def _format_local_docs(self, docs: List[Document]) -> str:
        if not docs:
            return "No local documents were retrieved."

        formatted_rows = []
        for idx, doc in enumerate(docs, start=1):
            source = doc.source or f"Document {idx}"
            content = doc.content[:500]  # Limit content length
            if len(doc.content) > 500:
                content += "..."
            formatted_rows.append(
                f"{idx}. {source}\n"
                f"   Content: {content}"
            )
        return "\n".join(formatted_rows)
    
    def _is_temporal_change_query(self, query: str) -> bool:
        """æ£€æµ‹æŸ¥è¯¢æ˜¯å¦ä¸æ—¶é—´å˜åŒ–é¢†åŸŸç›¸å…³"""
        temporal_change_keywords = [
            # æ•™è‚²æ’åç›¸å…³
            "å¤§å­¦", "é«˜æ ¡", "å­¦é™¢", "å­¦æ ¡", "æ’å", "QS", "THE", "ARWU", "US News",
            "university", "college", "ranking", "rankings", "education", "higher education",
            "é¦™æ¸¯ä¸­æ–‡å¤§å­¸", "é¦™æ¸¯ç§‘æŠ€å¤§å­¸", "é¦™æ¸¯å¤§å­¸", "CUHK", "HKUST", "HKU",
            "é¦™æ¸¯ä¸­æ–‡å¤§å­¦", "é¦™æ¸¯ç§‘æŠ€å¤§å­¦", "é¦™æ¸¯å¤§å­¦",
            # æ—¶é—´å˜åŒ–ç›¸å…³
            "æœ€è¿‘10å¹´", "è¿‡å»10å¹´", "10å¹´", "åå¹´", "å†å¹´", "å†å²", "å˜åŒ–", "è¶‹åŠ¿", "å‘å±•",
            "10 years", "decade", "historical", "trend", "development", "evolution",
            "å¯¹æ¯”", "æ¯”è¾ƒ", "å˜åŒ–è¶‹åŠ¿", "æ—¶é—´åºåˆ—", "å¹´åº¦", "é€å¹´",
            "comparison", "compare", "trend over time", "time series", "yearly", "year by year",
            # å…¶ä»–å¯èƒ½çš„æ—¶é—´å˜åŒ–æŸ¥è¯¢
            "å¢é•¿", "ä¸‹é™", "æ³¢åŠ¨", "å˜åŒ–ç‡", "å¢é•¿ç‡", "æ¶¨è·Œ",
            "growth", "decline", "fluctuation", "rate of change", "growth rate", "rise and fall"
        ]
        query_lower = query.lower()
        return any(keyword in query_lower for keyword in temporal_change_keywords)

    def _check_google_client_availability(self) -> Optional[GoogleSearchClient]:
        """
        Enhanced Google search client availability check with comprehensive error handling
        """
        logger.info("ğŸ” Checking Google search client availability...")

        # Check if search_client is CombinedSearchClient with clients
        if hasattr(self.search_client, "clients"):
            logger.debug(f"   Found clients attribute, client count: {len(self.search_client.clients)}")

            for i, client in enumerate(self.search_client.clients):
                logger.debug(f"   Client {i}: {type(client)}")
                if hasattr(client, "source_id"):
                    logger.debug(f"      source_id: {client.source_id}")
                    if client.source_id == "google":
                        logger.info("   âœ… Found Google search client!")
                        return client
        else:
            logger.debug("   âŒ search_client has no clients attribute")

        # Check if search_client itself is a GoogleSearchClient
        if isinstance(self.search_client, GoogleSearchClient):
            logger.info("   âœ… search_client is a GoogleSearchClient!")
            return self.search_client

        logger.warning("âš ï¸ No Google search client found")
        return None

    def _should_fallback_to_granular_search(self, query: str, hits: List[SearchHit]) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥è¿›è¡Œé¢—ç²’åŒ–æœç´¢fallback"""
        if not hits:
            return True
            
        # ä½¿ç”¨LLMåˆ¤æ–­æœç´¢ç»“æœæ˜¯å¦æ»¡è¶³æŸ¥è¯¢éœ€æ±‚
        query_lower = query.lower()
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ—¶é—´å˜åŒ–ç›¸å…³çš„å…³é”®è¯
        time_keywords = ["æœ€è¿‘10å¹´", "è¿‡å»10å¹´", "10å¹´", "åå¹´", "10 years", "decade", "å†å¹´", "å†å²", "å˜åŒ–", "è¶‹åŠ¿"]
        is_time_query = any(kw in query_lower for kw in time_keywords)
        
        if not is_time_query:
            return False
            
        # æ£€æŸ¥æœç´¢ç»“æœä¸­æ˜¯å¦åŒ…å«è¶³å¤Ÿçš„æ—¶é—´å˜åŒ–æ•°æ®
        combined_snippets = " ".join(hit.snippet.lower() for hit in hits if hit.snippet)
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å¹´ä»½+æ’åçš„æ¨¡å¼
        year_rank_pattern = r'\b(20\d{2})\b.*?(?:rank|æ’å|position|#\d+|top\s*\d+)'
        has_year_rank_data = bool(re.search(year_rank_pattern, combined_snippets))
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å¤šä¸ªå¹´ä»½çš„æ•°æ®
        year_pattern = r'\b(20\d{2})\b'
        years_found = re.findall(year_pattern, combined_snippets)
        has_multiple_years = len(set(years_found)) >= 3  # è‡³å°‘3ä¸ªä¸åŒå¹´ä»½çš„æ•°æ®
        
        # å¦‚æœæ²¡æœ‰è¶³å¤Ÿçš„æ—¶é—´å˜åŒ–æ•°æ®ï¼Œåˆ™éœ€è¦fallback
        return not (has_year_rank_data or has_multiple_years)
    
    def _perform_granular_search_fallback(
        self, 
        original_query: str, 
        effective_query: str, 
        num_search_results: int, 
        per_source_cap: int,
        freshness: Optional[str],
        date_restrict: Optional[str],
        timing_recorder: Optional[TimingRecorder]
    ) -> List[SearchHit]:
        """æ‰§è¡Œé¢—ç²’åŒ–æœç´¢fallback"""
        import json
        from api import HKGAIClient
        
        # ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨LLMç”Ÿæˆæ›´å®½æ³›çš„æœç´¢æŸ¥è¯¢
        broad_search_prompt = (
            f"åŸå§‹æŸ¥è¯¢ï¼š{original_query}\n\n"
            "è¿™æ˜¯ä¸€ä¸ªå…³äºæ—¶é—´å˜åŒ–çš„æŸ¥è¯¢ï¼Œä½†åˆå§‹æœç´¢ç»“æœæ²¡æœ‰æä¾›è¶³å¤Ÿçš„å†å²æ•°æ®ã€‚"
            "è¯·ç”Ÿæˆä¸€ä¸ªæ›´å®½æ³›çš„æœç´¢æŸ¥è¯¢ï¼Œç”¨äºè·å–ç›¸å…³çš„å†å²æ•°æ®ã€‚\n\n"
            "ç”Ÿæˆè§„åˆ™ï¼š\n"
            "1. å¦‚æœæŸ¥è¯¢æ¶‰åŠå¤§å­¦æ’åï¼Œç”ŸæˆåŒ…å«'å†å²æ’å'ã€'å†å¹´æ’å'æˆ–'å†å¹´å˜åŒ–'çš„æŸ¥è¯¢\n"
            "2. å¦‚æœæŸ¥è¯¢æ¶‰åŠ10å¹´å˜åŒ–ï¼Œç”ŸæˆåŒ…å«2016-2025å¹´ä»½èŒƒå›´çš„æŸ¥è¯¢\n"
            "3. å¦‚æœæŸ¥è¯¢æ¶‰åŠå…¶ä»–æ—¶é—´å˜åŒ–ï¼Œç”ŸæˆåŒ…å«'å†å²è¶‹åŠ¿'ã€'å†å¹´æ•°æ®'æˆ–'æ—¶é—´åºåˆ—'çš„æŸ¥è¯¢\n"
            "4. å®½æ³›æŸ¥è¯¢åº”è¯¥æ›´é€šç”¨ï¼Œä½†ä»ç„¶ä¿æŒä¸åŸå§‹æŸ¥è¯¢çš„ç›¸å…³æ€§\n\n"
            "åªè¿”å›ä¸€ä¸ªJSONå¯¹è±¡ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š\n"
            '{\n'
            '  "broad_query": "æ›´å®½æ³›çš„æœç´¢æŸ¥è¯¢",\n'
            '  "years": ["2016", "2017", "2018", "2019", "2020", "2021", "2022", "2023", "2024", "2025"]\n'
            '}'
        )
        
        try:
            response = self.llm_client.chat(
                system_prompt="ä½ æ˜¯æœç´¢æŸ¥è¯¢ä¼˜åŒ–ä¸“å®¶ï¼Œæ“…é•¿ç”Ÿæˆæ›´å®½æ³›ä½†ç›¸å…³çš„æœç´¢æŸ¥è¯¢ã€‚",
                user_prompt=broad_search_prompt,
                max_tokens=200,
                temperature=0.3,
            )
            
            content = response.get("content", "")
            # å°è¯•è§£æJSON
            start = content.find("{")
            end = content.rfind("}") + 1
            if start != -1 and end > start:
                json_str = content[start:end]
                llm_result = json.loads(json_str)
                broad_query = llm_result.get("broad_query", effective_query)
                years = llm_result.get("years", [])
            else:
                broad_query = effective_query
                years = []
        except Exception as e:
            print(f"LLMç”Ÿæˆå®½æ³›æŸ¥è¯¢å¤±è´¥: {e}")
            broad_query = effective_query
            years = []
        
        # ç¬¬äºŒæ­¥ï¼šæ‰§è¡Œå®½æ³›æœç´¢
        print(f"ğŸ” æ‰§è¡Œå®½æ³›æœç´¢: {broad_query}")
        broad_hits = self.search_client.search(
            broad_query,
            num_results=num_search_results * 2,  # è·å–æ›´å¤šç»“æœä»¥ä¾¿ç­›é€‰
            per_source_limit=per_source_cap * 2,
            freshness=freshness,
            date_restrict=date_restrict,
        )
        
        # åˆ†æå®½æ³›æœç´¢ç»“æœä¸­çš„æ—¶é—´å˜åŒ–æ•°æ®
        if broad_hits:
            combined_snippets = " ".join(hit.snippet.lower() for hit in broad_hits if hit.snippet)
            import re
            
            # ç»Ÿè®¡æ‰¾åˆ°çš„å¹´ä»½æ•°é‡
            year_pattern = r'\b(20\d{2})\b'
            years_found = re.findall(year_pattern, combined_snippets)
            unique_years = set(years_found)
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«æ’å/ä½ç½®ä¿¡æ¯
            rank_indicators = ['rank', 'æ’å', 'position', '#', 'top', 'ranking']
            has_rank_data = any(indicator in combined_snippets for indicator in rank_indicators)
            
            print(f"ğŸ“Š å®½æ³›æœç´¢åˆ†æï¼šæ‰¾åˆ° {len(unique_years)} ä¸ªä¸åŒå¹´ä»½çš„æ•°æ®ï¼ŒåŒ…å«æ’åä¿¡æ¯: {has_rank_data}")
            
            # å¯¹äºæ—¶é—´å˜åŒ–æŸ¥è¯¢ï¼Œæ€»æ˜¯æ‰§è¡Œé¢—ç²’åŒ–æœç´¢ä»¥è·å–æ›´å®Œæ•´çš„æ•°æ®
            print(f"ğŸ“Š å®½æ³›æœç´¢åˆ†æï¼šæ‰¾åˆ° {len(unique_years)} ä¸ªä¸åŒå¹´ä»½çš„æ•°æ®ï¼ŒåŒ…å«æ’åä¿¡æ¯: {has_rank_data}")
            print("ğŸ”„ ç»§ç»­æ‰§è¡Œé¢—ç²’åŒ–æœç´¢ä»¥è·å–æ›´å®Œæ•´çš„å†å²æ•°æ®...")
        
        # ç¬¬ä¸‰æ­¥ï¼šä»å®½æ³›æœç´¢ç»“æœä¸­æå–å¹´ä»½ä¿¡æ¯
        if not years:
            # å¦‚æœLLMæ²¡æœ‰æä¾›å¹´ä»½ï¼Œå°è¯•ä»æŸ¥è¯¢ä¸­æå–
            import re
            year_range_match = re.search(r'(20\d{2})\s*[-è‡³åˆ°]\s*(20\d{2})', original_query)
            if year_range_match:
                start_year = int(year_range_match.group(1))
                end_year = int(year_range_match.group(2))
                years = [str(year) for year in range(start_year, end_year + 1)]
            else:
                # é»˜è®¤ä½¿ç”¨æœ€è¿‘10å¹´
                current_year = 2025
                years = [str(year) for year in range(current_year - 9, current_year + 1)]
        
        # ç¬¬å››æ­¥ï¼šæ‰§è¡Œé¢—ç²’åŒ–æœç´¢ï¼ˆå¯¹äºæ—¶é—´å˜åŒ–æŸ¥è¯¢æ€»æ˜¯æ‰§è¡Œï¼‰
        if years:  # ç§»é™¤_should_fallback_to_granular_searchæ£€æŸ¥ï¼Œç¡®ä¿æ€»æ˜¯æ‰§è¡Œé¢—ç²’åŒ–æœç´¢
            logger.info("ğŸ” Starting granular search...")
            granular_search_start = time.perf_counter()
            granular_hits = []

            # ä½¿ç”¨å¢å¼ºçš„Googleå®¢æˆ·ç«¯å¯ç”¨æ€§æ£€æŸ¥
            google_client = self._check_google_client_availability()

            if not google_client:
                logger.warning("âš ï¸ No Google search client available, cannot perform granular search")
                return broad_hits

            # Add performance monitoring for granular search
            if timing_recorder:
                timing_recorder.start_operation("granular_search")
            
            if google_client:
                # ä¼˜åŒ–é¢—ç²’åŒ–æŸ¥è¯¢ï¼šæ™ºèƒ½é€‰æ‹©å…³é”®å¹´ä»½
                selected_years = years
                if len(years) > 6:
                    # æ™ºèƒ½é€‰æ‹©ç­–ç•¥ï¼šé€‰æ‹©å¼€å§‹å¹´ä»½ã€ç»“æŸå¹´ä»½å’Œä¸­é—´çš„å‡ ä¸ªå…³é”®å¹´ä»½
                    # å¯¹äº10å¹´æŸ¥è¯¢ï¼Œé€‰æ‹©ç¬¬1å¹´ã€ç¬¬3å¹´ã€ç¬¬5å¹´ã€ç¬¬7å¹´ã€ç¬¬10å¹´
                    if len(years) == 10:
                        selected_years = [years[0], years[2], years[4], years[6], years[-1]]
                    else:
                        # å¯¹äºå…¶ä»–é•¿åº¦çš„å¹´ä»½åˆ—è¡¨ï¼Œå‡åŒ€åˆ†å¸ƒé€‰æ‹©
                        step = max(1, len(years) // 5)
                        selected_years = [years[i] for i in range(0, len(years), step)]
                        if years[-1] not in selected_years:
                            selected_years.append(years[-1])
                    
                    print(f"ğŸ“… ä¼˜åŒ–é¢—ç²’åŒ–æœç´¢ï¼Œé€‰æ‹©å…³é”®å¹´ä»½: {selected_years}")
                else:
                    print(f"ğŸ“… æ‰§è¡Œé¢—ç²’åŒ–æœç´¢ï¼Œå¹´ä»½: {selected_years}")
                
                # ä¸ºæ¯ä¸ªé€‰å®šçš„å¹´ä»½ç”ŸæˆæŸ¥è¯¢
                for year in selected_years:
                    # æ™ºèƒ½ç”Ÿæˆæ›´ç²¾ç¡®çš„å¹´ä»½æŸ¥è¯¢
                    query_lower = original_query.lower()
                    
                    # æå–æŸ¥è¯¢ä¸­çš„å…³é”®å®ä½“ï¼ˆå¤§å­¦åç§°ç­‰ï¼‰
                    import re
                    # æå–å¤§å­¦åç§°
                    universities = []
                    if "é¦™æ¸¯ä¸­æ–‡å¤§å­¸" in original_query or "é¦™æ¸¯ä¸­æ–‡å¤§å­¦" in original_query:
                        universities.append("Chinese University of Hong Kong")
                        universities.append("CUHK")
                    if "é¦™æ¸¯ç§‘æŠ€å¤§å­¸" in original_query or "é¦™æ¸¯ç§‘æŠ€å¤§å­¦" in original_query:
                        universities.append("Hong Kong University of Science and Technology")
                        universities.append("HKUST")
                    
                    # æ ¹æ®æŸ¥è¯¢ç±»å‹ç”Ÿæˆä¸åŒçš„å¹´ä»½æŸ¥è¯¢
                    if "qs" in query_lower and ("æ’å" in original_query or "ranking" in query_lower):
                        # QSæ’åæŸ¥è¯¢ - ç”Ÿæˆæ›´ç®€æ´çš„æŸ¥è¯¢
                        if universities:
                            # å¦‚æœæœ‰å…·ä½“çš„å¤§å­¦ï¼ŒæŸ¥è¯¢è¿™äº›å¤§å­¦çš„QSæ’å
                            for uni in universities:
                                year_query = f"QS world university rankings {year} {uni}"
                                print(f"ğŸ” æœç´¢å¹´ä»½ {year}: {year_query}")
                                try:
                                    year_hits = google_client.search(
                                        year_query,
                                        num_results=max(2, num_search_results // (len(selected_years) * len(universities))),
                                        freshness=freshness,
                                        date_restrict=f"{year}-01-01..{year}-12-31",  # é™åˆ¶åœ¨ç‰¹å®šå¹´ä»½å†…
                                    )
                                    granular_hits.extend(year_hits)
                                except Exception as e:
                                    print(f"å¹´ä»½ {year} æœç´¢å¤±è´¥: {e}")
                            
                            # é¢å¤–æŸ¥è¯¢ï¼šé¦™æ¸¯å¤§å­¦QSæ’åï¼ˆä½œä¸ºå‚è€ƒï¼‰
                            hk_query = f"QS world university rankings {year} Hong Kong universities ranking"
                            print(f"ğŸ” æœç´¢å¹´ä»½ {year} (é¦™æ¸¯å¤§å­¦æ’å): {hk_query}")
                            try:
                                hk_hits = google_client.search(
                                    hk_query,
                                    num_results=2,
                                    freshness=freshness,
                                    date_restrict=f"{year}-01-01..{year}-12-31",  # é™åˆ¶åœ¨ç‰¹å®šå¹´ä»½å†…
                                )
                                granular_hits.extend(hk_hits)
                            except Exception as e:
                                print(f"å¹´ä»½ {year} é¦™æ¸¯å¤§å­¦æ’åæœç´¢å¤±è´¥: {e}")
                        else:
                            # å¦‚æœæ²¡æœ‰å…·ä½“å¤§å­¦ï¼ŒæŸ¥è¯¢QSæ’åæ€»ä½“æƒ…å†µ
                            year_query = f"QS world university rankings {year}"
                            print(f"ğŸ” æœç´¢å¹´ä»½ {year}: {year_query}")
                            try:
                                year_hits = google_client.search(
                                    year_query,
                                    num_results=max(3, num_search_results // len(selected_years)),
                                    freshness=freshness,
                                    date_restrict=f"{year}-01-01..{year}-12-31",  # é™åˆ¶åœ¨ç‰¹å®šå¹´ä»½å†…
                                )
                                granular_hits.extend(year_hits)
                            except Exception as e:
                                print(f"å¹´ä»½ {year} æœç´¢å¤±è´¥: {e}")
                        continue  # è·³è¿‡åç»­çš„é€šç”¨æŸ¥è¯¢é€»è¾‘
                    elif "the" in query_lower and ("æ’å" in original_query or "ranking" in query_lower):
                        # THEæ’åæŸ¥è¯¢
                        year_query = f"THE world university rankings {year}"
                    elif "arwu" in query_lower or "è½¯ç§‘" in original_query:
                        # ARWUæ’åæŸ¥è¯¢
                        year_query = f"ARWU academic ranking of world universities {year}"
                    elif "æ’å" in original_query or "ranking" in query_lower:
                        # é€šç”¨æ’åæŸ¥è¯¢
                        year_query = f"university rankings {year}"
                    elif "å¤§å­¦" in original_query or "university" in query_lower:
                        # å¤§å­¦ç›¸å…³æŸ¥è¯¢
                        year_query = f"university {year}"
                    else:
                        # é€šç”¨æŸ¥è¯¢
                        year_query = f"{original_query} {year}"
                    
                    print(f"ğŸ” æœç´¢å¹´ä»½ {year}: {year_query}")
                    
                    try:
                        year_hits = google_client.search(
                            year_query,
                            num_results=max(3, num_search_results // len(selected_years)),
                            freshness=freshness,
                            date_restrict=f"{year}-01-01..{year}-12-31",  # é™åˆ¶åœ¨ç‰¹å®šå¹´ä»½å†…
                        )
                        granular_hits.extend(year_hits)
                        logger.debug(f"âœ… Successfully searched year {year}, got {len(year_hits)} results")
                    except Exception as e:
                        logger.error(f"âŒ Year {year} search failed: {e}")
                        logger.debug(f"   Query: {year_query}")
                        if timing_recorder:
                            timing_recorder.record_error(f"granular_search_year_{year}", str(e))
                
                # åˆå¹¶å®½æ³›æœç´¢å’Œé¢—ç²’åŒ–æœç´¢ç»“æœï¼Œä¼˜å…ˆä¿ç•™é¢—ç²’åŒ–æœç´¢ç»“æœ
                all_hits = granular_hits + broad_hits
                
                # æ™ºèƒ½å»é‡ï¼šä¿ç•™æ›´ç›¸å…³çš„ç»“æœ
                seen_urls = set()
                deduped_hits = []
                for hit in all_hits:
                    url_key = hit.url or ""
                    if url_key not in seen_urls:
                        seen_urls.add(url_key)
                        deduped_hits.append(hit)
                
                # æŒ‰ç›¸å…³æ€§æ’åºï¼šä¼˜å…ˆåŒ…å«å¹´ä»½å’Œæ’åä¿¡æ¯çš„ç»“æœ
                def hit_relevance_score(hit):
                    score = 0
                    if hit.snippet:
                        snippet = hit.snippet.lower()
                        # æ£€æŸ¥æ˜¯å¦åŒ…å«å¹´ä»½
                        import re
                        years_in_snippet = re.findall(r'\b(20\d{2})\b', snippet)
                        score += len(years_in_snippet) * 2
                        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ’åä¿¡æ¯
                        rank_keywords = ['rank', 'æ’å', 'position', '#', 'top']
                        score += sum(1 for kw in rank_keywords if kw in snippet)
                    return score
                
                deduped_hits.sort(key=hit_relevance_score, reverse=True)

                # Record granular search performance
                granular_search_duration = (time.perf_counter() - granular_search_start) * 1000
                logger.info(f"âœ… Granular search completed in {granular_search_duration:.2f}ms, obtained {len(deduped_hits)} results")

                if timing_recorder:
                   timing_recorder.end_operation("granular_search", {
                       "duration_ms": granular_search_duration,
                       "results_count": len(deduped_hits),
                       "years_queried": len(selected_years),
                       "google_client_available": google_client is not None
                   })

                return deduped_hits[:num_search_results * 2]  # è¿”å›æ›´å¤šç»“æœä»¥ä¾¿ç­›é€‰
            else:
                logger.warning("âš ï¸ No Google search client available, cannot perform granular search")
                if timing_recorder:
                   timing_recorder.record_error("granular_search", "No Google client available")
                return broad_hits
        else:
            logger.info("No years specified for granular search")
            return broad_hits

    def build_prompt(self, query: str, hits: List[SearchHit], local_docs: List[Document] = None, ranking_info: str = "") -> str:
        search_context = self._format_search_hits(hits)
        local_context = self._format_local_docs(local_docs) if local_docs else ""
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æ’åæŸ¥è¯¢
        is_ranking_query = any(keyword in query.lower() for keyword in ['æ’å', 'ranking', 'rank'])
        
        if is_ranking_query:
            special_instructions = (
                "å¯¹äºæ’åæŸ¥è¯¢ï¼Œè¯·ç‰¹åˆ«æ³¨æ„ï¼š\n"
                "1. å¦‚æœæœç´¢ç»“æœä¸­æœ‰å…·ä½“çš„æ’åæ•°æ®ï¼Œè¯·ä½¿ç”¨è¿™äº›æ•°æ®\n"
                "2. å¦‚æœæ²¡æœ‰æ‰¾åˆ°å…·ä½“çš„æ’åæ•°å­—ï¼Œè¯·æ˜ç¡®è¯´æ˜'æœªæ‰¾åˆ°å…·ä½“æ’åæ•°æ®'\n"
                "3. ä¸è¦çŒœæµ‹æˆ–ç¼–é€ æ’åæ•°å­—\n"
                "4. å¦‚æœæœ‰å¤šä¸ªå¹´ä»½çš„æ’åæ•°æ®ï¼Œè¯·æŒ‰æ—¶é—´é¡ºåºå‘ˆç°\n"
                "5. å¦‚æœæœ‰å¤šä¸ªæ’åç³»ç»Ÿï¼ˆQSã€THEã€ARWUç­‰ï¼‰ï¼Œè¯·åˆ†åˆ«è¯´æ˜\n\n"
            )
        else:
            special_instructions = ""
        
        # æ„å»ºæç¤º
        prompt_parts = [
            f"ç”¨æˆ·é—®é¢˜ï¼š{query}\n\n",
        ]
        
        # æ·»åŠ æœç´¢ç»“æœä¸Šä¸‹æ–‡
        if search_context:
            prompt_parts.append("ç½‘ç»œæœç´¢ç»“æœï¼š\n")
            prompt_parts.append(search_context)
            prompt_parts.append("\n\n")
        
        # æ·»åŠ æœ¬åœ°æ–‡æ¡£ä¸Šä¸‹æ–‡
        if local_context:
            prompt_parts.append("æœ¬åœ°æ–‡æ¡£ï¼š\n")
            prompt_parts.append(local_context)
            prompt_parts.append("\n\n")
        
        # æ·»åŠ æ’åä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
        if ranking_info:
            prompt_parts.append(ranking_info)
            prompt_parts.append("\n\n")
        
        # æ·»åŠ ç‰¹æ®ŠæŒ‡ä»¤
        if special_instructions:
            prompt_parts.append(special_instructions)
        
        # æ·»åŠ æœ€ç»ˆæŒ‡ä»¤
        prompt_parts.append(
            "è¯·åŸºäºä»¥ä¸Šä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ã€‚"
            "å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯·æ˜ç¡®è¯´æ˜ã€‚"
            "è¯·ç”¨ä¸ç”¨æˆ·é—®é¢˜ç›¸åŒçš„è¯­è¨€å›ç­”ã€‚"
        )
        
        return "".join(prompt_parts)

    def _extract_ranking_data(self, hits: List[SearchHit]) -> Dict[str, Dict[str, int]]:
        """ä»æœç´¢ç»“æœä¸­æå–æ’åæ•°æ® - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œå¢å¼ºæ­£åˆ™è¡¨è¾¾å¼è¦†ç›–èŒƒå›´å’ŒåŒ¹é…èƒ½åŠ›"""
        logger.info("ğŸ” Starting ranking data extraction from search results...")

        cuhk_rankings = {}
        hkust_rankings = {}

        # Enhanced regex patterns for university names and ranking indicators
        university_patterns = {
            "cuhk": [
                r"chinese university of hong kong",
                r"cuhk",
                r"é¦™æ¸¯ä¸­æ–‡å¤§å­¸",
                r"é¦™æ¸¯ä¸­æ–‡å¤§å­¦",
                r"æ¸¯ä¸­å¤§",
                r"ä¸­æ–‡å¤§å­¸",
                r"ä¸­æ–‡å¤§å­¦"
            ],
            "hkust": [
                r"hong kong university of science and technology",
                r"hkust",
                r"é¦™æ¸¯ç§‘æŠ€å¤§å­¸",
                r"é¦™æ¸¯ç§‘æŠ€å¤§å­¦",
                r"æ¸¯ç§‘å¤§",
                r"ç§‘æŠ€å¤§å­¸",
                r"ç§‘æŠ€å¤§å­¦"
            ]
        }

        # Enhanced ranking patterns with broader coverage
        ranking_patterns = [
            r"ranked? #?(\d+)",
            r"æ’å.*?ç¬¬?(\d+)",
            r"position #?(\d+)",
            r"top\s*(\d+)",
            r"#(\d+)",
            r"(\d+)\s*(?:st|nd|rd|th)",
            r"(\d+)\s*ä½",
            r"(\d+)\s*å"
        ]

        # Enhanced year patterns with broader coverage
        year_patterns = [
            r"\b(20\d{2})\b",  # 4-digit years starting with 20
            r"(\d{4})å¹´",      # Chinese format: 2023å¹´
            r"(\d{4})å¹´åº¦",    # Chinese format: 2023å¹´åº¦
            r"\b(19\d{2})\b"   # 4-digit years starting with 19 (for historical data)
        ]

        for hit_idx, hit in enumerate(hits):
            if not hit.snippet:
                continue

            snippet = hit.snippet.lower()
            logger.debug(f"Processing hit {hit_idx + 1}: {hit.title[:50]}...")

            # Extract year information first
            extracted_years = []
            for year_pattern in year_patterns:
                year_matches = re.findall(year_pattern, snippet)
                for match in year_matches:
                    # Handle different match formats from regex
                    if isinstance(match, tuple):
                        year_str = match[0]
                    else:
                        year_str = match

                    # Clean up year string
                    year_str = str(year_str).strip()

                    # Convert 2-digit years to 4-digit (assuming 2000s)
                    if len(year_str) == 2:
                        year = f"20{year_str}"
                        if 2000 <= int(year) <= 2099:
                            extracted_years.append(year)
                    elif len(year_str) == 4 and year_str.startswith(('20', '19')):
                        extracted_years.append(year_str)
                    elif 'å¹´' in year_str:
                        # Extract year from Chinese format like "2022å¹´"
                        year_match = re.search(r'(\d{4})å¹´', year_str)
                        if year_match:
                            extracted_years.append(year_match.group(1))

            # Remove duplicates and sort
            unique_years = sorted(list(set(extracted_years)), reverse=True)
            logger.debug(f"   Extracted years: {unique_years}")

            # Extract CUHK rankings with enhanced patterns
            for year in unique_years:
                # Look for year followed immediately by ranking information
                # Use multiple specific patterns to catch different formats
                ranking_patterns = [
                    rf"{year}.*?(?:ranked?)\s+#(\d+)",  # "2022 ranked #45"
                    rf"{year}.*?(?:ranked?)\s+(\d+)(?:st|nd|rd|th)",  # "2022 ranked 45th"
                    rf"{year}.*?#(\d+)",  # "2022 #45"
                    rf"{year}.*?ç¬¬(\d+)",  # "2022 ç¬¬45"
                    rf"{year}.*?(\d+)(?:st|nd|rd|th)",  # "2022 45th"
                    rf"{year}.*?(\d+)\s*(?:ä½|å)",  # "2022 45ä½"
                    rf"{year}.*?and\s+(\d+)(?:st|nd|rd|th)",  # "2022 and 45th"
                    rf"{year}.*?in\s+(\d+)"  # "2022 in 45"
                ]

                for ranking_pattern in ranking_patterns:
                    ranking_matches = re.findall(ranking_pattern, snippet, re.IGNORECASE)
                    if ranking_matches:
                        for match in ranking_matches:
                            try:
                                if isinstance(match, tuple):
                                    rank_str = match[-1]  # Get the last group (the ranking number)
                                else:
                                    rank_str = match

                                rank = int(rank_str)
                                if 1 <= rank <= 1000:  # Reasonable ranking range
                                    # Check if CUHK is mentioned in the snippet
                                    cuhk_found = False
                                    for cuhk_pattern in university_patterns["cuhk"]:
                                        if re.search(cuhk_pattern, snippet, re.IGNORECASE):
                                            cuhk_found = True
                                            break

                                    if cuhk_found:
                                        cuhk_rankings[year] = rank
                                        logger.debug(f"   CUHK {year}: #{rank} via pattern: {ranking_pattern}")
                                        break
                            except (ValueError, IndexError) as e:
                                logger.debug(f"   Failed to extract CUHK {year} rank from '{match}': {e}")
                                continue
                        if year in cuhk_rankings:  # Found a ranking for this year
                            break

            # Extract HKUST rankings with enhanced patterns
            for year in unique_years:
                # Look for year followed immediately by ranking information
                # Use multiple patterns to catch different formats
                ranking_patterns = [
                    rf"{year}.*?(?:ranked?)\s+#?(\d+)",  # "2022 ranked #45"
                    rf"{year}.*?(?:ranked?)\s+(\d+)(?:st|nd|rd|th)",  # "2022 ranked 45th"
                    rf"{year}.*?#(\d+)",  # "2022 #45"
                    rf"{year}.*?ç¬¬(\d+)",  # "2022 ç¬¬45"
                    rf"{year}.*?(\d+)(?:st|nd|rd|th)",  # "2022 45th"
                    rf"{year}.*?(\d+)\s*(?:ä½|å)"  # "2022 45ä½"
                ]

                for ranking_pattern in ranking_patterns:
                    ranking_matches = re.findall(ranking_pattern, snippet, re.IGNORECASE)
                    if ranking_matches:
                        for match in ranking_matches:
                            try:
                                if isinstance(match, tuple):
                                    rank_str = match[-1]  # Get the last group (the ranking number)
                                else:
                                    rank_str = match

                                rank = int(rank_str)
                                if 1 <= rank <= 1000:  # Reasonable ranking range
                                    # Check if HKUST is mentioned in the snippet
                                    hkust_found = False
                                    for hkust_pattern in university_patterns["hkust"]:
                                        if re.search(hkust_pattern, snippet, re.IGNORECASE):
                                            hkust_found = True
                                            break

                                    if hkust_found:
                                        hkust_rankings[year] = rank
                                        logger.debug(f"   HKUST {year}: #{rank} via pattern: {ranking_pattern}")
                                        break
                            except (ValueError, IndexError) as e:
                                logger.debug(f"   Failed to extract HKUST {year} rank from '{match}': {e}")
                                continue
                        if year in hkust_rankings:  # Found a ranking for this year
                            break

        logger.info(f"âœ… Ranking data extraction completed. CUHK: {len(cuhk_rankings)} entries, HKUST: {len(hkust_rankings)} entries")
        return {
            "cuhk_rankings": cuhk_rankings,
            "hkust_rankings": hkust_rankings
        }

    def _validate_and_integrate_ranking_data(self, ranking_data: Dict[str, Dict[str, int]]) -> Dict[str, Dict[str, int]]:
        """
        Validate and integrate ranking data with comprehensive data quality checks
        """
        logger.info("ğŸ” Validating and integrating ranking data...")

        validated_data = {
            "cuhk_rankings": {},
            "hkust_rankings": {}
        }

        # Validate CUHK rankings
        for year, rank in ranking_data.get("cuhk_rankings", {}).items():
            try:
                # Convert year to string if it's not already
                year_str = str(year)

                # Validate year format and range
                if not year_str.startswith("20") or len(year_str) != 4:
                    logger.warning(f"Invalid CUHK year format: {year_str}")
                    continue

                year_int = int(year_str)
                if year_int < 2000 or year_int > 2099:
                    logger.warning(f"CUHK year out of reasonable range: {year_int}")
                    continue

                # Validate rank format and range
                rank_int = int(rank)
                if rank_int < 1 or rank_int > 1000:
                    logger.warning(f"CUHK rank out of reasonable range: {rank_int}")
                    continue

                # Store validated data
                validated_data["cuhk_rankings"][year_str] = rank_int
                logger.debug(f"âœ… Validated CUHK ranking: {year_str} -> #{rank_int}")

            except (ValueError, TypeError) as e:
                logger.warning(f"Invalid CUHK ranking data - year: {year}, rank: {rank}, error: {e}")
                continue

        # Validate HKUST rankings
        for year, rank in ranking_data.get("hkust_rankings", {}).items():
            try:
                # Convert year to string if it's not already
                year_str = str(year)

                # Validate year format and range
                if not year_str.startswith("20") or len(year_str) != 4:
                    logger.warning(f"Invalid HKUST year format: {year_str}")
                    continue

                year_int = int(year_str)
                if year_int < 2000 or year_int > 2099:
                    logger.warning(f"HKUST year out of reasonable range: {year_int}")
                    continue

                # Validate rank format and range
                rank_int = int(rank)
                if rank_int < 1 or rank_int > 1000:
                    logger.warning(f"HKUST rank out of reasonable range: {rank_int}")
                    continue

                # Store validated data
                validated_data["hkust_rankings"][year_str] = rank_int
                logger.debug(f"âœ… Validated HKUST ranking: {year_str} -> #{rank_int}")

            except (ValueError, TypeError) as e:
                logger.warning(f"Invalid HKUST ranking data - year: {year}, rank: {rank}, error: {e}")
                continue

        # Sort rankings by year for consistent output
        validated_data["cuhk_rankings"] = dict(sorted(validated_data["cuhk_rankings"].items()))
        validated_data["hkust_rankings"] = dict(sorted(validated_data["hkust_rankings"].items()))

        logger.info(f"âœ… Data validation completed. Valid CUHK: {len(validated_data['cuhk_rankings'])} entries, Valid HKUST: {len(validated_data['hkust_rankings'])} entries")
        return validated_data

    def _apply_rerank(
        self,
        query: str,
        hits: List[SearchHit],
        *,
        limit: Optional[int] = None,
    ) -> Tuple[List[SearchHit], List[Dict[str, object]]]:
        if not self.reranker or not hits:
            return hits, []

        try:
            reranked = self.reranker.rerank(query, hits)
        except Exception as exc:  # pragma: no cover - best effort resilience
            return hits, [{"error": str(exc)}]

        filtered: List[SearchHit] = []
        metadata: List[Dict[str, object]] = []
        domain_counts: Dict[str, int] = {}
        max_results = limit or len(reranked)

        for item in reranked:
            if item.score < self.min_rerank_score:
                metadata.append(
                    {
                        "url": item.hit.url,
                        "score": item.score,
                        "dropped": "below_min_score",
                    }
                )
                continue

            domain = self._extract_domain(item.hit.url)
            if domain and domain_counts.get(domain, 0) >= self.max_per_domain:
                metadata.append(
                    {
                        "url": item.hit.url,
                        "score": item.score,
                        "dropped": "per_domain_limit",
                    }
                )
                continue

            filtered.append(item.hit)
            metadata.append(
                {
                    "url": item.hit.url,
                    "score": item.score,
                    "kept": True,
                }
            )

            if domain:
                domain_counts[domain] = domain_counts.get(domain, 0) + 1

            if len(filtered) >= max_results:
                break

        if not filtered:
            return hits, metadata

        return filtered, metadata

    @staticmethod
    def _extract_domain(url: str) -> Optional[str]:
        if not url:
            return None
        return urlparse(url).netloc or None

    def answer(
        self,
        query: str,
        *,
        search_query: Optional[str] = None,
        num_search_results: int = 5,
        per_source_limit: Optional[int] = None,
        num_retrieved_docs: int = 5,
        max_tokens: int = 5000,
        temperature: float = 0.3,
        enable_search: bool = True,
        enable_local_docs: bool = True,
        reference_limit: Optional[int] = None,
        freshness: Optional[str] = None,
        date_restrict: Optional[str] = None,
        timing_recorder: Optional[TimingRecorder] = None,
        images: Optional[List[Dict[str, str]]] = None,
    ) -> Dict[str, object]:
        """Answer a query using the SearchRAG pipeline."""
        hits: List[SearchHit] = []
        effective_query = search_query.strip() if search_query else query
        search_error: Optional[str] = None

        search_warnings: List[str] = []
        raw_hits: List[SearchHit] = []

        # æ‰§è¡Œæœç´¢
        if enable_search:
            try:
                per_source_cap = per_source_limit if per_source_limit is not None else num_search_results
                
                # Calculate fetch limit to ensure we get enough candidates for reranking
                # If reranker is enabled, we want to fetch (per_source_cap * num_sources) results
                # CombinedSearchClient will aggregate them.
                fetch_limit = num_search_results
                if self.reranker:
                    num_sources = 1
                    if hasattr(self.search_client, "clients"):
                        num_sources = len(self.search_client.clients)
                    fetch_limit = per_source_cap * num_sources

                raw_hits = self.search_client.search(
                    effective_query,
                    num_results=fetch_limit,
                    per_source_limit=per_source_cap,
                    freshness=freshness,
                    date_restrict=date_restrict,
                )
                hits = list(raw_hits)
            except Exception as exc:
                # Surface search errors while still letting the LLM see local docs
                hits = []
                search_error = str(exc)
            finally:
                if timing_recorder:
                    timings_getter = getattr(self.search_client, "get_last_timings", None)
                    if callable(timings_getter):
                        timing_recorder.extend_search_timings(timings_getter())

        if raw_hits:
            get_last_errors = getattr(self.search_client, "get_last_errors", None)
            if callable(get_last_errors):
                errors = get_last_errors() or []
                if errors:
                    for item in errors:
                        source = str(item.get("source") or "æœç´¢æœåŠ¡")
                        detail = str(item.get("error") or "æœªçŸ¥é”™è¯¯")
                        if source.lower().startswith("mcp"):
                            search_warnings.append(f"{source} æœªæ­£å¸¸å·¥ä½œï¼Œå·²ä½¿ç”¨å…¶ä»–æœç´¢ç»“æœã€‚åŸå› ï¼š{detail}")
                        else:
                            search_warnings.append(f"{source} å‡ºç°å¼‚å¸¸ï¼š{detail}")

        # åº”ç”¨é‡æ’åº
        hits, rerank_meta = self._apply_rerank(query, hits, limit=num_search_results)

        # æ£€æŸ¥æ˜¯å¦éœ€è¦LLM fallbackï¼ˆé’ˆå¯¹æ—¶é—´å˜åŒ–ç±»æŸ¥è¯¢ï¼‰
        if enable_search and self._is_temporal_change_query(query):
            # å¯¹äºæ—¶é—´å˜åŒ–æŸ¥è¯¢ï¼Œæ€»æ˜¯å°è¯•æ‰§è¡Œé¢—ç²’åŒ–æœç´¢ä»¥è·å–æ›´å…¨é¢çš„å†å²æ•°æ®
            print("ğŸ”„ æ£€æµ‹åˆ°æ—¶é—´å˜åŒ–æŸ¥è¯¢ï¼Œå¯åŠ¨LLM fallbackæœºåˆ¶ä»¥è·å–å†å²æ•°æ®...")
            fallback_hits = self._perform_granular_search_fallback(query, effective_query, num_search_results, per_source_cap, freshness, date_restrict, timing_recorder)
            if fallback_hits:
                hits = fallback_hits
                print(f"âœ… Fallbackæœç´¢å®Œæˆï¼Œè·å¾—{len(fallback_hits)}æ¡ç»“æœ")
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æ’åæŸ¥è¯¢ï¼Œå¦‚æœæ˜¯åˆ™æå–æ’åæ•°æ®
        is_ranking_query = any(keyword in query.lower() for keyword in ['æ’å', 'ranking', 'rank'])
        ranking_data = None
        if is_ranking_query:
            logger.info("ğŸ” æ£€æµ‹åˆ°æ’åæŸ¥è¯¢ï¼Œæå–æ’åæ•°æ®...")
            ranking_data = self._extract_ranking_data(hits)

            # Validate and integrate the extracted ranking data
            validated_ranking_data = self._validate_and_integrate_ranking_data(ranking_data)

            logger.info(f"âœ… æå–åˆ°CUHKæ’åæ•°æ®: {validated_ranking_data['cuhk_rankings']}")
            logger.info(f"âœ… æå–åˆ°HKUSTæ’åæ•°æ®: {validated_ranking_data['hkust_rankings']}")

            # Use validated data instead of raw extracted data
            ranking_data = validated_ranking_data
        
        # æ£€ç´¢æœ¬åœ°æ–‡æ¡£
        retrieved_docs: List[Document] = []
        if enable_local_docs and self.vector_store:
            retrieved_docs = self.vector_store.search(query, k=num_retrieved_docs)

        # æ„å»ºæç¤º
        if is_ranking_query and ranking_data:
            ranking_info = "\n\næå–çš„æ’åæ•°æ®:\n"
            if ranking_data['cuhk_rankings']:
                ranking_info += "CUHKæ’å:\n"
                for year, rank in sorted(ranking_data['cuhk_rankings'].items()):
                    ranking_info += f"- {year}å¹´: #{rank}\n"
            
            if ranking_data['hkust_rankings']:
                ranking_info += "HKUSTæ’å:\n"
                for year, rank in sorted(ranking_data['hkust_rankings'].items()):
                    ranking_info += f"- {year}å¹´: #{rank}\n"
            
            user_prompt = self.build_prompt(query, hits, retrieved_docs, ranking_info)
        else:
            user_prompt = self.build_prompt(query, hits, retrieved_docs)

        # è°ƒç”¨LLMç”Ÿæˆç­”æ¡ˆ
        response_start = time.perf_counter()
        try:
            response = self.llm_client.chat(
                system_prompt=self.system_prompt,
                user_prompt=user_prompt,
                max_tokens=max_tokens,
                temperature=temperature,
                images=images,
            )
        finally:
            if timing_recorder:
                duration_ms = (time.perf_counter() - response_start) * 1000
                timing_recorder.record_llm_call(
                    label="search_rag_answer",
                    duration_ms=duration_ms,
                    provider=getattr(self.llm_client, "provider", None),
                    model=getattr(self.llm_client, "model_id", None),
                )

        # æ„å»ºç­”æ¡ˆ
        answer = response.get("content")
        reference_hits = hits if reference_limit is None else hits[:reference_limit]
        
        if answer:
            # æ·»åŠ ç½‘ç»œæ¥æºå¼•ç”¨
            if reference_hits:
                answer += "\n\n**ç½‘ç»œæ¥æºï¼š**\n"
                for idx, hit in enumerate(reference_hits, start=1):
                    title = hit.title or f"Result {idx}"
                    url = hit.url or ""
                    bullet = f"{idx}. [{title}]({url})" if url else f"{idx}. {title}"
                    answer += f"{bullet}\n"
            
            # æ·»åŠ æœ¬åœ°æ–‡æ¡£å¼•ç”¨
            if retrieved_docs:
                answer += "\n\n**æœ¬åœ°æ–‡æ¡£æ¥æºï¼š**\n"
                for idx, doc in enumerate(retrieved_docs, start=1):
                    source = doc.source or f"æ–‡æ¡£ {idx}"
                    answer += f"{idx}. {source}\n"

        # æ„å»ºè¿”å›ç»“æœ
        payload: Dict[str, object] = {
            "query": query,
            "answer": answer,
            "search_hits": [asdict(hit) for hit in hits],
            "retrieved_docs": [asdict(doc) for doc in retrieved_docs],
            "llm_raw": response.get("raw"),
            "llm_warning": response.get("warning"),
            "llm_error": response.get("error"),
            "rerank": rerank_meta or None,
        }
        
        if search_error:
            payload["search_error"] = search_error
        if search_warnings:
            payload["search_warnings"] = search_warnings
        if ranking_data:
            payload["ranking_data"] = ranking_data

        return payload

    def answer_stream(
        self,
        query: str,
        *,
        search_query: Optional[str] = None,
        num_search_results: int = 5,
        per_source_limit: Optional[int] = None,
        num_retrieved_docs: int = 5,
        max_tokens: int = 5000,
        temperature: float = 0.3,
        enable_search: bool = True,
        enable_local_docs: bool = True,
        reference_limit: Optional[int] = None,
        freshness: Optional[str] = None,
        date_restrict: Optional[str] = None,
        timing_recorder: Optional[TimingRecorder] = None,
    ):
        import json
        
        # Prefer keyword-focused query generated upstream when available.
        effective_query = search_query.strip() if search_query else query

        per_source_cap = per_source_limit if per_source_limit is not None else num_search_results
        hits = []
        search_warnings: List[str] = []
        
        # æ‰§è¡Œæœç´¢
        if enable_search:
            hits = self.search_client.search(
                effective_query,
                num_results=num_search_results,
                per_source_limit=per_source_cap,
                freshness=freshness,
                date_restrict=date_restrict,
            )
            
            if timing_recorder:
                timings_getter = getattr(self.search_client, "get_last_timings", None)
                if callable(timings_getter):
                    timing_recorder.extend_search_timings(timings_getter())
            
            get_last_errors = getattr(self.search_client, "get_last_errors", None)
            if callable(get_last_errors):
                errors = get_last_errors() or []
                if hits and errors:
                    for item in errors:
                        source = str(item.get("source") or "æœç´¢æœåŠ¡")
                        detail = str(item.get("error") or "æœªçŸ¥é”™è¯¯")
                        if source.lower().startswith("mcp"):
                            search_warnings.append(f"{source} æœªæ­£å¸¸å·¥ä½œï¼Œå·²ä½¿ç”¨å…¶ä»–æœç´¢ç»“æœã€‚åŸå› ï¼š{detail}")
                        else:
                            search_warnings.append(f"{source} å‡ºç°å¼‚å¸¸ï¼š{detail}")
        
        # åº”ç”¨é‡æ’åº
        hits, rerank_meta = self._apply_rerank(query, hits, limit=num_search_results)

        # æ£€æŸ¥æ˜¯å¦éœ€è¦LLM fallbackï¼ˆé’ˆå¯¹æ—¶é—´å˜åŒ–ç±»æŸ¥è¯¢ï¼‰
        if enable_search and self._is_temporal_change_query(query):
            # å¯¹äºæ—¶é—´å˜åŒ–æŸ¥è¯¢ï¼Œæ€»æ˜¯å°è¯•æ‰§è¡Œé¢—ç²’åŒ–æœç´¢ä»¥è·å–æ›´å…¨é¢çš„å†å²æ•°æ®
            print("ğŸ”„ æ£€æµ‹åˆ°æ—¶é—´å˜åŒ–æŸ¥è¯¢ï¼Œå¯åŠ¨LLM fallbackæœºåˆ¶ä»¥è·å–å†å²æ•°æ®...")
            fallback_hits = self._perform_granular_search_fallback(query, effective_query, num_search_results, per_source_cap, freshness, date_restrict, timing_recorder)
            if fallback_hits:
                hits = fallback_hits
                print(f"âœ… Fallbackæœç´¢å®Œæˆï¼Œè·å¾—{len(fallback_hits)}æ¡ç»“æœ")

        # æ£€ç´¢æœ¬åœ°æ–‡æ¡£
        retrieved_docs: List[Document] = []
        if enable_local_docs and self.vector_store:
            retrieved_docs = self.vector_store.search(query, k=num_retrieved_docs)

        # First, yield preliminary data
        preliminary_data = {
            "query": query,
            "search_hits": [asdict(hit) for hit in hits],
            "retrieved_docs": [asdict(doc) for doc in retrieved_docs],
            "rerank": rerank_meta or None,
            "search_query": effective_query,
        }
        if search_warnings:
            preliminary_data["search_warnings"] = search_warnings
        
        yield json.dumps({"type": "preliminary", "data": preliminary_data})

        user_prompt = self.build_prompt(query, hits, retrieved_docs)
        response_start = time.perf_counter()
        
        # Stream the response
        full_answer = ""
        try:
            stream = self.llm_client.chat_stream(
                system_prompt=self.system_prompt,
                user_prompt=user_prompt,
                max_tokens=max_tokens,
                temperature=temperature,
            )
            for chunk in stream:
                if chunk.startswith("Error:"):
                    yield json.dumps({"type": "error", "data": chunk})
                    return
                full_answer += chunk
                yield json.dumps({"type": "content", "data": chunk})

        finally:
            if timing_recorder:
                duration_ms = (time.perf_counter() - response_start) * 1000
                timing_recorder.record_llm_call(
                    label="search_rag_answer_stream",
                    duration_ms=duration_ms,
                    provider=getattr(self.llm_client, "provider", None),
                    model=getattr(self.llm_client, "model_id", None),
                )

        # Finally, yield the references
        reference_hits = hits if reference_limit is None else hits[:reference_limit]
        if full_answer:
            # æ·»åŠ ç½‘ç»œæ¥æºå¼•ç”¨
            if reference_hits:
                reference_text = "\n\n**ç½‘ç»œæ¥æºï¼š**\n"
                for idx, hit in enumerate(reference_hits, start=1):
                    title = hit.title or f"Result {idx}"
                    url = hit.url or ""
                    bullet = f"{idx}. [{title}]({url})" if url else f"{idx}. {title}"
                    reference_text += f"{bullet}\n"
                yield json.dumps({"type": "references", "data": reference_text})
            
            # æ·»åŠ æœ¬åœ°æ–‡æ¡£å¼•ç”¨
            if retrieved_docs:
                reference_text = "\n\n**æœ¬åœ°æ–‡æ¡£æ¥æºï¼š**\n"
                for idx, doc in enumerate(retrieved_docs, start=1):
                    source = doc.source or f"æ–‡æ¡£ {idx}"
                    reference_text += f"{idx}. {source}\n"
                yield json.dumps({"type": "local_references", "data": reference_text})
