from __future__ import annotations

import time
from dataclasses import asdict
from typing import Dict, List, Optional, Tuple
from urllib.parse import urlparse

from api import HKGAIClient
from search import SearchClient, SearchHit
from rerank import BaseReranker
from timing_utils import TimingRecorder


DEFAULT_SYSTEM_PROMPT = (
    "You are an information assistant. "
    "Answer user questions concisely using ONLY the provided search results. "
    "CRITICAL: Do NOT fabricate, invent, or guess any specific data (such as scores, numbers, statistics, dates, or names) "
    "that is not EXPLICITLY stated in the search results. "
    "If specific information is not found in the search results, clearly state 'æœªåœ¨æœç´¢ç»“æœä¸­æ‰¾åˆ°å…·ä½“æ•°æ®' or 'specific data not found in search results'. "
    "When unsure, acknowledge the uncertainty instead of guessing. "
    "Always answer in the same language as the user's question."
)


class NoRAGBaseline:
    """Minimal pipeline that sends search snippets to the LLM without local retrieval."""

    def __init__(
        self,
        llm_client: HKGAIClient,
        search_client: SearchClient,
        system_prompt: str = DEFAULT_SYSTEM_PROMPT,
        *,
        reranker: Optional[BaseReranker] = None,
        min_rerank_score: float = 0.0,
        max_per_domain: int = 1,
    ) -> None:
        self.llm_client = llm_client
        self.search_client = search_client
        self.system_prompt = system_prompt
        self.reranker = reranker
        self.min_rerank_score = min_rerank_score
        self.max_per_domain = max(1, max_per_domain)

    def _format_search_hits(self, hits: List[SearchHit]) -> str:
        if not hits:
            return "No search results were returned."

        formatted_rows = []
        for idx, hit in enumerate(hits, start=1):
            snippet = hit.snippet or "No snippet available."
            url = hit.url or "No URL available."
            title = hit.title or f"Result {idx}"
            formatted_rows.append(
                f"{idx}. {title}\n"
                f"   URL: {url}\n"
                f"   Snippet: {snippet}"
            )
        return "\n".join(formatted_rows)
    
    def _is_temporal_change_query(self, query: str) -> bool:
        """æ£€æµ‹æŸ¥è¯¢æ˜¯å¦ä¸æ—¶é—´å˜åŒ–é¢†åŸŸç›¸å…³"""
        temporal_change_keywords = [
            # æ•™è‚²æ’åç›¸å…³
            "å¤§å­¦", "é«˜æ ¡", "å­¦é™¢", "å­¦æ ¡", "æ’å", "QS", "THE", "ARWU", "US News",
            "university", "college", "ranking", "rankings", "education", "higher education",
            "é¦™æ¸¯ä¸­æ–‡å¤§å­¸", "é¦™æ¸¯ç§‘æŠ€å¤§å­¸", "é¦™æ¸¯å¤§å­¸", "CUHK", "HKUST", "HKU",
            "é¦™æ¸¯ä¸­æ–‡å¤§å­¦", "é¦™æ¸¯ç§‘æŠ€å¤§å­¦", "é¦™æ¸¯å¤§å­¦",
            # æ—¶é—´å˜åŒ–ç›¸å…³
            "æœ€è¿‘10å¹´", "è¿‡å»10å¹´", "10å¹´", "åå¹´", "å†å¹´", "å†å²", "å˜åŒ–", "è¶‹åŠ¿", "å‘å±•",
            "10 years", "decade", "historical", "trend", "development", "evolution",
            "å¯¹æ¯”", "æ¯”è¾ƒ", "å˜åŒ–è¶‹åŠ¿", "æ—¶é—´åºåˆ—", "å¹´åº¦", "é€å¹´",
            "comparison", "compare", "trend over time", "time series", "yearly", "year by year",
            # å…¶ä»–å¯èƒ½çš„æ—¶é—´å˜åŒ–æŸ¥è¯¢
            "å¢é•¿", "ä¸‹é™", "æ³¢åŠ¨", "å˜åŒ–ç‡", "å¢é•¿ç‡", "æ¶¨è·Œ",
            "growth", "decline", "fluctuation", "rate of change", "growth rate", "rise and fall"
        ]
        query_lower = query.lower()
        return any(keyword in query_lower for keyword in temporal_change_keywords)
    
    def _should_fallback_to_granular_search(self, query: str, hits: List[SearchHit]) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥è¿›è¡Œé¢—ç²’åŒ–æœç´¢fallback"""
        if not hits:
            return True
            
        # ä½¿ç”¨LLMåˆ¤æ–­æœç´¢ç»“æœæ˜¯å¦æ»¡è¶³æŸ¥è¯¢éœ€æ±‚
        query_lower = query.lower()
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ—¶é—´å˜åŒ–ç›¸å…³çš„å…³é”®è¯
        time_keywords = ["æœ€è¿‘10å¹´", "è¿‡å»10å¹´", "10å¹´", "åå¹´", "10 years", "decade", "å†å¹´", "å†å²", "å˜åŒ–", "è¶‹åŠ¿"]
        is_time_query = any(kw in query_lower for kw in time_keywords)
        
        if not is_time_query:
            return False
            
        # æ£€æŸ¥æœç´¢ç»“æœä¸­æ˜¯å¦åŒ…å«è¶³å¤Ÿçš„æ—¶é—´å˜åŒ–æ•°æ®
        combined_snippets = " ".join(hit.snippet.lower() for hit in hits if hit.snippet)
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å¹´ä»½+æ’åçš„æ¨¡å¼
        import re
        year_rank_pattern = r'\b(20\d{2})\b.*?(?:rank|æ’å|position|#\d+|top\s*\d+)'
        has_year_rank_data = bool(re.search(year_rank_pattern, combined_snippets))
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å¤šä¸ªå¹´ä»½çš„æ•°æ®
        year_pattern = r'\b(20\d{2})\b'
        years_found = re.findall(year_pattern, combined_snippets)
        has_multiple_years = len(set(years_found)) >= 3  # è‡³å°‘3ä¸ªä¸åŒå¹´ä»½çš„æ•°æ®
        
        # å¦‚æœæ²¡æœ‰è¶³å¤Ÿçš„æ—¶é—´å˜åŒ–æ•°æ®ï¼Œåˆ™éœ€è¦fallback
        return not (has_year_rank_data or has_multiple_years)
    
    def _perform_granular_search_fallback(
        self, 
        original_query: str, 
        effective_query: str, 
        num_search_results: int, 
        per_source_cap: int,
        freshness: Optional[str],
        date_restrict: Optional[str],
        timing_recorder: Optional[TimingRecorder]
    ) -> List[SearchHit]:
        """æ‰§è¡Œé¢—ç²’åŒ–æœç´¢fallback"""
        import json
        from api import HKGAIClient
        
        # ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨LLMç”Ÿæˆæ›´å®½æ³›çš„æœç´¢æŸ¥è¯¢
        broad_search_prompt = (
            f"åŸå§‹æŸ¥è¯¢ï¼š{original_query}\n\n"
            "è¿™æ˜¯ä¸€ä¸ªå…³äºæ—¶é—´å˜åŒ–çš„æŸ¥è¯¢ï¼Œä½†åˆå§‹æœç´¢ç»“æœæ²¡æœ‰æä¾›è¶³å¤Ÿçš„å†å²æ•°æ®ã€‚"
            "è¯·ç”Ÿæˆä¸€ä¸ªæ›´å®½æ³›çš„æœç´¢æŸ¥è¯¢ï¼Œç”¨äºè·å–ç›¸å…³çš„å†å²æ•°æ®ã€‚\n\n"
            "ç”Ÿæˆè§„åˆ™ï¼š\n"
            "1. å¦‚æœæŸ¥è¯¢æ¶‰åŠå¤§å­¦æ’åï¼Œç”ŸæˆåŒ…å«'å†å²æ’å'ã€'å†å¹´æ’å'æˆ–'å†å¹´å˜åŒ–'çš„æŸ¥è¯¢\n"
            "2. å¦‚æœæŸ¥è¯¢æ¶‰åŠ10å¹´å˜åŒ–ï¼Œç”ŸæˆåŒ…å«2016-2025å¹´ä»½èŒƒå›´çš„æŸ¥è¯¢\n"
            "3. å¦‚æœæŸ¥è¯¢æ¶‰åŠå…¶ä»–æ—¶é—´å˜åŒ–ï¼Œç”ŸæˆåŒ…å«'å†å²è¶‹åŠ¿'ã€'å†å¹´æ•°æ®'æˆ–'æ—¶é—´åºåˆ—'çš„æŸ¥è¯¢\n"
            "4. å®½æ³›æŸ¥è¯¢åº”è¯¥æ›´é€šç”¨ï¼Œä½†ä»ç„¶ä¿æŒä¸åŸå§‹æŸ¥è¯¢çš„ç›¸å…³æ€§\n\n"
            "åªè¿”å›ä¸€ä¸ªJSONå¯¹è±¡ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š\n"
            '{\n'
            '  "broad_query": "æ›´å®½æ³›çš„æœç´¢æŸ¥è¯¢",\n'
            '  "years": ["2016", "2017", "2018", "2019", "2020", "2021", "2022", "2023", "2024", "2025"]\n'
            '}'
        )
        
        try:
            response = self.llm_client.chat(
                system_prompt="ä½ æ˜¯æœç´¢æŸ¥è¯¢ä¼˜åŒ–ä¸“å®¶ï¼Œæ“…é•¿ç”Ÿæˆæ›´å®½æ³›ä½†ç›¸å…³çš„æœç´¢æŸ¥è¯¢ã€‚",
                user_prompt=broad_search_prompt,
                max_tokens=200,
                temperature=0.3,
            )
            
            content = response.get("content", "")
            # å°è¯•è§£æJSON
            start = content.find("{")
            end = content.rfind("}") + 1
            if start != -1 and end > start:
                json_str = content[start:end]
                llm_result = json.loads(json_str)
                broad_query = llm_result.get("broad_query", effective_query)
                years = llm_result.get("years", [])
            else:
                broad_query = effective_query
                years = []
        except Exception as e:
            print(f"LLMç”Ÿæˆå®½æ³›æŸ¥è¯¢å¤±è´¥: {e}")
            broad_query = effective_query
            years = []
        
        # ç¬¬äºŒæ­¥ï¼šæ‰§è¡Œå®½æ³›æœç´¢
        print(f"ğŸ” æ‰§è¡Œå®½æ³›æœç´¢: {broad_query}")
        broad_hits = self.search_client.search(
            broad_query,
            num_results=num_search_results * 2,  # è·å–æ›´å¤šç»“æœä»¥ä¾¿ç­›é€‰
            per_source_limit=per_source_cap * 2,
            freshness=freshness,
            date_restrict=date_restrict,
        )
        
        # åˆ†æå®½æ³›æœç´¢ç»“æœä¸­çš„æ—¶é—´å˜åŒ–æ•°æ®
        if broad_hits:
            combined_snippets = " ".join(hit.snippet.lower() for hit in broad_hits if hit.snippet)
            import re
            
            # ç»Ÿè®¡æ‰¾åˆ°çš„å¹´ä»½æ•°é‡
            year_pattern = r'\b(20\d{2})\b'
            years_found = re.findall(year_pattern, combined_snippets)
            unique_years = set(years_found)
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«æ’å/ä½ç½®ä¿¡æ¯
            rank_indicators = ['rank', 'æ’å', 'position', '#', 'top', 'ranking']
            has_rank_data = any(indicator in combined_snippets for indicator in rank_indicators)
            
            print(f"ğŸ“Š å®½æ³›æœç´¢åˆ†æï¼šæ‰¾åˆ° {len(unique_years)} ä¸ªä¸åŒå¹´ä»½çš„æ•°æ®ï¼ŒåŒ…å«æ’åä¿¡æ¯: {has_rank_data}")
            
            # å¯¹äºæ—¶é—´å˜åŒ–æŸ¥è¯¢ï¼Œæ€»æ˜¯æ‰§è¡Œé¢—ç²’åŒ–æœç´¢ä»¥è·å–æ›´å®Œæ•´çš„æ•°æ®
            # æ³¨é‡Šæ‰æ—©æœŸè¿”å›ï¼Œç¡®ä¿æ€»æ˜¯æ‰§è¡Œé¢—ç²’åŒ–æœç´¢
            # if len(unique_years) >= 3 and has_rank_data:
            #     print("âœ… å®½æ³›æœç´¢å·²æ‰¾åˆ°è¶³å¤Ÿçš„æ—¶é—´å˜åŒ–æ•°æ®ï¼Œæ— éœ€è¿›ä¸€æ­¥é¢—ç²’åŒ–æœç´¢")
            #     return broad_hits[:num_search_results * 2]
            print(f"ğŸ“Š å®½æ³›æœç´¢åˆ†æï¼šæ‰¾åˆ° {len(unique_years)} ä¸ªä¸åŒå¹´ä»½çš„æ•°æ®ï¼ŒåŒ…å«æ’åä¿¡æ¯: {has_rank_data}")
            print("ğŸ”„ ç»§ç»­æ‰§è¡Œé¢—ç²’åŒ–æœç´¢ä»¥è·å–æ›´å®Œæ•´çš„å†å²æ•°æ®...")
        
        # ç¬¬ä¸‰æ­¥ï¼šä»å®½æ³›æœç´¢ç»“æœä¸­æå–å¹´ä»½ä¿¡æ¯
        if not years:
            # å¦‚æœLLMæ²¡æœ‰æä¾›å¹´ä»½ï¼Œå°è¯•ä»æŸ¥è¯¢ä¸­æå–
            import re
            year_range_match = re.search(r'(20\d{2})\s*[-è‡³åˆ°]\s*(20\d{2})', original_query)
            if year_range_match:
                start_year = int(year_range_match.group(1))
                end_year = int(year_range_match.group(2))
                years = [str(year) for year in range(start_year, end_year + 1)]
            else:
                # é»˜è®¤ä½¿ç”¨æœ€è¿‘10å¹´
                current_year = 2025
                years = [str(year) for year in range(current_year - 9, current_year + 1)]
        
        # ç¬¬å››æ­¥ï¼šæ‰§è¡Œé¢—ç²’åŒ–æœç´¢ï¼ˆå¯¹äºæ—¶é—´å˜åŒ–æŸ¥è¯¢æ€»æ˜¯æ‰§è¡Œï¼‰
        if years:  # ç§»é™¤_should_fallback_to_granular_searchæ£€æŸ¥ï¼Œç¡®ä¿æ€»æ˜¯æ‰§è¡Œé¢—ç²’åŒ–æœç´¢
            print("ğŸ” å¼€å§‹é¢—ç²’åŒ–æœç´¢...")
            granular_hits = []
            
            # åªä½¿ç”¨Googleæœç´¢è¿›è¡Œé¢—ç²’åŒ–æŸ¥è¯¢
            google_client = None
            print(f"ğŸ” æŸ¥æ‰¾Googleæœç´¢å®¢æˆ·ç«¯...")
            print(f"   search_clientç±»å‹: {type(self.search_client)}")
            print(f"   search_clientå±æ€§: {dir(self.search_client)}")
            
            # æ£€æŸ¥search_clientæ˜¯å¦æ˜¯CombinedSearchClient
            if hasattr(self.search_client, "clients"):
                print(f"   æ‰¾åˆ°clientså±æ€§ï¼Œå®¢æˆ·ç«¯æ•°é‡: {len(self.search_client.clients)}")
                for i, client in enumerate(self.search_client.clients):
                    print(f"   å®¢æˆ·ç«¯ {i}: {type(client)}")
                    if hasattr(client, "source_id"):
                        print(f"      source_id: {client.source_id}")
                    if hasattr(client, "source_id") and client.source_id == "google":
                        google_client = client
                        print(f"   âœ… æ‰¾åˆ°Googleæœç´¢å®¢æˆ·ç«¯!")
            else:
                print(f"   âŒ search_clientæ²¡æœ‰clientså±æ€§")
            
            if google_client:
                # ä¼˜åŒ–é¢—ç²’åŒ–æŸ¥è¯¢ï¼šæ™ºèƒ½é€‰æ‹©å…³é”®å¹´ä»½
                selected_years = years
                if len(years) > 6:
                    # æ™ºèƒ½é€‰æ‹©ç­–ç•¥ï¼šé€‰æ‹©å¼€å§‹å¹´ä»½ã€ç»“æŸå¹´ä»½å’Œä¸­é—´çš„å‡ ä¸ªå…³é”®å¹´ä»½
                    # å¯¹äº10å¹´æŸ¥è¯¢ï¼Œé€‰æ‹©ç¬¬1å¹´ã€ç¬¬3å¹´ã€ç¬¬5å¹´ã€ç¬¬7å¹´ã€ç¬¬10å¹´
                    if len(years) == 10:
                        selected_years = [years[0], years[2], years[4], years[6], years[-1]]
                    else:
                        # å¯¹äºå…¶ä»–é•¿åº¦çš„å¹´ä»½åˆ—è¡¨ï¼Œå‡åŒ€åˆ†å¸ƒé€‰æ‹©
                        step = max(1, len(years) // 5)
                        selected_years = [years[i] for i in range(0, len(years), step)]
                        if years[-1] not in selected_years:
                            selected_years.append(years[-1])
                    
                    print(f"ğŸ“… ä¼˜åŒ–é¢—ç²’åŒ–æœç´¢ï¼Œé€‰æ‹©å…³é”®å¹´ä»½: {selected_years}")
                else:
                    print(f"ğŸ“… æ‰§è¡Œé¢—ç²’åŒ–æœç´¢ï¼Œå¹´ä»½: {selected_years}")
                
                # ä¸ºæ¯ä¸ªé€‰å®šçš„å¹´ä»½ç”ŸæˆæŸ¥è¯¢
                for year in selected_years:
                    # æ™ºèƒ½ç”Ÿæˆæ›´ç²¾ç¡®çš„å¹´ä»½æŸ¥è¯¢
                    query_lower = original_query.lower()
                    
                    # æå–æŸ¥è¯¢ä¸­çš„å…³é”®å®ä½“ï¼ˆå¤§å­¦åç§°ç­‰ï¼‰
                    import re
                    # æå–å¤§å­¦åç§°
                    universities = []
                    if "é¦™æ¸¯ä¸­æ–‡å¤§å­¸" in original_query or "é¦™æ¸¯ä¸­æ–‡å¤§å­¦" in original_query:
                        universities.append("Chinese University of Hong Kong")
                        universities.append("CUHK")
                    if "é¦™æ¸¯ç§‘æŠ€å¤§å­¸" in original_query or "é¦™æ¸¯ç§‘æŠ€å¤§å­¦" in original_query:
                        universities.append("Hong Kong University of Science and Technology")
                        universities.append("HKUST")
                    
                    # æ ¹æ®æŸ¥è¯¢ç±»å‹ç”Ÿæˆä¸åŒçš„å¹´ä»½æŸ¥è¯¢
                    if "qs" in query_lower and ("æ’å" in original_query or "ranking" in query_lower):
                        # QSæ’åæŸ¥è¯¢ - ç”Ÿæˆæ›´ç®€æ´çš„æŸ¥è¯¢
                        if universities:
                            # å¦‚æœæœ‰å…·ä½“çš„å¤§å­¦ï¼ŒæŸ¥è¯¢è¿™äº›å¤§å­¦çš„QSæ’å
                            for uni in universities:
                                year_query = f"QS world university rankings {year} {uni}"
                                print(f"ğŸ” æœç´¢å¹´ä»½ {year}: {year_query}")
                                try:
                                    year_hits = google_client.search(
                                        year_query,
                                        num_results=max(2, num_search_results // (len(selected_years) * len(universities))),
                                        freshness=freshness,
                                        date_restrict=f"{year}-01-01..{year}-12-31",  # é™åˆ¶åœ¨ç‰¹å®šå¹´ä»½å†…
                                    )
                                    granular_hits.extend(year_hits)
                                except Exception as e:
                                    print(f"å¹´ä»½ {year} æœç´¢å¤±è´¥: {e}")
                            
                            # é¢å¤–æŸ¥è¯¢ï¼šé¦™æ¸¯å¤§å­¦QSæ’åï¼ˆä½œä¸ºå‚è€ƒï¼‰
                            hk_query = f"QS world university rankings {year} Hong Kong universities ranking"
                            print(f"ğŸ” æœç´¢å¹´ä»½ {year} (é¦™æ¸¯å¤§å­¦æ’å): {hk_query}")
                            try:
                                hk_hits = google_client.search(
                                    hk_query,
                                    num_results=2,
                                    freshness=freshness,
                                    date_restrict=f"{year}-01-01..{year}-12-31",  # é™åˆ¶åœ¨ç‰¹å®šå¹´ä»½å†…
                                )
                                granular_hits.extend(hk_hits)
                            except Exception as e:
                                print(f"å¹´ä»½ {year} é¦™æ¸¯å¤§å­¦æ’åæœç´¢å¤±è´¥: {e}")
                        else:
                            # å¦‚æœæ²¡æœ‰å…·ä½“å¤§å­¦ï¼ŒæŸ¥è¯¢QSæ’åæ€»ä½“æƒ…å†µ
                            year_query = f"QS world university rankings {year}"
                            print(f"ğŸ” æœç´¢å¹´ä»½ {year}: {year_query}")
                            try:
                                year_hits = google_client.search(
                                    year_query,
                                    num_results=max(3, num_search_results // len(selected_years)),
                                    freshness=freshness,
                                    date_restrict=f"{year}-01-01..{year}-12-31",  # é™åˆ¶åœ¨ç‰¹å®šå¹´ä»½å†…
                                )
                                granular_hits.extend(year_hits)
                            except Exception as e:
                                print(f"å¹´ä»½ {year} æœç´¢å¤±è´¥: {e}")
                        continue  # è·³è¿‡åç»­çš„é€šç”¨æŸ¥è¯¢é€»è¾‘
                    elif "the" in query_lower and ("æ’å" in original_query or "ranking" in query_lower):
                        # THEæ’åæŸ¥è¯¢
                        year_query = f"THE world university rankings {year}"
                    elif "arwu" in query_lower or "è½¯ç§‘" in original_query:
                        # ARWUæ’åæŸ¥è¯¢
                        year_query = f"ARWU academic ranking of world universities {year}"
                    elif "æ’å" in original_query or "ranking" in query_lower:
                        # é€šç”¨æ’åæŸ¥è¯¢
                        year_query = f"university rankings {year}"
                    elif "å¤§å­¦" in original_query or "university" in query_lower:
                        # å¤§å­¦ç›¸å…³æŸ¥è¯¢
                        year_query = f"university {year}"
                    else:
                        # é€šç”¨æŸ¥è¯¢
                        year_query = f"{original_query} {year}"
                    
                    print(f"ğŸ” æœç´¢å¹´ä»½ {year}: {year_query}")
                    
                    try:
                        year_hits = google_client.search(
                            year_query,
                            num_results=max(3, num_search_results // len(selected_years)),
                            freshness=freshness,
                            date_restrict=f"{year}-01-01..{year}-12-31",  # é™åˆ¶åœ¨ç‰¹å®šå¹´ä»½å†…
                        )
                        granular_hits.extend(year_hits)
                    except Exception as e:
                        print(f"å¹´ä»½ {year} æœç´¢å¤±è´¥: {e}")
                
                # åˆå¹¶å®½æ³›æœç´¢å’Œé¢—ç²’åŒ–æœç´¢ç»“æœï¼Œä¼˜å…ˆä¿ç•™é¢—ç²’åŒ–æœç´¢ç»“æœ
                all_hits = granular_hits + broad_hits
                
                # æ™ºèƒ½å»é‡ï¼šä¿ç•™æ›´ç›¸å…³çš„ç»“æœ
                seen_urls = set()
                deduped_hits = []
                for hit in all_hits:
                    url_key = hit.url or ""
                    if url_key not in seen_urls:
                        seen_urls.add(url_key)
                        deduped_hits.append(hit)
                
                # æŒ‰ç›¸å…³æ€§æ’åºï¼šä¼˜å…ˆåŒ…å«å¹´ä»½å’Œæ’åä¿¡æ¯çš„ç»“æœ
                def hit_relevance_score(hit):
                    score = 0
                    if hit.snippet:
                        snippet = hit.snippet.lower()
                        # æ£€æŸ¥æ˜¯å¦åŒ…å«å¹´ä»½
                        import re
                        years_in_snippet = re.findall(r'\b(20\d{2})\b', snippet)
                        score += len(years_in_snippet) * 2
                        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ’åä¿¡æ¯
                        rank_keywords = ['rank', 'æ’å', 'position', '#', 'top']
                        score += sum(1 for kw in rank_keywords if kw in snippet)
                    return score
                
                deduped_hits.sort(key=hit_relevance_score, reverse=True)
                
                print(f"âœ… é¢—ç²’åŒ–æœç´¢å®Œæˆï¼Œå…±è·å¾— {len(deduped_hits)} æ¡ç»“æœ")
                return deduped_hits[:num_search_results * 2]  # è¿”å›æ›´å¤šç»“æœä»¥ä¾¿ç­›é€‰
            else:
                print("âš ï¸ æœªæ‰¾åˆ°Googleæœç´¢å®¢æˆ·ç«¯ï¼Œæ— æ³•æ‰§è¡Œé¢—ç²’åŒ–æœç´¢")
                return broad_hits
        else:
            return broad_hits

    def build_prompt(self, query: str, hits: List[SearchHit], ranking_info: str = "") -> str:
        context_block = self._format_search_hits(hits)
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æ’åæŸ¥è¯¢
        is_ranking_query = any(keyword in query.lower() for keyword in ['æ’å', 'ranking', 'rank'])
        
        if is_ranking_query:
            special_instructions = (
                "SPECIAL INSTRUCTIONS FOR RANKING QUERIES:\n"
                "1. Carefully extract ALL ranking data from the search results, even if it's not in a standard format.\n"
                "2. Look for patterns like 'Year #Rank', 'Year: Rank', 'Year ranked', 'Year position', 'Rank #Year', etc.\n"
                "3. Pay special attention to university official websites which often contain ranking tables.\n"
                "4. Extract ranking information from titles, snippets, and any visible text.\n"
                "5. For university rankings, look for both global rankings and regional rankings.\n"
                "6. If ranking data is scattered across multiple results, compile it into a coherent comparison table.\n"
                "7. If specific years are missing from the results, explicitly mention which years are not covered.\n"
                "8. For Chinese universities, look for both English and Chinese names (CUHK/é¦™æ¸¯ä¸­æ–‡å¤§å­¸, HKUST/é¦™æ¸¯ç§‘æŠ€å¤§å­¸).\n"
                "9. If ranking data is provided below, use it to create a comprehensive comparison table.\n\n"
            )
            # For ranking queries, modify the important rules to allow using pre-extracted ranking data
            important_rules = (
                "IMPORTANT RULES FOR RANKING QUERIES:\n"
                "1. You may use specific ranking data that is EXPLICITLY mentioned in the search results below.\n"
                "2. You may ALSO use the pre-extracted ranking data provided below the search results.\n"
                "3. If ranking data is missing for certain years, explicitly mention which years are not covered.\n"
                "4. DO NOT guess or invent ranking numbers that are not found in either the search results or the pre-extracted data.\n"
                "5. Create a comprehensive comparison table using all available ranking data.\n\n"
            )
        else:
            special_instructions = ""
            important_rules = (
                "IMPORTANT RULES:\n"
                "1. ONLY include specific data (scores, statistics, numbers, names) that are EXPLICITLY mentioned in the search results below.\n"
                "2. If specific data (like individual player scores, detailed statistics) is NOT found in the search results, "
                "say 'æœç´¢ç»“æœä¸­æœªæåŠå…·ä½“æ•°æ®' or 'not mentioned in search results' - DO NOT guess or invent numbers.\n"
                "3. For sports queries: only report scores and statistics that appear verbatim in the snippets.\n\n"
            )
        
        return (
            "You are given a set of search results. "
            "Use them to answer the question at the end. "
            "When citing sources, use the format (URL 1), (URL 2), etc., "
            "where the number corresponds to the search result number.\n\n"
            f"{important_rules}"
            f"{special_instructions}"
            f"Search Results:\n{context_block}\n"
            f"{ranking_info}\n\n"
            f"Question: {query}\n\n"
            "Answer (remember: NO fabricated data):"
        )

    def _extract_ranking_data(self, hits: List[SearchHit]) -> Dict[str, object]:
        """Extract ranking data from search hits for university ranking queries."""
        import re
        
        cuhk_rankings = {}
        hkust_rankings = {}
        other_rankings = {}
        
        # ä¿¡ä»»åº¦è¯„åˆ†ï¼šå®˜æ–¹å¤§å­¦ç½‘ç«™ > QSå®˜æ–¹ç½‘ç«™ > æ–°é—»åª’ä½“ > å…¶ä»–
        def get_source_trust_score(url: str, title: str) -> int:
            """æ ¹æ®URLå’Œæ ‡é¢˜è¯„ä¼°æ¥æºçš„ä¿¡ä»»åº¦"""
            if not url:
                return 1
            
            url_lower = url.lower()
            title_lower = title.lower() if title else ""
            
            # å®˜æ–¹å¤§å­¦ç½‘ç«™
            if 'cuhk.edu.hk' in url_lower or 'cuhk.edu.cn' in url_lower:
                return 10
            if 'hkust.edu.hk' in url_lower:
                return 10
            
            # QSå®˜æ–¹ç½‘ç«™
            if 'topuniversities.com' in url_lower and 'university-rankings' in url_lower:
                return 9
            
            # çŸ¥åæ•™è‚²åª’ä½“
            if 'timeshighereducation.com' in url_lower:
                return 8
            if 'scmp.com' in url_lower:
                return 7
            
            # ä¸€èˆ¬æ–°é—»åª’ä½“
            if any(domain in url_lower for domain in ['news', 'reuters', 'bbc', 'cnn']):
                return 5
            
            # å…¶ä»–æ¥æº
            return 3
        
        for i, hit in enumerate(hits, 1):
            title = hit.title if hit.title else ''
            snippet = hit.snippet if hit.snippet else ''
            url = hit.url if hit.url else ''
            text = f"{title} {snippet}"
            
            # è·å–æ¥æºä¿¡ä»»åº¦
            trust_score = get_source_trust_score(url, title)
            
            # æ£€æŸ¥æ˜¯å¦ä¸CUHKç›¸å…³
            is_cuhk = ('cuhk' in text.lower() or 'chinese university of hong kong' in text.lower() or 
                       'é¦™æ¸¯ä¸­æ–‡' in text or 'é¦™æ¸¯ä¸­æ–‡å¤§å­¸' in text)
            
            # æ£€æŸ¥æ˜¯å¦ä¸HKUSTç›¸å…³
            is_hkust = ('hkust' in text.lower() or 'hong kong university of science and technology' in text.lower() or 
                        'é¦™æ¸¯ç§‘æŠ€' in text or 'é¦™æ¸¯ç§‘æŠ€å¤§å­¸' in text)
            
            # æå–æ’åä¿¡æ¯çš„å¤šç§æ¨¡å¼ï¼Œä¼˜å…ˆä½¿ç”¨æ›´ç²¾ç¡®çš„æ¨¡å¼
            rank_patterns = [
                # é«˜å¯ä¿¡åº¦æ¨¡å¼ï¼šæ˜ç¡®çš„å¹´ä»½å’Œæ’åç»„åˆ
                (r'(20\d{2})[^0-9]*#?(\d{1,3})', 9),  # 2020 #42
                (r'(20\d{2})[^0-9]*ranked?[^0-9]*(\d{1,3})', 8),  # 2020 ranked 42
                (r'(20\d{2})[^0-9]*æ’å[^0-9]*(\d{1,3})', 8),  # 2020 æ’å 42
                (r'#?(\d{1,3})[^0-9]*(20\d{2})', 7),  # #42 2020
                (r'ranked?[^0-9]*(\d{1,3})[^0-9]*(20\d{2})', 7),  # ranked 42 2020
                (r'æ’å[^0-9]*(\d{1,3})[^0-9]*(20\d{2})', 7),  # æ’å 42 2020
                
                # ä¸­ç­‰å¯ä¿¡åº¦æ¨¡å¼ï¼šQSç›¸å…³
                (r'QS World University Rankings[^0-9]*(\d{1,3})', 6),  # QS World University Rankings 42
                (r'QS.*?(\d{1,3})', 5),  # QS #42
                
                # ä½å¯ä¿¡åº¦æ¨¡å¼ï¼šå•ç‹¬çš„æ’åä¿¡æ¯
                (r'(\d{1,3})', 3),  # å•ç‹¬çš„æ•°å­—
            ]
            
            # å¯¹æ¯ä¸ªæ¨¡å¼è¿›è¡ŒåŒ¹é…
            for pattern, pattern_score in rank_patterns:
                matches = re.findall(pattern, text)
                for match in matches:
                    # å¤„ç†åŒ¹é…ç»“æœ
                    if isinstance(match, tuple) and len(match) == 2:
                        # åŒ…å«å¹´ä»½å’Œæ’åçš„æƒ…å†µ
                        year_str, rank_str = match
                        if year_str.isdigit() and rank_str.isdigit():
                            year = int(year_str)
                            rank = int(rank_str)
                            
                            # éªŒè¯å¹´ä»½å’Œæ’åçš„åˆç†æ€§
                            if 2000 <= year <= 2030 and 1 <= rank <= 500:
                                # é¢å¤–éªŒè¯è§„åˆ™ï¼šè¿‡æ»¤æ˜æ˜¾ä¸åˆç†çš„æ’å
                                # å¯¹äºCUHKå’ŒHKUSTè¿™æ ·çš„é¡¶å°–å¤§å­¦ï¼Œä¸–ç•Œæ’åé€šå¸¸åœ¨1-100ä¹‹é—´
                                # æ’ååœ¨200+çš„å¯èƒ½æ˜¯ç‰¹å®šé¢†åŸŸæ’åæˆ–åœ°åŒºæ’åï¼Œéœ€è¦æ›´ä¸¥æ ¼çš„éªŒè¯
                                is_reasonable_rank = True
                                if rank > 150:
                                    # å¯¹äºé«˜æ’åæ•°å­—ï¼Œæ£€æŸ¥æ˜¯å¦åŒ…å«ç‰¹å®šå…³é”®è¯
                                    if not any(keyword in text.lower() for keyword in 
                                              ['asia', 'äºšæ´²', 'subject', 'å­¦ç§‘', 'faculty', 'å­¦é™¢', 'engineering', 'å·¥ç¨‹']):
                                        # å¦‚æœæ²¡æœ‰æ˜ç¡®è¯´æ˜æ˜¯åœ°åŒºæ’åæˆ–å­¦ç§‘æ’åï¼Œé™ä½ä¿¡ä»»åº¦
                                        combined_score = trust_score + pattern_score - 5
                                        print(f"è­¦å‘Š: {year}å¹´æ’å#{rank}å¯èƒ½ä¸æ˜¯å…¨çƒæ’åï¼Œé™ä½ä¿¡ä»»åº¦")
                                
                                # è®¡ç®—ç»¼åˆä¿¡ä»»åº¦
                                combined_score = trust_score + pattern_score
                                
                                # åªæ¥å—é«˜ä¿¡ä»»åº¦çš„æ•°æ®
                                if combined_score >= 10:  # åªæ¥å—é«˜ä¿¡ä»»åº¦çš„æ•°æ®
                                    if is_cuhk:
                                        # å¦‚æœè¯¥å¹´ä»½å·²æœ‰æ•°æ®ï¼Œåªä¿ç•™æ›´é«˜ä¿¡ä»»åº¦çš„æ•°æ®
                                        if year not in cuhk_rankings or combined_score > cuhk_rankings[year][1]:
                                            cuhk_rankings[year] = (rank, combined_score)
                                            print(f"æå–åˆ°CUHKæ’å: {year}å¹´ #{rank} (ä¿¡ä»»åº¦: {combined_score}, æ¥æº: URL {i})")
                                    elif is_hkust:
                                        if year not in hkust_rankings or combined_score > hkust_rankings[year][1]:
                                            hkust_rankings[year] = (rank, combined_score)
                                            print(f"æå–åˆ°HKUSTæ’å: {year}å¹´ #{rank} (ä¿¡ä»»åº¦: {combined_score}, æ¥æº: URL {i})")
                    elif isinstance(match, str) and match.isdigit():
                        # åªæœ‰æ’åçš„æƒ…å†µï¼Œå°è¯•ä»æ–‡æœ¬ä¸­æå–å¹´ä»½
                        rank = int(match)
                        if 1 <= rank <= 500:
                            # å°è¯•ä»æ–‡æœ¬ä¸­æå–å¹´ä»½
                            year_matches = re.findall(r'\b(20\d{2})\b', text)
                            for year_str in year_matches:
                                year = int(year_str)
                                if 2000 <= year <= 2030:
                                    # é¢å¤–éªŒè¯è§„åˆ™ï¼šè¿‡æ»¤æ˜æ˜¾ä¸åˆç†çš„æ’å
                                    if rank > 150:
                                        # å¯¹äºé«˜æ’åæ•°å­—ï¼Œæ£€æŸ¥æ˜¯å¦åŒ…å«ç‰¹å®šå…³é”®è¯
                                        if not any(keyword in text.lower() for keyword in 
                                                  ['asia', 'äºšæ´²', 'subject', 'å­¦ç§‘', 'faculty', 'å­¦é™¢', 'engineering', 'å·¥ç¨‹']):
                                            # å¦‚æœæ²¡æœ‰æ˜ç¡®è¯´æ˜æ˜¯åœ°åŒºæ’åæˆ–å­¦ç§‘æ’åï¼Œé™ä½ä¿¡ä»»åº¦
                                            pattern_score_adjusted = pattern_score - 5
                                            print(f"è­¦å‘Š: {year}å¹´æ’å#{rank}å¯èƒ½ä¸æ˜¯å…¨çƒæ’åï¼Œé™ä½ä¿¡ä»»åº¦")
                                        else:
                                            pattern_score_adjusted = pattern_score
                                    else:
                                        pattern_score_adjusted = pattern_score
                                    
                                    # è®¡ç®—ç»¼åˆä¿¡ä»»åº¦
                                    combined_score = trust_score + pattern_score_adjusted
                                    
                                    # åªæ¥å—é«˜ä¿¡ä»»åº¦çš„æ•°æ®
                                    if combined_score >= 10:  # åªæ¥å—é«˜ä¿¡ä»»åº¦çš„æ•°æ®
                                        if is_cuhk:
                                            if year not in cuhk_rankings or combined_score > cuhk_rankings[year][1]:
                                                cuhk_rankings[year] = (rank, combined_score)
                                                print(f"æå–åˆ°CUHKæ’å: {year}å¹´ #{rank} (ä¿¡ä»»åº¦: {combined_score}, æ¥æº: URL {i})")
                                        elif is_hkust:
                                            if year not in hkust_rankings or combined_score > hkust_rankings[year][1]:
                                                hkust_rankings[year] = (rank, combined_score)
                                                print(f"æå–åˆ°HKUSTæ’å: {year}å¹´ #{rank} (ä¿¡ä»»åº¦: {combined_score}, æ¥æº: URL {i})")
        
        # æŸ¥æ‰¾åŒ…å«"top"çš„æ’åä¿¡æ¯
        for i, hit in enumerate(hits, 1):
            title = hit.title if hit.title else ''
            snippet = hit.snippet if hit.snippet else ''
            url = hit.url if hit.url else ''
            text = f"{title} {snippet}"
            
            # è·å–æ¥æºä¿¡ä»»åº¦
            trust_score = get_source_trust_score(url, title)
            
            # æŸ¥æ‰¾åŒ…å«"top"çš„æ’åä¿¡æ¯
            top_pattern = r'(20\d{2})[^0-9]*top\s*(\d{1,3})'
            matches = re.findall(top_pattern, text.lower())
            for year_str, rank_str in matches:
                if year_str.isdigit() and rank_str.isdigit():
                    year = int(year_str)
                    rank = int(rank_str)
                    
                    # éªŒè¯å¹´ä»½å’Œæ’åçš„åˆç†æ€§
                    if 2000 <= year <= 2030 and 1 <= rank <= 500:
                        # é¢å¤–éªŒè¯è§„åˆ™ï¼šè¿‡æ»¤æ˜æ˜¾ä¸åˆç†çš„æ’å
                        top_pattern_score = 6  # "top"æ¨¡å¼çš„ä¿¡ä»»åº¦
                        if rank > 150:
                            # å¯¹äºé«˜æ’åæ•°å­—ï¼Œæ£€æŸ¥æ˜¯å¦åŒ…å«ç‰¹å®šå…³é”®è¯
                            if not any(keyword in text.lower() for keyword in 
                                      ['asia', 'äºšæ´²', 'subject', 'å­¦ç§‘', 'faculty', 'å­¦é™¢', 'engineering', 'å·¥ç¨‹']):
                                # å¦‚æœæ²¡æœ‰æ˜ç¡®è¯´æ˜æ˜¯åœ°åŒºæ’åæˆ–å­¦ç§‘æ’åï¼Œé™ä½ä¿¡ä»»åº¦
                                top_pattern_score -= 5
                                print(f"è­¦å‘Š: {year}å¹´Top {rank}å¯èƒ½ä¸æ˜¯å…¨çƒæ’åï¼Œé™ä½ä¿¡ä»»åº¦")
                        
                        # è®¡ç®—ç»¼åˆä¿¡ä»»åº¦
                        combined_score = trust_score + top_pattern_score
                        
                        # åªæ¥å—é«˜ä¿¡ä»»åº¦çš„æ•°æ®
                        if combined_score >= 10:
                            is_cuhk = ('cuhk' in text.lower() or 'chinese university of hong kong' in text.lower() or 
                                       'é¦™æ¸¯ä¸­æ–‡' in text or 'é¦™æ¸¯ä¸­æ–‡å¤§å­¸' in text)
                            is_hkust = ('hkust' in text.lower() or 'hong kong university of science and technology' in text.lower() or 
                                         'é¦™æ¸¯ç§‘æŠ€' in text or 'é¦™æ¸¯ç§‘æŠ€å¤§å­¸' in text)
                            
                            if is_cuhk:
                                if year not in cuhk_rankings or combined_score > cuhk_rankings[year][1]:
                                    cuhk_rankings[year] = (rank, combined_score)
                                    print(f"æå–åˆ°CUHKæ’å(Top): {year}å¹´ Top {rank} (ä¿¡ä»»åº¦: {combined_score}, æ¥æº: URL {i})")
                            elif is_hkust:
                                if year not in hkust_rankings or combined_score > hkust_rankings[year][1]:
                                    hkust_rankings[year] = (rank, combined_score)
                                    print(f"æå–åˆ°HKUSTæ’å(Top): {year}å¹´ Top {rank} (ä¿¡ä»»åº¦: {combined_score}, æ¥æº: URL {i})")
        
        # æå–æ’åæ•°æ®ï¼Œåªä¿ç•™æ’åå€¼ï¼ˆå»æ‰ä¿¡ä»»åº¦åˆ†æ•°ï¼‰
        cuhk_final = {year: data[0] for year, data in cuhk_rankings.items()}
        hkust_final = {year: data[0] for year, data in hkust_rankings.items()}
        
        return {
            'cuhk_rankings': cuhk_final,
            'hkust_rankings': hkust_final,
            'other_rankings': other_rankings
        }

    def answer(
        self,
        query: str,
        *,
        search_query: Optional[str] = None,
        num_search_results: int = 5,
        per_source_limit: Optional[int] = None,
        max_tokens: int = 5000,
        temperature: float = 0.3,
        freshness: Optional[str] = None,
        date_restrict: Optional[str] = None,
        timing_recorder: Optional[TimingRecorder] = None,
        reference_limit: Optional[int] = None,
        images: Optional[List[Dict[str, str]]] = None,
    ) -> Dict[str, object]:
        # Prefer keyword-focused query generated upstream when available.
        effective_query = search_query.strip() if search_query else query

        per_source_cap = per_source_limit if per_source_limit is not None else num_search_results
        hits = self.search_client.search(
            effective_query,
            num_results=num_search_results,
            per_source_limit=per_source_cap,
            freshness=freshness,
            date_restrict=date_restrict,
        )
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦LLM fallbackï¼ˆé’ˆå¯¹æ—¶é—´å˜åŒ–ç±»æŸ¥è¯¢ï¼‰
        if self._is_temporal_change_query(query):
            # å¯¹äºæ—¶é—´å˜åŒ–æŸ¥è¯¢ï¼Œæ€»æ˜¯å°è¯•æ‰§è¡Œé¢—ç²’åŒ–æœç´¢ä»¥è·å–æ›´å…¨é¢çš„å†å²æ•°æ®
            print("ğŸ”„ æ£€æµ‹åˆ°æ—¶é—´å˜åŒ–æŸ¥è¯¢ï¼Œå¯åŠ¨LLM fallbackæœºåˆ¶ä»¥è·å–å†å²æ•°æ®...")
            fallback_hits = self._perform_granular_search_fallback(query, effective_query, num_search_results, per_source_cap, freshness, date_restrict, timing_recorder)
            if fallback_hits:
                hits = fallback_hits
                print(f"âœ… Fallbackæœç´¢å®Œæˆï¼Œè·å¾—{len(fallback_hits)}æ¡ç»“æœ")
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æ’åæŸ¥è¯¢ï¼Œå¦‚æœæ˜¯åˆ™æå–æ’åæ•°æ®
        is_ranking_query = any(keyword in query.lower() for keyword in ['æ’å', 'ranking', 'rank'])
        ranking_data = None
        if is_ranking_query:
            print("ğŸ” æ£€æµ‹åˆ°æ’åæŸ¥è¯¢ï¼Œæå–æ’åæ•°æ®...")
            ranking_data = self._extract_ranking_data(hits)
            print(f"âœ… æå–åˆ°CUHKæ’åæ•°æ®: {ranking_data['cuhk_rankings']}")
            print(f"âœ… æå–åˆ°HKUSTæ’åæ•°æ®: {ranking_data['hkust_rankings']}")
        if timing_recorder:
            timings_getter = getattr(self.search_client, "get_last_timings", None)
            if callable(timings_getter):
                timing_recorder.extend_search_timings(timings_getter())
        search_warnings: List[str] = []
        get_last_errors = getattr(self.search_client, "get_last_errors", None)
        if callable(get_last_errors):
            errors = get_last_errors() or []
            if hits and errors:
                for item in errors:
                    source = str(item.get("source") or "æœç´¢æœåŠ¡")
                    detail = str(item.get("error") or "æœªçŸ¥é”™è¯¯")
                    if source.lower().startswith("mcp"):
                        search_warnings.append(f"{source} æœªæ­£å¸¸å·¥ä½œï¼Œå·²ä½¿ç”¨å…¶ä»–æœç´¢ç»“æœã€‚åŸå› ï¼š{detail}")
                    else:
                        search_warnings.append(f"{source} å‡ºç°å¼‚å¸¸ï¼š{detail}")
        hits, rerank_meta = self._apply_rerank(query, hits, limit=num_search_results)
        # å¦‚æœæ˜¯æ’åæŸ¥è¯¢ä¸”æœ‰æå–çš„æ’åæ•°æ®ï¼Œåˆ™å°†å…¶æ·»åŠ åˆ°æç¤ºä¸­
        if is_ranking_query and ranking_data:
            ranking_info = "\n\næå–çš„æ’åæ•°æ®:\n"
            if ranking_data['cuhk_rankings']:
                ranking_info += "CUHKæ’å:\n"
                for year, rank in sorted(ranking_data['cuhk_rankings'].items()):
                    ranking_info += f"- {year}å¹´: #{rank}\n"
            
            if ranking_data['hkust_rankings']:
                ranking_info += "HKUSTæ’å:\n"
                for year, rank in sorted(ranking_data['hkust_rankings'].items()):
                    ranking_info += f"- {year}å¹´: #{rank}\n"
            
            # ä¿®æ”¹æç¤ºä»¥åŒ…å«æ’åæ•°æ®
            context_block = self._format_search_hits(hits)
            user_prompt = self.build_prompt(query, hits, ranking_info)
        else:
            user_prompt = self.build_prompt(query, hits)
        response_start = time.perf_counter()
        try:
            response = self.llm_client.chat(
                system_prompt=self.system_prompt,
                user_prompt=user_prompt,
                max_tokens=max_tokens,
                temperature=temperature,
                images=images,
            )
        finally:
            if timing_recorder:
                duration_ms = (time.perf_counter() - response_start) * 1000
                timing_recorder.record_llm_call(
                    label="search_answer",
                    duration_ms=duration_ms,
                    provider=getattr(self.llm_client, "provider", None),
                    model=getattr(self.llm_client, "model_id", None),
                )

        # Build answer with URL references
        answer = response.get("content")
        reference_hits = hits if reference_limit is None else hits[:reference_limit]
        if answer and reference_hits:
            # Append reference list
            answer += "\n\n**å‚è€ƒé“¾æ¥ï¼š**\n"
            for idx, hit in enumerate(reference_hits, start=1):
                url = hit.url or "No URL available."
                title = hit.title or f"ç»“æœ {idx}"
                answer += f"{idx}. [{title}]({url})\n"

        result: Dict[str, object] = {
            "query": query,
            "answer": answer,
            "search_hits": [asdict(hit) for hit in hits],
            "llm_raw": response.get("raw"),
            "llm_warning": response.get("warning"),
            "llm_error": response.get("error"),
            "rerank": rerank_meta or None,
            "search_query": effective_query,
        }
        if search_warnings:
            result["search_warnings"] = search_warnings
        return result

    def answer_stream(
        self,
        query: str,
        *,
        search_query: Optional[str] = None,
        num_search_results: int = 5,
        per_source_limit: Optional[int] = None,
        max_tokens: int = 5000,
        temperature: float = 0.3,
        freshness: Optional[str] = None,
        date_restrict: Optional[str] = None,
        timing_recorder: Optional[TimingRecorder] = None,
        reference_limit: Optional[int] = None,
    ):
        # Prefer keyword-focused query generated upstream when available.
        effective_query = search_query.strip() if search_query else query

        per_source_cap = per_source_limit if per_source_limit is not None else num_search_results
        hits = self.search_client.search(
            effective_query,
            num_results=num_search_results,
            per_source_limit=per_source_cap,
            freshness=freshness,
            date_restrict=date_restrict,
        )
        if timing_recorder:
            timings_getter = getattr(self.search_client, "get_last_timings", None)
            if callable(timings_getter):
                timing_recorder.extend_search_timings(timings_getter())
        search_warnings: List[str] = []
        get_last_errors = getattr(self.search_client, "get_last_errors", None)
        if callable(get_last_errors):
            errors = get_last_errors() or []
            if hits and errors:
                for item in errors:
                    source = str(item.get("source") or "æœç´¢æœåŠ¡")
                    detail = str(item.get("error") or "æœªçŸ¥é”™è¯¯")
                    if source.lower().startswith("mcp"):
                        search_warnings.append(f"{source} æœªæ­£å¸¸å·¥ä½œï¼Œå·²ä½¿ç”¨å…¶ä»–æœç´¢ç»“æœã€‚åŸå› ï¼š{detail}")
                    else:
                        search_warnings.append(f"{source} å‡ºç°å¼‚å¸¸ï¼š{detail}")
        
        hits, rerank_meta = self._apply_rerank(query, hits, limit=num_search_results)

        # First, yield preliminary data
        preliminary_data = {
            "query": query,
            "search_hits": [asdict(hit) for hit in hits],
            "rerank": rerank_meta or None,
            "search_query": effective_query,
        }
        if search_warnings:
            preliminary_data["search_warnings"] = search_warnings
        
        yield json.dumps({"type": "preliminary", "data": preliminary_data})


        user_prompt = self.build_prompt(query, hits)
        response_start = time.perf_counter()
        
        # Stream the response
        full_answer = ""
        try:
            stream = self.llm_client.chat_stream(
                system_prompt=self.system_prompt,
                user_prompt=user_prompt,
                max_tokens=max_tokens,
                temperature=temperature,
            )
            for chunk in stream:
                if chunk.startswith("Error:"):
                    yield json.dumps({"type": "error", "data": chunk})
                    return
                full_answer += chunk
                yield json.dumps({"type": "content", "data": chunk})

        finally:
            if timing_recorder:
                duration_ms = (time.perf_counter() - response_start) * 1000
                timing_recorder.record_llm_call(
                    label="search_answer_stream",
                    duration_ms=duration_ms,
                    provider=getattr(self.llm_client, "provider", None),
                    model=getattr(self.llm_client, "model_id", None),
                )

        # Finally, yield the references
        reference_hits = hits if reference_limit is None else hits[:reference_limit]
        if full_answer and reference_hits:
            reference_text = "\n\n**å‚è€ƒé“¾æ¥ï¼š**\n"
            for idx, hit in enumerate(reference_hits, start=1):
                url = hit.url or "No URL available."
                title = hit.title or f"ç»“æœ {idx}"
                reference_text += f"{idx}. [{title}]({url})\n"
            yield json.dumps({"type": "references", "data": reference_text})

    def _apply_rerank(
        self,
        query: str,
        hits: List[SearchHit],
        *,
        limit: Optional[int] = None,
    ) -> Tuple[List[SearchHit], List[Dict[str, object]]]:
        if not self.reranker or not hits:
            return hits, []

        try:
            reranked = self.reranker.rerank(query, hits)
        except Exception as exc:  # pragma: no cover - best effort resilience
            return hits, [{"error": str(exc)}]

        filtered: List[SearchHit] = []
        metadata: List[Dict[str, object]] = []
        domain_counts: Dict[str, int] = {}
        max_results = limit or len(reranked)

        for item in reranked:
            domain = self._extract_domain(item.hit.url)
            if domain and domain_counts.get(domain, 0) >= self.max_per_domain:
                metadata.append(
                    {
                        "url": item.hit.url,
                        "score": item.score,
                        "dropped": "per_domain_limit",
                    }
                )
                continue
            if item.score is not None and item.score < self.min_rerank_score:
                metadata.append(
                    {
                        "url": item.hit.url,
                        "score": item.score,
                        "dropped": "below_min_score",
                    }
                )
                continue

            filtered.append(item.hit)
            metadata.append(
                {
                    "url": item.hit.url,
                    "score": item.score,
                    "kept": True,
                }
            )

            if domain:
                domain_counts[domain] = domain_counts.get(domain, 0) + 1

            if len(filtered) >= max_results:
                break

        if not filtered:
            return hits, metadata

        return filtered, metadata

    @staticmethod
    def _extract_domain(url: str) -> Optional[str]:
        if not url:
            return None
        return urlparse(url).netloc or None
